1
00:00:00,000 --> 00:00:02,366
内容/录制:Z0MI 酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,366 --> 00:00:02,833
哈喽大家好

3
00:00:02,833 --> 00:00:03,766
已经不知不觉

4
00:00:03,800 --> 00:00:05,933
来到了 MoE 架构里面

5
00:00:05,933 --> 00:00:07,366
接近最后的尾声了

6
00:00:11,266 --> 00:00:13,833
那今天来看看 more in MoE

7
00:00:13,833 --> 00:00:16,266
就是 MoE 还有哪些更多的内容

8
00:00:16,266 --> 00:00:17,300
那今天的主要内容

9
00:00:17,333 --> 00:00:19,933
来看一下 V-MoE 的一个可视化的原理

10
00:00:19,933 --> 00:00:22,933
还有 Soft-MoE 的一个可视化的原理

11
00:00:22,966 --> 00:00:25,866
那这个 V-MoE 还是 Soft-MoE

12
00:00:26,333 --> 00:00:30,366
主要还是来自于这里面的一个 maarten

13
00:00:30,366 --> 00:00:34,000
里面关于 MoE 一个相关的可视化

14
00:00:34,000 --> 00:00:36,200
相关的内容来进行一个解读

15
00:00:36,566 --> 00:00:38,600
那回到整个视频目录大纲了

16
00:00:38,600 --> 00:00:41,366
今天会跟大家分开两个内容

17
00:00:41,366 --> 00:00:42,600
第一个就是 V-MoE

18
00:00:42,600 --> 00:00:45,600
第二个就是 soft-MoE 相关的事情

19
00:00:45,866 --> 00:00:47,366
在整个 MOE 混合专家

20
00:00:47,400 --> 00:00:49,166
现在已经来到了最后了

21
00:00:49,166 --> 00:00:50,466
那 ZOMI 在这里面

22
00:00:50,466 --> 00:00:51,633
觉得最核心的第一个

23
00:00:51,633 --> 00:00:53,066
就是 MoE 的基本介绍

24
00:00:53,133 --> 00:00:55,533
让大家了解一下 MoE 是什么

25
00:00:55,533 --> 00:00:57,600
那这个是 ZOMI 做了一个收费

26
00:00:57,600 --> 00:01:00,200
因为过年的时候因为剪视频了

27
00:01:00,200 --> 00:01:02,333
花了我自己好多的钱

28
00:01:02,533 --> 00:01:04,733
那后面的 ZOMI 觉得可能比较有意思

29
00:01:04,733 --> 00:01:07,133
就是整个 MoE 的核心的架构的原理

30
00:01:07,133 --> 00:01:10,166
看一下 MoE 有哪些最重要的东西

31
00:01:10,200 --> 00:01:11,966
这里面的大模型遇到 MoE

32
00:01:11,966 --> 00:01:13,533
反倒是更多的一个

33
00:01:13,566 --> 00:01:15,866
对于混合专家的思考

34
00:01:16,133 --> 00:01:18,666
那手撕 MoE 代码就是带着大家自己

35
00:01:18,666 --> 00:01:21,366
用昇腾的硬件和昇腾服务器

36
00:01:21,400 --> 00:01:23,933
简单实现一个对应的 MOE

37
00:01:23,933 --> 00:01:25,466
今天来到了最后一个内容

38
00:01:25,466 --> 00:01:27,466
MOE 的未来啦

39
00:01:27,533 --> 00:01:28,266
那里面

40
00:01:28,266 --> 00:01:30,866
V-MOE 主要是视觉跟多模态的 MOE

41
00:01:30,866 --> 00:01:31,700
soft-MOE

42
00:01:31,733 --> 00:01:34,233
主要是基于 V-MOE 的一个具体的改

43
00:01:34,233 --> 00:01:36,333
进做一个可微的内容

44
00:01:36,800 --> 00:01:37,666
那今天

45
00:01:37,666 --> 00:01:41,700
马上来到第一个 V-MOE 的一个可视化原理

46
00:01:44,166 --> 00:01:45,133
现在来看一下

47
00:01:45,133 --> 00:01:46,433
第一个内容

48
00:01:46,433 --> 00:01:48,366
V-MoE 的一个可视化内容

49
00:01:48,433 --> 00:01:51,266
那其实蛮明确的知道了

50
00:01:51,266 --> 00:01:54,733
这个 V-MoE 其实叫做 vision Multi of expert

51
00:01:54,766 --> 00:01:56,000
那整个 V-MoE

52
00:01:56,000 --> 00:01:58,533
其实是继承 vision-Transformer

53
00:01:58,566 --> 00:01:59,333
那 vision-Transformer

54
00:01:59,333 --> 00:02:01,833
其实很好的去理解

55
00:02:01,866 --> 00:02:04,500
以前的所有的大语言模型

56
00:02:04,533 --> 00:02:06,166
或者整个 Transformer 的数

57
00:02:06,166 --> 00:02:07,966
是一系列的 Token

58
00:02:08,166 --> 00:02:09,966
那会把一个 sequence

59
00:02:09,966 --> 00:02:11,800
转换成为具体的 Token

60
00:02:11,800 --> 00:02:14,166
Token 是不可见的一些向量

61
00:02:14,166 --> 00:02:15,766
那这里面可以看到

62
00:02:15,766 --> 00:02:17,600
会把整个 sequence

63
00:02:17,600 --> 00:02:19,866
切分成很多个不同的词元

64
00:02:19,866 --> 00:02:20,766
然后每个词元

65
00:02:20,800 --> 00:02:23,000
给 embedding size 进行处理

66
00:02:23,166 --> 00:02:25,066
但是对于一个图片来说

67
00:02:25,066 --> 00:02:26,900
整个 v-Transformer 怎么去实现

68
00:02:26,933 --> 00:02:28,433
首先会把一个图片来

69
00:02:28,433 --> 00:02:30,900
进行划分成不同的格子

70
00:02:30,933 --> 00:02:33,066
那每个格子叫做 patch

71
00:02:33,066 --> 00:02:34,500
因为有一系列的格子

72
00:02:34,533 --> 00:02:36,200
所以会叫做 patches

73
00:02:36,266 --> 00:02:37,066
每个 patch

74
00:02:37,066 --> 00:02:39,266
就像刚才的一个语言一样

75
00:02:39,266 --> 00:02:40,266
或者语料一样

76
00:02:40,266 --> 00:02:42,100
变成一个具体的 Token

77
00:02:42,333 --> 00:02:44,600
进行给大模型的输入

78
00:02:44,600 --> 00:02:45,800
但是整体来说

79
00:02:45,800 --> 00:02:47,966
它怎么变成一个 Transformer 架构

80
00:02:47,966 --> 00:02:49,733
能够接受的输入

81
00:02:49,733 --> 00:02:50,166
很重要

82
00:02:50,166 --> 00:02:52,733
就把这一堆的一个 patches

83
00:02:52,833 --> 00:02:54,333
进行一个拉长

84
00:02:54,400 --> 00:02:57,333
拉长一条算叫做 Flatten 的操作

85
00:02:57,333 --> 00:02:57,666
当然了

86
00:02:57,666 --> 00:03:00,833
Pytorch 里面也有一个 Flatten 相关的 API

87
00:03:01,166 --> 00:03:03,833
那现在可以看到每一个格子

88
00:03:03,833 --> 00:03:05,033
也就是图片里面

89
00:03:05,033 --> 00:03:06,466
所谓的每一个 patch

90
00:03:06,566 --> 00:03:07,733
就变成了一系

91
00:03:07,733 --> 00:03:10,800
列的或者一堆 sequence 相关的内容

92
00:03:10,800 --> 00:03:12,333
跟一个大语言模型

93
00:03:12,333 --> 00:03:14,933
里面的一个词元的输入

94
00:03:14,933 --> 00:03:16,233
就非常的像了

95
00:03:16,233 --> 00:03:18,666
于是现在经过一个线性层

96
00:03:18,666 --> 00:03:20,366
因为上面的一个 Flatten

97
00:03:20,666 --> 00:03:23,166
它代表的是一系列的图片的 patch

98
00:03:23,200 --> 00:03:24,833
加一个线性层

99
00:03:24,833 --> 00:03:27,300
那这个线性层在 Pytorch 里面实现

100
00:03:27,333 --> 00:03:30,266
可能只是简单的一个矩阵层的操作

101
00:03:30,266 --> 00:03:32,666
也就是 Linear 相关的操作

102
00:03:32,833 --> 00:03:33,900
有了 Linear 之后

103
00:03:33,933 --> 00:03:35,400
就会把每一个 patch

104
00:03:35,400 --> 00:03:37,666
变成一个具体的向量了

105
00:03:37,966 --> 00:03:39,033
但是这些向量

106
00:03:39,033 --> 00:03:40,066
不能直接输入

107
00:03:40,066 --> 00:03:42,100
因为是一个图片

108
00:03:42,133 --> 00:03:44,800
图片有对应的位置和相关的信息

109
00:03:44,800 --> 00:03:45,133
于是

110
00:03:45,133 --> 00:03:48,133
在这里面加入了一个 classics

111
00:03:48,133 --> 00:03:49,433
也就是一个标志位

112
00:03:49,600 --> 00:03:51,233
然后再把一个 position

113
00:03:51,233 --> 00:03:53,700
然后合并到每一个向量里面

114
00:03:53,733 --> 00:03:55,533
这里面有一个 position 的标志位

115
00:03:55,533 --> 00:03:57,966
这里面也有一个 position 的标志位

116
00:03:58,000 --> 00:04:01,933
每一个 patches 都有一个 position 的标志位

117
00:04:01,966 --> 00:04:05,566
作为整一个后面 Transformer 的一个输入

118
00:04:05,766 --> 00:04:07,066
当然了整个 Transformer

119
00:04:07,066 --> 00:04:07,933
它有一个 Encoder

120
00:04:07,966 --> 00:04:09,166
也有一个 decorder

121
00:04:09,166 --> 00:04:10,400
然后在 Encoder

122
00:04:10,400 --> 00:04:13,066
包括 decorder 里面的 FFN 层

123
00:04:13,200 --> 00:04:14,866
才会替换为专家

124
00:04:14,866 --> 00:04:17,366
所以说整个 vision moe

125
00:04:17,400 --> 00:04:20,000
跟 vision Transformer 是非常的像

126
00:04:20,366 --> 00:04:22,466
所以只要你思想提得早

127
00:04:22,466 --> 00:04:23,666
文章发的早

128
00:04:23,666 --> 00:04:25,733
你就基本上在整个领域

129
00:04:25,766 --> 00:04:28,766
会有非常大的一个具体的影响力

130
00:04:29,200 --> 00:04:31,433
现在打开整个 vision Transformer

131
00:04:31,433 --> 00:04:33,233
里面的一个 Encoder

132
00:04:33,233 --> 00:04:34,733
看到一个 Encoder

133
00:04:34,766 --> 00:04:36,466
还是 decorder 也好打开

134
00:04:36,466 --> 00:04:38,533
除了刚才的一个 vision Transformer

135
00:04:38,566 --> 00:04:39,966
跟 Transformer 最大区别就是

136
00:04:39,966 --> 00:04:43,800
关于图片的往上面的真正基于 Transformer

137
00:04:43,800 --> 00:04:45,966
里面结构的预处理不一样

138
00:04:45,966 --> 00:04:47,833
后面的内容基本上一样

139
00:04:47,833 --> 00:04:50,433
而这里面的专家也是一样

140
00:04:50,600 --> 00:04:51,766
通过 FN 层

141
00:04:51,766 --> 00:04:52,666
把专家

142
00:04:52,666 --> 00:04:55,633
替换成为很多个小的 FN 层

143
00:04:55,633 --> 00:04:57,900
然后通过一个累积的操作

144
00:04:57,933 --> 00:05:00,566
就变成最终的输出了

145
00:05:00,566 --> 00:05:05,000
这个就是 vision MoE V-MOE 相关的内容

146
00:05:05,633 --> 00:05:06,066
但是

147
00:05:06,066 --> 00:05:08,433
大家值得注意的就是图片

148
00:05:08,433 --> 00:05:10,033
跟语言不同

149
00:05:10,033 --> 00:05:11,366
语言可以有上下文

150
00:05:11,400 --> 00:05:12,833
但是图片的信息

151
00:05:12,833 --> 00:05:15,433
比语料的信息密集很多

152
00:05:16,166 --> 00:05:18,566
那语言语料了可以做均衡负载

153
00:05:18,566 --> 00:05:19,933
图片做均衡负载

154
00:05:19,933 --> 00:05:21,733
或者做那个专家截断时候

155
00:05:21,733 --> 00:05:22,633
怎么去实现

156
00:05:22,633 --> 00:05:23,333
那现在

157
00:05:23,366 --> 00:05:26,133
还是以左边的这个狗狗的图片为例子

158
00:05:26,133 --> 00:05:28,400
现在有很多个 patches

159
00:05:28,400 --> 00:05:29,733
切分成现在

160
00:05:29,733 --> 00:05:31,866
每个专家就处理一部分的数据

161
00:05:31,866 --> 00:05:34,433
叫做 expert 的一个 capacity

162
00:05:34,466 --> 00:05:35,466
专家的容量

163
00:05:35,466 --> 00:05:37,466
假设这里面的专家容量是 4

164
00:05:37,666 --> 00:05:39,133
那就每一个专家

165
00:05:39,166 --> 00:05:41,133
都可以处理 4 个 patches

166
00:05:41,133 --> 00:05:43,466
或者 4 个数据的样本

167
00:05:43,600 --> 00:05:45,333
但是这些数据的样本

168
00:05:45,333 --> 00:05:46,600
有可能处理完之后

169
00:05:46,666 --> 00:05:49,866
就导致最后的 patches 没有重点

170
00:05:49,866 --> 00:05:50,966
找不到重点

171
00:05:51,000 --> 00:05:54,433
或者过滤掉了可能核心的信息

172
00:05:54,966 --> 00:05:56,366
为了解决这个问题

173
00:05:56,366 --> 00:05:58,800
其实第一个能够想到最好的方法

174
00:05:58,800 --> 00:06:01,633
就是把 FFN 的一个 capacity

175
00:06:01,633 --> 00:06:03,833
也就是专家的容量变大

176
00:06:04,066 --> 00:06:05,833
使得最终整个专家

177
00:06:05,833 --> 00:06:08,066
每个专家都能处理越来越多

178
00:06:08,066 --> 00:06:09,666
或者更多的数据

179
00:06:09,733 --> 00:06:11,733
但是把专家变大

180
00:06:11,733 --> 00:06:13,566
那跟一个没有专家

181
00:06:13,566 --> 00:06:15,633
或者一个非常大的一个 FN 层

182
00:06:15,633 --> 00:06:16,633
有啥区别

183
00:06:16,633 --> 00:06:19,566
所以说这不是 MOE 架构该处理的事情

184
00:06:19,600 --> 00:06:21,166
或者 MOE 架构的优势

185
00:06:21,466 --> 00:06:22,366
那 MOE 的架构

186
00:06:22,400 --> 00:06:24,266
就把每一个 FFN

187
00:06:24,266 --> 00:06:25,566
都做的比较小

188
00:06:25,600 --> 00:06:27,233
有很多个 FFN

189
00:06:27,566 --> 00:06:29,933
那为了解决 capacity 的一个

190
00:06:29,933 --> 00:06:31,566
就专家容量的问题

191
00:06:31,833 --> 00:06:34,633
这里面在 V-MOE 的文章里面

192
00:06:34,633 --> 00:06:35,833
就提出了一个点

193
00:06:35,833 --> 00:06:38,300
叫做 priority scorer

194
00:06:38,300 --> 00:06:39,633
也就是优先得分

195
00:06:39,633 --> 00:06:42,166
尽可能的让每一个专家

196
00:06:42,200 --> 00:06:43,800
都处理比较核心

197
00:06:43,800 --> 00:06:46,266
一些图片里面的相关的信息

198
00:06:46,266 --> 00:06:47,433
就像这边图里面

199
00:06:47,433 --> 00:06:48,233
可以看到

200
00:06:48,233 --> 00:06:51,266
其实图片的中间相关的内容

201
00:06:51,266 --> 00:06:54,933
这只狗狗才是最核心的内容

202
00:06:55,166 --> 00:06:58,400
或里面的信息才会更加丰富和复杂

203
00:06:58,433 --> 00:06:59,633
旁边的这些律数

204
00:06:59,633 --> 00:07:00,966
其实不应该去处理

205
00:07:01,000 --> 00:07:02,266
所以 FFN

206
00:07:02,266 --> 00:07:03,300
专家

207
00:07:03,333 --> 00:07:06,233
应该更多的去处理相关的内容

208
00:07:06,233 --> 00:07:06,700
因此

209
00:07:06,733 --> 00:07:10,133
就有了刚才提出的 priority scorer 这个内容

210
00:07:10,333 --> 00:07:11,366
通过 priority scorer

211
00:07:11,366 --> 00:07:13,400
尽可能的让专家

212
00:07:13,400 --> 00:07:15,533
去处理核心的信息

213
00:07:15,533 --> 00:07:16,166
那这里面

214
00:07:16,166 --> 00:07:17,266
可能 priority scorer

215
00:07:17,266 --> 00:07:19,100
或者 importance patch

216
00:07:19,133 --> 00:07:20,833
会分开不同的内容

217
00:07:20,833 --> 00:07:22,900
但是在没有全部全选的时候

218
00:07:22,933 --> 00:07:25,400
可能对整个图片进行感知

219
00:07:25,400 --> 00:07:26,066
但实际上

220
00:07:26,066 --> 00:07:27,266
一张图片里面

221
00:07:27,266 --> 00:07:28,300
最核心的内容

222
00:07:28,333 --> 00:07:30,333
或者真正有用的信息的内容

223
00:07:30,333 --> 00:07:32,266
是集中在一个地方

224
00:07:33,166 --> 00:07:34,633
有了 V-MoE 之后

225
00:07:34,633 --> 00:07:37,866
你会发现实际上还有很多的问题

226
00:07:38,133 --> 00:07:39,800
例如怎么做可微

227
00:07:39,800 --> 00:07:40,800
怎么做训练

228
00:07:40,800 --> 00:07:42,233
还有图像的信息融合

229
00:07:42,233 --> 00:07:43,333
怎么去实现

230
00:07:43,433 --> 00:07:45,333
于是为了解决这些问题

231
00:07:45,366 --> 00:07:47,766
业界就出现了另外一个新的 idea

232
00:07:47,766 --> 00:07:49,600
叫做 soft-MoE

233
00:07:49,600 --> 00:07:51,733
去解决刚才提到的一系列问题

234
00:07:51,733 --> 00:07:53,733
或者 V-MOE 带来的一个问题

235
00:07:58,800 --> 00:07:59,733
那么可以看到

236
00:07:59,733 --> 00:08:00,433
V-MOE

237
00:08:00,433 --> 00:08:03,500
通过选择最核心的内容去处理

238
00:08:03,533 --> 00:08:05,666
或者去剖析掉一些不重要的信息

239
00:08:05,833 --> 00:08:07,433
但是那些信息就不重要了吗

240
00:08:07,433 --> 00:08:08,300
这里面

241
00:08:08,333 --> 00:08:10,966
可能会丢失一些真正有用的信息

242
00:08:10,966 --> 00:08:12,166
例如狗狗的尾巴

243
00:08:12,166 --> 00:08:13,933
可能就真的把整个 patch

244
00:08:13,933 --> 00:08:15,400
或者 sample 给丢掉了

245
00:08:15,400 --> 00:08:17,733
因为 capacity 的一个截断

246
00:08:17,733 --> 00:08:19,266
还有一个 importance

247
00:08:19,266 --> 00:08:20,500
一个相关的内容

248
00:08:20,566 --> 00:08:21,600
为了解决这个问题

249
00:08:21,600 --> 00:08:23,000
就出现 soft MOE

250
00:08:23,000 --> 00:08:25,033
那回顾一下整个 priority scorer

251
00:08:25,033 --> 00:08:27,466
主要是对一个输入

252
00:08:27,633 --> 00:08:29,433
进行一个 importance 打分

253
00:08:29,433 --> 00:08:31,933
把一些 less important 的东西去掉

254
00:08:31,966 --> 00:08:34,000
把 more important 的东西合并掉

255
00:08:34,000 --> 00:08:35,800
然后 more important 的东西更多

256
00:08:35,800 --> 00:08:37,633
或者里面的 patch sample

257
00:08:37,633 --> 00:08:40,133
更多地给到一个 FN 专家

258
00:08:40,166 --> 00:08:41,166
进行处理

259
00:08:41,166 --> 00:08:41,933
那最后

260
00:08:41,933 --> 00:08:44,333
你会发现有很多没有处理的 patches

261
00:08:44,333 --> 00:08:45,666
就丢失掉了

262
00:08:45,733 --> 00:08:46,633
所以在这里面

263
00:08:46,633 --> 00:08:49,866
Soft-MoE 更多希望融合一些信息

264
00:08:49,866 --> 00:08:52,066
然后把丢失的信息找回来

265
00:08:52,066 --> 00:08:52,700
那这里面

266
00:08:52,733 --> 00:08:54,833
更多的就把多个信息

267
00:08:54,833 --> 00:08:56,533
来进行一个合并

268
00:08:56,566 --> 00:08:57,800
那这里面可以看到

269
00:08:57,800 --> 00:08:58,766
两个信息了

270
00:08:58,766 --> 00:09:00,366
通过一个融合的计算

271
00:09:00,366 --> 00:09:03,566
把它合并成为一个 patches 进行处理

272
00:09:03,566 --> 00:09:05,233
通过这种方式来去实现

273
00:09:05,233 --> 00:09:07,133
利用看一下下面的这个图案

274
00:09:07,166 --> 00:09:09,833
把一些相关的有用的一个 patches

275
00:09:09,833 --> 00:09:11,233
进行了一个合并

276
00:09:11,533 --> 00:09:12,266
合并的方式

277
00:09:12,266 --> 00:09:14,100
可以通过加权求和

278
00:09:14,166 --> 00:09:15,766
也可以通过 contact 方式

279
00:09:15,766 --> 00:09:17,766
把具体变成一个具体

280
00:09:17,766 --> 00:09:20,666
patches 给到 FFN 层进行处理

281
00:09:20,666 --> 00:09:21,366
那这个时候

282
00:09:21,400 --> 00:09:23,666
就可能更加高度的去融合

283
00:09:23,666 --> 00:09:25,433
相关的特征

284
00:09:25,566 --> 00:09:26,733
那现在

285
00:09:26,733 --> 00:09:27,333
刚才讲

286
00:09:27,333 --> 00:09:30,233
很需要看一下具体的一个计算

287
00:09:30,633 --> 00:09:33,233
现在假定有一个具体的输入

288
00:09:33,233 --> 00:09:35,266
这个数叫做 patch embeddings

289
00:09:35,266 --> 00:09:36,966
因为把整个图片

290
00:09:37,000 --> 00:09:38,200
通过 embedding 的方式

291
00:09:38,200 --> 00:09:41,033
或者刚才讲到的 Linear 加 position

292
00:09:41,033 --> 00:09:43,700
还有一个相关的计算之后

293
00:09:43,800 --> 00:09:46,466
就变成了具体的一个 s 的矩阵

294
00:09:46,466 --> 00:09:48,766
那这里面定义几个参数

295
00:09:48,800 --> 00:09:49,933
那首先行组

296
00:09:49,933 --> 00:09:50,766
列了

297
00:09:50,766 --> 00:09:52,666
就定义为 dimension d

298
00:09:52,666 --> 00:09:55,333
代表的是输入的 Vector 的大小

299
00:09:55,400 --> 00:09:57,533
然后纵坐标是 m

300
00:09:57,533 --> 00:09:59,566
m 就代表有多少个 patches

301
00:09:59,600 --> 00:10:00,433
通过这种方式

302
00:10:00,433 --> 00:10:02,100
就把每一张图片

303
00:10:02,200 --> 00:10:03,400
或者每一个 patches

304
00:10:03,400 --> 00:10:06,266
具体表示为一个向量或者一个矩阵

305
00:10:06,266 --> 00:10:07,233
或者一个张量了

306
00:10:07,233 --> 00:10:08,033
后面

307
00:10:08,533 --> 00:10:11,666
现在提供一个可以学习的矩阵

308
00:10:11,666 --> 00:10:12,833
叫做Φ

309
00:10:12,966 --> 00:10:15,566
那这个Φ的矩阵里面的一个行

310
00:10:15,566 --> 00:10:16,933
是一个 d

311
00:10:16,966 --> 00:10:18,333
也要对应的一个 input

312
00:10:18,333 --> 00:10:20,033
Vector 的一个对应的大小

313
00:10:20,466 --> 00:10:23,666
这里面的矩阵是 m 乘以 d

314
00:10:23,766 --> 00:10:26,733
那这个矩阵是 n 乘以 d

315
00:10:26,800 --> 00:10:27,433
那最后

316
00:10:27,433 --> 00:10:29,333
得到的结论或者答案

317
00:10:29,366 --> 00:10:32,433
就是 m 乘以 n 的这一个矩阵

318
00:10:32,433 --> 00:10:33,966
那这里面 d 的矩阵

319
00:10:34,000 --> 00:10:36,633
其实你可以发现每一个小的方块

320
00:10:36,633 --> 00:10:37,966
是一个 P

321
00:10:38,000 --> 00:10:40,033
P 就是 capacity 的 factor

322
00:10:40,066 --> 00:10:42,366
这里面就根据 capacity 的 factor

323
00:10:42,400 --> 00:10:44,566
就分开多个 expert

324
00:10:44,566 --> 00:10:45,533
那这里面的 expert

325
00:10:45,533 --> 00:10:46,166
现在还没有

326
00:10:46,166 --> 00:10:47,533
具体的一个分开

327
00:10:47,533 --> 00:10:49,233
只是通过矩阵的方式

328
00:10:49,466 --> 00:10:51,133
进行一个处理和选择

329
00:10:51,233 --> 00:10:52,733
接着得到另外一个矩阵

330
00:10:52,766 --> 00:10:55,666
叫做 R 的这个就是路由的矩阵

331
00:10:55,666 --> 00:10:57,633
那刚才的是 m 乘以 d

332
00:10:57,633 --> 00:10:59,533
然后 n 乘以 d

333
00:10:59,566 --> 00:11:00,233
那最后

334
00:11:00,233 --> 00:11:03,166
就变成了一个 m 乘以 n

335
00:11:03,200 --> 00:11:03,600
当然了

336
00:11:03,600 --> 00:11:05,600
有多少个专家或 capacity

337
00:11:05,600 --> 00:11:06,333
就除以 p

338
00:11:06,333 --> 00:11:10,466
就等于多少个专家 expert 相关的内容了

339
00:11:10,866 --> 00:11:13,666
那现在可以看到对应的每一行

340
00:11:13,666 --> 00:11:14,700
因为每每一行

341
00:11:14,733 --> 00:11:16,533
都是一个维度 m

342
00:11:16,800 --> 00:11:19,200
m 就是代表每一个具体的 patches

343
00:11:19,200 --> 00:11:22,366
跟每一个专家之间的一个关系了

344
00:11:22,366 --> 00:11:23,400
所以这里面

345
00:11:23,400 --> 00:11:25,633
就很好的去处理专家

346
00:11:25,633 --> 00:11:28,633
跟图像之间的关联关系

347
00:11:28,966 --> 00:11:30,333
有了对应的专家之后

348
00:11:30,333 --> 00:11:33,166
对整个专家乘以一个 soft max

349
00:11:33,400 --> 00:11:36,266
就是对应的一个路由的输出

350
00:11:37,000 --> 00:11:40,266
再乘以对应的输入的 embedding patches

351
00:11:40,266 --> 00:11:43,466
那最终就得到输出的一个 update

352
00:11:43,466 --> 00:11:44,833
patches embedding 了

353
00:11:44,833 --> 00:11:46,966
也就是更新后的 patches embedding

354
00:11:47,033 --> 00:11:48,533
通过这种方式

355
00:11:49,000 --> 00:11:51,333
使得每一个 patches

356
00:11:51,333 --> 00:11:54,033
跟专家进行一个有机的结合

357
00:11:54,033 --> 00:11:55,433
这种结合叫做 soft

358
00:11:55,433 --> 00:11:57,633
而不是简单的一个求和

359
00:11:57,633 --> 00:12:00,466
或者加权平均的方式来去实现

360
00:12:01,000 --> 00:12:02,833
现在了解完这一系列的计算之后

361
00:12:02,833 --> 00:12:05,533
来回顾一下整个 MoE

362
00:12:05,566 --> 00:12:07,633
或者 Soft-MoE 是怎么去计算

363
00:12:07,633 --> 00:12:10,266
首先输入一个 x 的一个 inbanding

364
00:12:10,266 --> 00:12:11,733
或者 purchase inbanding

365
00:12:11,800 --> 00:12:12,366
然后

366
00:12:12,366 --> 00:12:15,400
乘一个可学习的一个向量Φ

367
00:12:15,733 --> 00:12:16,800
通过这个可学习

368
00:12:16,800 --> 00:12:17,333
向量Φ

369
00:12:17,333 --> 00:12:19,433
得到一个路由的矩阵 R

370
00:12:19,600 --> 00:12:20,533
路由的矩阵 R

371
00:12:20,533 --> 00:12:23,033
就根据 expert 来进行一个划分

372
00:12:23,033 --> 00:12:24,100
这里面是 m 行

373
00:12:24,133 --> 00:12:26,000
每一行来代表一个 patches

374
00:12:26,000 --> 00:12:28,333
每个 patches 来跟每一个 expert

375
00:12:28,333 --> 00:12:29,333
每一个专家

376
00:12:29,333 --> 00:12:31,600
都建立了对应的关联关系

377
00:12:31,600 --> 00:12:33,433
或者都学习了对应的关联关系

378
00:12:33,600 --> 00:12:33,866
然后

379
00:12:33,866 --> 00:12:36,166
给到 Softmax 进行一个处理

380
00:12:36,200 --> 00:12:37,233
Softmax 处理完了

381
00:12:37,233 --> 00:12:40,666
就得到了更新后的一个具体的 x

382
00:12:40,666 --> 00:12:42,300
就具体的输入的值了

383
00:12:42,333 --> 00:12:43,466
就这张图

384
00:12:43,466 --> 00:12:45,700
这个图就是有点像融合后

385
00:12:45,800 --> 00:12:46,733
特征融合后

386
00:12:46,733 --> 00:12:48,833
或者特征加持后的一个具体的输入

387
00:12:49,266 --> 00:12:50,033
那后面的内容

388
00:12:50,033 --> 00:12:52,133
就跟其他的 MoE 的方式比较像了

389
00:12:52,166 --> 00:12:54,600
每个得到了对应的 MoE

390
00:12:54,600 --> 00:12:55,533
具体的向量

391
00:12:55,533 --> 00:12:57,666
或者对应的具体的张量的内容

392
00:12:57,666 --> 00:12:59,166
然后对它进行一个句子输入

393
00:12:59,200 --> 00:13:01,133
得到对应的 y

394
00:13:01,733 --> 00:13:02,000
最后

395
00:13:02,000 --> 00:13:04,466
这个 r 的矩阵处理在上面以外了

396
00:13:04,466 --> 00:13:05,533
还在下面

397
00:13:05,566 --> 00:13:06,800
以每一列

398
00:13:06,833 --> 00:13:09,066
每一列因为作为一个 expert 来进行

399
00:13:09,066 --> 00:13:10,100
乘以一个 Softmax

400
00:13:10,133 --> 00:13:11,966
然后跟里面的 y 的输出

401
00:13:11,966 --> 00:13:13,800
进行一个累积的相乘

402
00:13:13,833 --> 00:13:17,033
最后就得到最终的输出 Y‘了

403
00:13:17,033 --> 00:13:19,333
通过这种方式来去实现

404
00:13:19,966 --> 00:13:21,766
整一个 soft MOE 里面

405
00:13:21,766 --> 00:13:24,833
最核心的就是这个路由的 Matrix 了

406
00:13:25,066 --> 00:13:26,266
这个路由的 Matrix

407
00:13:26,266 --> 00:13:28,300
不仅仅影响输入的 Matrix

408
00:13:28,333 --> 00:13:30,966
而且也影响输出的专家

409
00:13:30,966 --> 00:13:32,766
通过这两种方式进行约束

410
00:13:32,766 --> 00:13:34,366
进行可微的求和

411
00:13:34,400 --> 00:13:36,533
使得整个 soft MOE

412
00:13:36,533 --> 00:13:38,266
就或者 visionMOE

413
00:13:38,266 --> 00:13:39,866
更加容易的去实现

414
00:13:39,866 --> 00:13:41,100
或者信息

415
00:13:41,133 --> 00:13:42,833
保留的更加的好

416
00:13:43,800 --> 00:13:47,133
每一期都有一个简单的小结和思考啦

417
00:13:50,533 --> 00:13:52,166
其实 ZOMI 在日常里面

418
00:13:52,233 --> 00:13:54,133
或者之前跟同事的一个讨论

419
00:13:54,166 --> 00:13:55,066
会发现哦

420
00:13:55,133 --> 00:13:56,766
大家对 MoE 的一个认知

421
00:13:56,766 --> 00:13:59,866
就是 MoE 基本上就是将数的数据

422
00:13:59,866 --> 00:14:01,166
根据任务

423
00:14:01,600 --> 00:14:03,833
任务的类型划分成多个区域

424
00:14:03,833 --> 00:14:05,966
所变成多个 expert 专家了

425
00:14:06,000 --> 00:14:07,600
那每个区域的数据

426
00:14:07,600 --> 00:14:10,766
就分配一个或者多个专家进行处理

427
00:14:10,766 --> 00:14:11,666
那这种方式

428
00:14:11,666 --> 00:14:15,100
是非常的天然的去适配 NLP

429
00:14:15,133 --> 00:14:16,866
就是自然语言处理的任务

430
00:14:16,866 --> 00:14:18,833
但是对于图像的任务

431
00:14:19,033 --> 00:14:21,266
MoE 是天然不适合

432
00:14:21,333 --> 00:14:23,433
而不是针对像素进行处理

433
00:14:23,433 --> 00:14:24,066
那这个

434
00:14:24,066 --> 00:14:25,066
是很多人

435
00:14:25,066 --> 00:14:27,833
或者之前有些专家跟 ZOMI 去讨论

436
00:14:27,833 --> 00:14:29,733
不过你会发看完发现

437
00:14:29,766 --> 00:14:33,066
处理完一个 V-MoE 跟 Soft-MoE 之后

438
00:14:33,366 --> 00:14:33,966
你会发现

439
00:14:33,966 --> 00:14:37,366
MoE 它真的不适合处理图像的数据吗？？？？？？?

440
00:14:37,366 --> 00:14:38,233
那这个问题

441
00:14:38,233 --> 00:14:40,233
留给大家去思考思考

442
00:14:40,400 --> 00:14:43,000
为什么大家一开始觉得 Transformer 架构

443
00:14:43,000 --> 00:14:45,066
就是专门针对 NLP

444
00:14:45,366 --> 00:14:47,466
现在 Transformer 架构都能做大语言

445
00:14:47,466 --> 00:14:48,233
模型生成

446
00:14:48,233 --> 00:14:49,366
多模态生成

447
00:14:49,466 --> 00:14:50,433
图像生成

448
00:14:50,433 --> 00:14:51,633
视频生成了

449
00:14:51,633 --> 00:14:52,366
那未来

450
00:14:52,400 --> 00:14:55,633
Moe 能不能在图像领域大放光彩

451
00:14:55,633 --> 00:14:57,466
有没有相关的研究

452
00:14:57,466 --> 00:14:59,133
希望现在的一些学生

453
00:14:59,166 --> 00:15:01,133
或者硕士跟博士的学生

454
00:15:01,133 --> 00:15:04,733
也去研究更多相关的创新和探索

455
00:15:04,766 --> 00:15:05,566
今天的内容

456
00:15:05,566 --> 00:15:06,533
就先到这里为止

457
00:15:06,533 --> 00:15:07,066
谢谢各位

458
00:15:07,066 --> 00:15:07,866
拜了个拜

