1
00:00:01,850 --> 00:00:02,650
hello

2
00:00:02,650 --> 00:00:03,333
大家好

3
00:00:03,333 --> 00:00:05,933
我是每天熬夜更新的一个 ZOMI

4
00:00:05,966 --> 00:00:08,766
今天我们来到了 deepseek 里面

5
00:00:08,766 --> 00:00:10,600
连续五天发布的 day one

6
00:00:10,600 --> 00:00:13,250
重点发布了一个 Flash MLA

7
00:00:13,450 --> 00:00:15,050
对它进行一个深度解读

8
00:00:19,133 --> 00:00:20,250
那我们可以看到

9
00:00:20,250 --> 00:00:22,766
其实在今天的凌晨早上

10
00:00:22,766 --> 00:00:24,283
就是 7 点多的时候

11
00:00:24,283 --> 00:00:25,483
Deepseek 已经发布了

12
00:00:25,483 --> 00:00:27,733
今天的一个非常核心的内容

13
00:00:27,733 --> 00:00:29,366
叫做 Flash MLA

14
00:00:29,366 --> 00:00:31,850
也就是它的一个 Deepseek V2 跟 V3

15
00:00:31,850 --> 00:00:34,366
所使用到的一个 attention 的机制

16
00:00:34,366 --> 00:00:35,483
那它发布之后

17
00:00:35,483 --> 00:00:36,966
最重要的就是开源了

18
00:00:36,966 --> 00:00:38,400
在这条链接里面

19
00:00:38,733 --> 00:00:40,050
所以今天

20
00:00:40,050 --> 00:00:41,566
ZOMI 跟大家一起去看一下

21
00:00:41,566 --> 00:00:43,200
Deepseek 开源的一个 Flash MLA

22
00:00:43,200 --> 00:00:44,600
有哪些主要的特性

23
00:00:44,650 --> 00:00:45,250
那其实

24
00:00:45,250 --> 00:00:46,600
简单的一句话就是

25
00:00:46,600 --> 00:00:47,333
Flash MLA

26
00:00:47,333 --> 00:00:50,050
适用于 Hopper GPU 的一个高效的 MLA 内核

27
00:00:50,050 --> 00:00:52,133
针对可变的长序列进行一个服务
 
28
00:00:52,133 --> 00:00:52,850
进行优化

29
00:00:52,850 --> 00:00:54,683
就是长序列进行优化速度了

30
00:00:54,683 --> 00:00:57,050
在 H800 的 SXM5 GPU 上面

31
00:00:57,050 --> 00:00:59,250
具有 3,000 Gbps 的内存速度

32
00:00:59,250 --> 00:01:01,483
还有 500 TFLOPS 的一个计算的上限

33
00:01:01,483 --> 00:01:03,166
也就是内存很高

34
00:01:03,166 --> 00:01:05,600
计算的峰值也很高

35
00:01:06,083 --> 00:01:07,133
那在整个内容里面

36
00:01:07,133 --> 00:01:09,733
Flash MLA 的开源链接就在这里面

37
00:01:09,733 --> 00:01:11,450
来 ZOMI 分享的这个 PPT

38
00:01:11,450 --> 00:01:13,766
就开源在下面的这条链接

39
00:01:13,800 --> 00:01:14,450
最后

40
00:01:14,450 --> 00:01:16,483
在这个视频里面所讲解的论文

41
00:01:16,483 --> 00:01:17,766
包括相关的注释

42
00:01:17,766 --> 00:01:20,733
还有我们把它下下来的论文解解读

43
00:01:20,733 --> 00:01:22,733
都放在下面的这条链接

44
00:01:22,733 --> 00:01:23,766
有兴趣的小伙伴

45
00:01:23,766 --> 00:01:25,766
也可以下载来去了解

46
00:01:26,200 --> 00:01:27,683
那回到我们今天的视频

47
00:01:27,683 --> 00:01:30,566
主要是跟大家去分享四个内容

48
00:01:30,566 --> 00:01:30,933
第一个

49
00:01:30,933 --> 00:01:31,366
就是

50
00:01:31,366 --> 00:01:34,566
Deepseek V2 的一个 MLA 论文的一个解读

51
00:01:34,600 --> 00:01:35,966
既然我们要了解 Flash

52
00:01:35,966 --> 00:01:37,933
MLA 肯定要去了解一下

53
00:01:37,933 --> 00:01:40,333
或者看一下 MLA 的一个具体的原理

54
00:01:40,483 --> 00:01:41,533
了解完论文之后

55
00:01:41,533 --> 00:01:43,083
更多的是抽象出来

56
00:01:43,083 --> 00:01:44,366
回到 PPT

57
00:01:44,366 --> 00:01:45,483
重点的去看一下

58
00:01:45,483 --> 00:01:47,850
MLA 的一个大概的原理

59
00:01:47,850 --> 00:01:49,333
有了 MLA 的一个原理之后

60
00:01:49,333 --> 00:01:50,650
我们看一下 Flash

61
00:01:50,650 --> 00:01:51,933
MLA 里面的 Flash

62
00:01:51,933 --> 00:01:54,366
到底是怎么去实现

63
00:01:54,366 --> 00:01:55,683
包括它的代码注释

64
00:01:55,683 --> 00:01:58,050
还有对应的整个代码仓的走读

65
00:01:58,050 --> 00:02:00,766
最后做一个简单的思考跟总结

66
00:02:00,800 --> 00:02:01,800
那今天的内容

67
00:02:01,800 --> 00:02:02,400
事不宜迟

68
00:02:02,400 --> 00:02:04,933
我们马上开启第一个内容

69
00:02:06,000 --> 00:02:08,283
第一个内容就是 Deepseek 的 V2

70
00:02:08,283 --> 00:02:10,050
MLA 的论文解读

71
00:02:10,050 --> 00:02:12,733
我们现在马上打开这一篇论文

72
00:02:13,166 --> 00:02:14,733
现在我们打开这一篇论文

73
00:02:14,733 --> 00:02:15,966
看一下 Deepseek V2

74
00:02:15,966 --> 00:02:17,650
有什么特别之处

75
00:02:17,650 --> 00:02:19,333
就最重要的 Deepseek V2 里面

76
00:02:19,333 --> 00:02:20,166
我们就提到了

77
00:02:20,166 --> 00:02:21,966
在整个摘要里面

78
00:02:21,966 --> 00:02:25,450
很核心的一点就是提出了一个 MLA

79
00:02:25,650 --> 00:02:26,483
说的 MLA

80
00:02:26,483 --> 00:02:27,650
就是 Multi-head

81
00:02:27,850 --> 00:02:30,683
多头的一个潜在的 attention 结构

82
00:02:30,683 --> 00:02:32,133
那这个 attention 结构

83
00:02:32,133 --> 00:02:34,333
就取代了以前的 MHA

84
00:02:34,333 --> 00:02:36,283
也就是我们多了一个 latent

85
00:02:36,366 --> 00:02:37,600
潜在的一个注意力

86
00:02:37,600 --> 00:02:39,133
或者潜在的一个引变量

87
00:02:39,166 --> 00:02:40,333
整个 MA

88
00:02:40,333 --> 00:02:41,483
最核心的就是

89
00:02:41,483 --> 00:02:44,250
压缩了一个 KV 的 Cache 的值

90
00:02:44,283 --> 00:02:46,966
通过一个潜在的向量

91
00:02:47,250 --> 00:02:49,683
接着这篇文章就说到了 Deepseek V2

92
00:02:49,683 --> 00:02:52,333
节省了 42%的一个训练的耗时

93
00:02:52,333 --> 00:02:55,450
还有减低了整个 KV Cache 的 93.3%

94
00:02:55,566 --> 00:02:57,000
相关的一个显存

95
00:02:57,083 --> 00:02:59,450
最后实现整体的吞吐了

96
00:02:59,450 --> 00:03:01,000
提升了 5.76 倍

97
00:03:01,083 --> 00:03:02,850
效果非常的牛逼

98
00:03:02,850 --> 00:03:05,683
最后还开源在这一条链接上面

99
00:03:05,683 --> 00:03:06,883
但是这个开源

100
00:03:06,883 --> 00:03:09,533
仅限于一个模型的 checkpoint

101
00:03:09,533 --> 00:03:10,166
在这里面

102
00:03:10,166 --> 00:03:12,366
从左边的这个图可以看到

103
00:03:12,400 --> 00:03:15,050
整体的一个 MMLU 的 performance

104
00:03:15,050 --> 00:03:16,933
效果非常的好

105
00:03:17,133 --> 00:03:17,566
另外的话

106
00:03:17,566 --> 00:03:18,966
右边就刚才讲到

107
00:03:18,966 --> 00:03:22,400
一系列的相关的一个节省的内存

108
00:03:22,400 --> 00:03:23,850
提升的效率了

109
00:03:23,850 --> 00:03:25,766
那在这本整篇文章里面

110
00:03:25,766 --> 00:03:27,000
现在我们看一下

111
00:03:27,000 --> 00:03:28,566
整体的一个目录大纲

112
00:03:28,566 --> 00:03:31,450
因为今天的重点是在 MLA

113
00:03:31,650 --> 00:03:33,600
就是第二个网络模型

114
00:03:33,600 --> 00:03:35,650
加构的这一个环节

115
00:03:35,733 --> 00:03:38,083
整个 MLA 重点最重要的一句话

116
00:03:38,083 --> 00:03:41,083
就是 Boost influenced efficient

117
00:03:41,083 --> 00:03:43,733
就是提升一个推理的速度来

118
00:03:43,733 --> 00:03:44,200
这里面

119
00:03:44,200 --> 00:03:46,933
ZOMI 可能会比较关心的就是 2.11

120
00:03:46,933 --> 00:03:48,483
还有 2.1.2

121
00:03:48,483 --> 00:03:50,600
还有 2.1.3

122
00:03:50,933 --> 00:03:53,450
那 2.1.4 更多的是一个对比了

123
00:03:53,450 --> 00:03:55,850
那后面的除了一个 Attention 之后

124
00:03:55,850 --> 00:03:57,650
其实 Deepseek V2

125
00:03:57,650 --> 00:03:59,800
最重要的就是一个 MoE 的架构

126
00:03:59,800 --> 00:04:01,966
所以 2.2 就是 MoE 相关的内容

127
00:04:01,966 --> 00:04:04,766
因为不在今天的一个 MLA 的解读

128
00:04:04,766 --> 00:04:05,966
所以我们就重点

129
00:04:05,966 --> 00:04:08,566
去看看 2.1 有哪些内容

130
00:04:08,850 --> 00:04:11,133
那我们现在马上往后翻

131
00:04:11,133 --> 00:04:12,133
因为整个 induction

132
00:04:12,133 --> 00:04:13,533
没什么太多的东西

133
00:04:13,533 --> 00:04:14,133
主要是讲

134
00:04:14,133 --> 00:04:16,966
反正我们现在的整个创新的架构

135
00:04:16,966 --> 00:04:17,933
非常的牛逼

136
00:04:17,933 --> 00:04:18,800
看一下这个图

137
00:04:18,800 --> 00:04:20,333
这个图其实我们现在

138
00:04:20,333 --> 00:04:22,766
最重要的就是在 attention 架构的时候

139
00:04:22,850 --> 00:04:26,283
就提出了一个 MLA 的具体的算法

140
00:04:26,450 --> 00:04:28,966
那我们现在先不看刚才那个图

141
00:04:28,966 --> 00:04:32,050
而是先看一看整个架构有哪些不一样

142
00:04:32,050 --> 00:04:34,000
或者我们看一下论文里面的重点

143
00:04:34,483 --> 00:04:36,366
那整篇论文里面的 Architecture

144
00:04:36,366 --> 00:04:37,483
就说到了

145
00:04:37,650 --> 00:04:39,166
在 Attention 结构的时候

146
00:04:39,166 --> 00:04:40,683
我们设计了一个 MLA

147
00:04:40,850 --> 00:04:41,850
那这个 MLA

148
00:04:41,850 --> 00:04:43,733
最重要的就是使用了 lowRank

149
00:04:43,766 --> 00:04:46,683
低秩的 KV 联合压缩的方式

150
00:04:46,933 --> 00:04:48,966
使得我们整体在推理的时候

151
00:04:49,166 --> 00:04:51,800
节省了很多的 KV Cache 的显存

152
00:04:51,933 --> 00:04:55,450
有限的增加了推理的速度

153
00:04:55,533 --> 00:04:56,450
接着我们现在

154
00:04:56,450 --> 00:04:59,733
就正式的打开 2.1 的这个内容

155
00:04:59,733 --> 00:05:02,000
Boost inference efficient

156
00:05:02,166 --> 00:05:03,683
怎么去加速

157
00:05:03,683 --> 00:05:05,966
说实话在一开始你会发现

158
00:05:05,966 --> 00:05:08,966
其实在整个 KV 开始里面

159
00:05:08,966 --> 00:05:10,533
特别是推理的扩张中

160
00:05:10,533 --> 00:05:12,083
非常的占用显存

161
00:05:12,083 --> 00:05:12,533
于是

162
00:05:12,533 --> 00:05:15,450
大家就提出了各种各样的 MQA 也好

163
00:05:15,450 --> 00:05:16,650
GQA 也好

164
00:05:16,850 --> 00:05:18,450
还有各种各样的不同的算法

165
00:05:18,450 --> 00:05:20,766
但是这些算法效果都有限

166
00:05:20,800 --> 00:05:22,333
所以在 Deepseek V2 里面

167
00:05:22,333 --> 00:05:23,966
我们针对这个 Attention 结构

168
00:05:23,966 --> 00:05:25,483
就提出了一个低秩

169
00:05:25,483 --> 00:05:27,850
KV 的联合压缩的算法

170
00:05:28,200 --> 00:05:29,133
那 2.1.1

171
00:05:29,133 --> 00:05:32,366
就讲我们可能传统的 standard MHA

172
00:05:32,366 --> 00:05:34,400
它是怎么去实现

173
00:05:34,400 --> 00:05:36,933
那最重要的公式可能就是这一条了

174
00:05:36,933 --> 00:05:38,850
有了 QKV 的相乘

175
00:05:38,850 --> 00:05:41,000
最后加一个 softmax 就完成了

176
00:05:41,000 --> 00:05:42,366
那左边我们看一下这个图

177
00:05:42,366 --> 00:05:43,166
蛮有意思

178
00:05:43,400 --> 00:05:44,683
原来的 MHA

179
00:05:44,733 --> 00:05:48,250
每个 query 对应一个 k 一个 v 这种方式

180
00:05:48,250 --> 00:05:50,850
所以 QKV 的 TOKEN 是一一对应

181
00:05:50,850 --> 00:05:52,250
那有了 GQA

182
00:05:52,250 --> 00:05:54,600
这个 g 就是更多的是 group

183
00:05:54,600 --> 00:05:55,800
组成一个组

184
00:05:55,966 --> 00:05:57,733
怎么组成一个 group

185
00:05:57,733 --> 00:05:58,250
蛮有意思

186
00:05:58,250 --> 00:06:00,050
就是对 query

187
00:06:00,050 --> 00:06:03,083
q 两两的进行一个组组

188
00:06:03,283 --> 00:06:04,766
那组成一个组之后

189
00:06:04,766 --> 00:06:07,366
上面就交给我们独立的 k 跟 v

190
00:06:07,366 --> 00:06:08,283
去实现

191
00:06:08,283 --> 00:06:10,000
那这个时候我们可以看到

192
00:06:10,050 --> 00:06:12,766
整体的 KV 的显存是减少

193
00:06:12,933 --> 00:06:16,533
后来又有了一个 multi-query attention

194
00:06:16,533 --> 00:06:19,683
那这个 multi-query attention 就更加疯狂了

195
00:06:19,683 --> 00:06:23,083
把很多个 query 全部都组成一个 k

196
00:06:23,200 --> 00:06:25,333
跟 v 进行一个计算

197
00:06:25,333 --> 00:06:28,450
但是随着我们为了节省 KV Cache

198
00:06:28,600 --> 00:06:31,166
你会发现集成度越来越高

199
00:06:31,166 --> 00:06:32,733
但是集成度越来越高之后

200
00:06:32,733 --> 00:06:33,933
就导致一个问题

201
00:06:33,933 --> 00:06:37,450
模型的性能或者模型的效果就降低了

202
00:06:37,450 --> 00:06:37,800
于是

203
00:06:37,800 --> 00:06:40,483
就出现了我们最后一张图的内容

204
00:06:40,483 --> 00:06:42,250
就最后一张图里面

205
00:06:42,250 --> 00:06:45,133
就提出了一个 Multi head latent attention

206
00:06:45,133 --> 00:06:47,850
这个 latent 就是我们潜在的隐变量

207
00:06:47,850 --> 00:06:48,566
那通过

208
00:06:48,566 --> 00:06:50,966
一开始也是跟那个 MHA 一样

209
00:06:50,966 --> 00:06:52,333
但是这里面的 q

210
00:06:52,333 --> 00:06:53,533
或者 KV

211
00:06:53,533 --> 00:06:56,850
就衍生了一个隐变量的 latentn KV

212
00:06:56,933 --> 00:06:59,366
通过这类坦的 KV 映射回去

213
00:06:59,366 --> 00:07:00,050
那这个时候

214
00:07:00,050 --> 00:07:02,083
就有点类似于一个 LoRA

215
00:07:02,083 --> 00:07:03,933
或者一个低秩分解

216
00:07:03,933 --> 00:07:05,083
或者低秩微调

217
00:07:05,083 --> 00:07:07,483
我们把以前很大参数量的东西

218
00:07:07,483 --> 00:07:09,966
通过压缩成一个很小的向量

219
00:07:09,966 --> 00:07:11,000
进行一个计算

220
00:07:11,000 --> 00:07:12,366
或者进行一个存储

221
00:07:12,366 --> 00:07:13,250
那这种方式

222
00:07:13,250 --> 00:07:16,600
有效的去节省 KV 的显存

223
00:07:17,166 --> 00:07:18,600
我们继续往下看一下

224
00:07:18,600 --> 00:07:21,483
其实基本上后面的就是对应的公式了

225
00:07:21,483 --> 00:07:24,883
那这条公式最核心的就是 2.1.2

226
00:07:24,883 --> 00:07:26,250
这里面的内容

227
00:07:26,250 --> 00:07:30,400
那 2.1.2 最核心的就是标题 low Rank

228
00:07:30,400 --> 00:07:33,333
低秩的 KV 的联合压缩

229
00:07:33,533 --> 00:07:34,366
最核心

230
00:07:34,366 --> 00:07:37,650
这里面就是说到了 the core of MLA

231
00:07:37,650 --> 00:07:39,000
是 low Rank

232
00:07:39,000 --> 00:07:40,083
那为了实现 LoRA

233
00:07:40,083 --> 00:07:41,800
我们看一下下面这条公式

234
00:07:42,050 --> 00:07:44,083
HT 就是对我们整个网络模型

235
00:07:44,083 --> 00:07:46,133
或者对 attention 结构的输入

236
00:07:46,166 --> 00:07:48,566
通过一个低秩的一个矩阵

237
00:07:48,566 --> 00:07:50,850
我们叫做 wDKV

238
00:07:51,000 --> 00:07:54,733
然后变成我们一个压缩后的 CT KV

239
00:07:54,766 --> 00:07:56,166
这么一个隐向量

240
00:07:56,166 --> 00:07:57,683
或者压缩后的向量

241
00:07:57,683 --> 00:07:59,250
那压缩后的向量

242
00:07:59,250 --> 00:08:00,250
它的一个大小

243
00:08:00,250 --> 00:08:03,050
会比我们输入的大小要小很多

244
00:08:03,050 --> 00:08:03,766
所以现在

245
00:08:03,766 --> 00:08:07,850
我们才回过头来看一下这么一个图

246
00:08:08,133 --> 00:08:09,133
我们现在放大这个图

247
00:08:09,133 --> 00:08:09,766
可以看到

248
00:08:09,766 --> 00:08:13,083
整体的 input heater HT 是比较长

249
00:08:13,083 --> 00:08:15,250
也就是 heat size 是非常大

250
00:08:15,250 --> 00:08:17,566
那经过一个隐变量

251
00:08:17,566 --> 00:08:18,883
CT KV 之后

252
00:08:18,883 --> 00:08:19,683
我们可以发现

253
00:08:19,683 --> 00:08:22,133
它整体的 CT 的 litence 的一个长度

254
00:08:22,133 --> 00:08:24,250
或者里面的 Hidden size 的一个宽度

255
00:08:24,450 --> 00:08:27,166
会比 input 的时候要小了很多

256
00:08:27,166 --> 00:08:28,733
那通过这种方式

257
00:08:28,733 --> 00:08:30,683
去压缩整体的编码

258
00:08:30,683 --> 00:08:33,400
或者压缩 KV 的 Cache 的值

259
00:08:33,450 --> 00:08:34,650
那通过这种方式

260
00:08:34,650 --> 00:08:37,250
再去生成 k 跟 v

261
00:08:37,250 --> 00:08:37,933
另外的话

262
00:08:37,933 --> 00:08:40,800
我们还有一个另外的 KTR 来

263
00:08:40,800 --> 00:08:44,166
这个 TRR 了就代表我的 Rotary inbending

264
00:08:44,166 --> 00:08:46,366
旋转位置编码

265
00:08:46,366 --> 00:08:48,766
不过我们现在还没讲到旋转位置编码

266
00:08:48,766 --> 00:08:49,600
我们现在

267
00:08:49,600 --> 00:08:53,000
回到对应的一个具体的公式里面

268
00:08:53,733 --> 00:08:55,566
当然了我们还有 k 跟 v

269
00:08:55,566 --> 00:08:58,800
同样是按照以上方式去进行计算

270
00:08:58,800 --> 00:08:59,333
只不过

271
00:08:59,333 --> 00:09:02,333
我们现在输入就变成了一个 CT KV

272
00:09:02,333 --> 00:09:04,883
还有 CTKV 也是对应

273
00:09:04,883 --> 00:09:08,650
隐向量都是同一个 capacitor

274
00:09:08,766 --> 00:09:09,566
那接着

275
00:09:09,566 --> 00:09:10,483
我们现在

276
00:09:10,483 --> 00:09:13,400
为了去解决激活的问题

277
00:09:13,400 --> 00:09:14,533
因为在训练的时候

278
00:09:14,533 --> 00:09:16,600
激活会占用大量的参数

279
00:09:16,600 --> 00:09:19,050
所以这里面把一个 q

280
00:09:19,366 --> 00:09:21,400
也经过了一个压缩编码统一了

281
00:09:21,400 --> 00:09:22,733
变成一个 latent

282
00:09:22,800 --> 00:09:25,766
那虽然在查询的时候

283
00:09:25,766 --> 00:09:28,133
我们不会去减少 KV Cache

284
00:09:28,133 --> 00:09:29,333
因为在输入的时候

285
00:09:29,333 --> 00:09:30,250
q

286
00:09:30,250 --> 00:09:32,200
q 是随时的更新

287
00:09:32,200 --> 00:09:33,566
不过为了增加

288
00:09:33,566 --> 00:09:36,000
或者为了提升训练的速度

289
00:09:36,283 --> 00:09:38,450
就是减少一个激活的显存

290
00:09:38,483 --> 00:09:39,133
所以 q

291
00:09:39,133 --> 00:09:42,766
也同样的经过了一个相关的计算

292
00:09:42,766 --> 00:09:43,733
或者相关的压缩

293
00:09:43,733 --> 00:09:45,333
也就是我们左边的这个

294
00:09:45,333 --> 00:09:46,083
q 

295
00:09:46,083 --> 00:09:48,600
也经过一个 latent 的一个压缩

296
00:09:49,000 --> 00:09:50,933
kv 也经过一个 latent 的压缩

297
00:09:50,933 --> 00:09:52,966
我们现在来解读完这一层之后

298
00:09:52,966 --> 00:09:54,200
我们继续往下

299
00:09:54,200 --> 00:09:57,533
看一下下面的一个计算的公式

300
00:09:57,850 --> 00:10:00,000
那下面就讲到了 2.1.3

301
00:10:00,000 --> 00:10:04,250
就是解偶 Rotary inbending 旋转编码

302
00:10:04,250 --> 00:10:05,800
为什么要解偶旋转编码

303
00:10:05,800 --> 00:10:09,283
因为我们刚才要把 KV 就是 KV Cache

304
00:10:09,283 --> 00:10:10,250
其实实际上

305
00:10:10,250 --> 00:10:12,800
对一个位置是非常敏感

306
00:10:12,800 --> 00:10:15,600
所以我们叫做 position sensitive 

307
00:10:15,600 --> 00:10:17,566
那为了解决这个问题

308
00:10:17,566 --> 00:10:19,883
我们单独的去对 CT

309
00:10:20,166 --> 00:10:22,366
单独的去算 Rotary inbending

310
00:10:22,366 --> 00:10:24,050
然后得到一个 q 的值

311
00:10:24,050 --> 00:10:25,133
然后把 q 的值

312
00:10:25,133 --> 00:10:27,883
合并到我们刚才的一个 latent 里面

313
00:10:27,966 --> 00:10:29,050
同样

314
00:10:29,050 --> 00:10:32,650
k 的值也是经过单独的一个 Rotary inbending

315
00:10:32,650 --> 00:10:36,483
然后合进去 k 的一个变量里面

316
00:10:36,483 --> 00:10:38,883
最后经过 Softmax 之后

317
00:10:38,933 --> 00:10:43,050
就直接输出一个规划后的结果

318
00:10:43,050 --> 00:10:44,883
那这个就是最终的答案

319
00:10:44,883 --> 00:10:48,533
同样的我们回到上面的这个图

320
00:10:48,533 --> 00:10:49,683
可以看一看了

321
00:10:49,683 --> 00:10:51,600
那这里面我们讲到了 q

322
00:10:51,600 --> 00:10:53,800
我们会单独的一个 Rotary inbending

323
00:10:53,800 --> 00:10:55,400
我们会把 latent

324
00:10:55,400 --> 00:10:58,333
hidden size 单独组个 Rotary inbending

325
00:10:58,333 --> 00:11:01,850
然后跟刚才的 k 跟 V2 Rotary inbending

326
00:11:01,850 --> 00:11:03,083
进行一个 contact

327
00:11:03,366 --> 00:11:05,200
然后输给 KV Cache 进行一

328
00:11:05,200 --> 00:11:06,200
个计算

329
00:11:06,200 --> 00:11:07,283
同样的 q

330
00:11:07,283 --> 00:11:09,050
也是通过这种方式

331
00:11:09,050 --> 00:11:10,683
把整个 Rotary inbending

332
00:11:10,683 --> 00:11:12,450
跟刚才压缩后的 q

333
00:11:12,450 --> 00:11:13,566
进行一个 conk cat 之后

334
00:11:13,566 --> 00:11:15,166
统一进行计算

335
00:11:15,166 --> 00:11:16,366
最后就把 q

336
00:11:16,533 --> 00:11:19,000
KV 经过压缩

337
00:11:19,000 --> 00:11:21,200
跟 Rotray embending 的 concept 之后

338
00:11:21,366 --> 00:11:24,650
丢给 MHA 呀进行一个计算

339
00:11:24,683 --> 00:11:26,733
所以在 MHA 这个内容里面

340
00:11:26,733 --> 00:11:31,133
最核心的就是这么一块具体的计算

341
00:11:31,166 --> 00:11:32,450
当然在 MHA 呀

342
00:11:32,450 --> 00:11:34,333
在整个 fashiontation 里面

343
00:11:34,333 --> 00:11:36,850
就加入了很多分页的 KV 的缓存

344
00:11:36,850 --> 00:11:38,650
还有异步的内存拷贝

345
00:11:38,966 --> 00:11:41,933
还有一个双模式的相关的执行

346
00:11:41,966 --> 00:11:42,850
通过这种方式

347
00:11:42,850 --> 00:11:45,333
去实现了整个 MLA 的加速

348
00:11:45,333 --> 00:11:47,133
那我们这个具体的代码

349
00:11:47,133 --> 00:11:49,000
将会在后面进行展开

350
00:11:49,000 --> 00:11:49,766
我们还是

351
00:11:49,766 --> 00:11:51,650
回到这一篇论文里面

352
00:11:51,650 --> 00:11:52,533
那这篇论文

353
00:11:52,533 --> 00:11:53,683
其实蛮有意思

354
00:11:53,683 --> 00:11:54,883
就是在最后

355
00:11:54,883 --> 00:11:56,850
我们先不在这篇论文里面

356
00:11:56,850 --> 00:12:00,050
展开它的一个 MoE 相关的内容

357
00:12:00,050 --> 00:12:01,933
因为 MoE 不是我们今天关心

358
00:12:01,933 --> 00:12:02,850
而是 attention

359
00:12:02,850 --> 00:12:05,000
是我们今天所关心

360
00:12:05,000 --> 00:12:06,483
就是 MLA

361
00:12:06,533 --> 00:12:08,533
那整个在要里面

362
00:12:08,533 --> 00:12:09,533
特别是在复录

363
00:12:09,533 --> 00:12:11,000
我们现在往下翻

364
00:12:11,566 --> 00:12:12,966
在整个 package 里面

365
00:12:12,966 --> 00:12:15,883
这里面就想到了整个 formula

366
00:12:15,883 --> 00:12:17,483
一个 MLA1 的计算

367
00:12:17,483 --> 00:12:19,450
就是下面这一坨公式

368
00:12:19,450 --> 00:12:22,483
那基本上跟以前之前的 MHA 差不多

369
00:12:22,483 --> 00:12:23,083
那重点

370
00:12:23,083 --> 00:12:26,050
就是加了两个中间的隐层

371
00:12:26,050 --> 00:12:27,883
或者 latent 的向量

372
00:12:27,883 --> 00:12:28,883
CT

373
00:12:28,883 --> 00:12:30,400
KV 还有 KTR

374
00:12:30,683 --> 00:12:32,083
其他的基本上都一样

375
00:12:32,083 --> 00:12:34,650
也就是基本上算 QKV

376
00:12:34,650 --> 00:12:35,800
算完 QKV 之后

377
00:12:35,800 --> 00:12:37,566
给 Softmax 进行计算

378
00:12:37,566 --> 00:12:39,600
然后规划进行一个输出

379
00:12:39,650 --> 00:12:40,800
那我们接下来

380
00:12:40,800 --> 00:12:41,850
回到我们

381
00:12:41,850 --> 00:12:42,683
PPT 里面

382
00:12:43,850 --> 00:12:46,566
刚才跟大家简单的走读了一下

383
00:12:46,683 --> 00:12:48,966
Deepseek V2 里面的 MLA 的原理

384
00:12:48,966 --> 00:12:50,800
或者它的论文的相关的概况之后

385
00:12:50,800 --> 00:12:53,050
我们现在来到第五个内容

386
00:12:53,050 --> 00:12:55,483
看一下整个 MLA 的一个原理的概述

387
00:12:55,533 --> 00:12:56,366
说实话

388
00:12:56,366 --> 00:12:59,166
整个 MLA 其实最核心的思想了

389
00:12:59,166 --> 00:13:01,966
就是把一个 KV Cache 里面

390
00:13:01,966 --> 00:13:03,733
以前占用很大的空间

391
00:13:03,733 --> 00:13:05,650
现在我通过一个 latent

392
00:13:05,850 --> 00:13:07,283
一个潜在的空间变量了

393
00:13:07,283 --> 00:13:09,600
来去实现了一个整体的压缩

394
00:13:09,600 --> 00:13:12,050
然后再反压缩回来进行一个计算

395
00:13:12,050 --> 00:13:13,933
那最核心的就类似于 LoRA

396
00:13:13,933 --> 00:13:16,683
通过 low Rank 的方式去实现

397
00:13:17,333 --> 00:13:19,566
最重要的就展开这个图怎么去计算

398
00:13:19,566 --> 00:13:20,166
那这个图

399
00:13:20,166 --> 00:13:20,733
我们其实

400
00:13:20,733 --> 00:13:23,083
刚才已经跟大家简单的讲过了

401
00:13:23,083 --> 00:13:26,083
现在面就重点的去阐述一下

402
00:13:26,083 --> 00:13:26,800
那首先

403
00:13:26,800 --> 00:13:27,333
很核心

404
00:13:27,333 --> 00:13:29,366
我们要关注一个隐空间的变列

405
00:13:29,366 --> 00:13:30,333
就是 CTKV

406
00:13:30,400 --> 00:13:32,933
那 CT 就是我们输入的 HT

407
00:13:33,000 --> 00:13:34,283
也就是这个输入

408
00:13:34,283 --> 00:13:36,766
一个低秩的投影的向量

409
00:13:36,766 --> 00:13:37,483
那长度

410
00:13:37,483 --> 00:13:40,133
绝对会比 HT 要小很多

411
00:13:40,133 --> 00:13:40,766
那这里面

412
00:13:40,766 --> 00:13:42,483
很重要的就是 CDKV

413
00:13:42,483 --> 00:13:44,966
是所有的 head 是共享

414
00:13:44,966 --> 00:13:46,766
也就是我们整个 head

415
00:13:46,766 --> 00:13:48,566
是所有共享都是 CKV

416
00:13:48,650 --> 00:13:50,850
所以我们整个 MHA 里面

417
00:13:50,966 --> 00:13:53,850
本来是需要缓存多个变量

418
00:13:53,850 --> 00:13:56,250
第一个就是 k 跟 v

419
00:13:56,366 --> 00:13:57,050
那现在

420
00:13:57,050 --> 00:13:59,366
我们只需要缓存 latent

421
00:13:59,366 --> 00:14:01,133
CT 就基本上可以了

422
00:14:01,133 --> 00:14:03,600
以前重要的是缓存两个大的张量

423
00:14:03,600 --> 00:14:04,050
那现在

424
00:14:04,050 --> 00:14:06,733
我们只需要缓存一个压缩后

425
00:14:06,733 --> 00:14:07,566
小空间

426
00:14:07,566 --> 00:14:08,333
或者低维

427
00:14:08,333 --> 00:14:10,566
一个 CT 的一个向量就可以了

428
00:14:10,566 --> 00:14:13,283
那节省了很多的内存的空间

429
00:14:13,366 --> 00:14:16,333
那第二个就是为了去适配 LP

430
00:14:16,333 --> 00:14:17,933
就是 Rotary in bending

431
00:14:18,050 --> 00:14:20,200
一个旋转向量编码

432
00:14:20,200 --> 00:14:22,366
因为在文字里面或者 NLP 里面

433
00:14:22,366 --> 00:14:24,400
TOKEN 之间的依赖关系

434
00:14:24,400 --> 00:14:25,483
非常的重要

435
00:14:25,483 --> 00:14:28,050
所以需要加入一个 Rotary in bending

436
00:14:28,050 --> 00:14:30,166
也就是一个 ported possession

437
00:14:30,166 --> 00:14:32,483
位置编码的一个内容

438
00:14:32,483 --> 00:14:34,450
那现在所有的 head

439
00:14:34,450 --> 00:14:36,883
实际上是共享一个 KTR

440
00:14:36,883 --> 00:14:38,333
并且在世纪的时候

441
00:14:38,333 --> 00:14:40,333
让整个 WKR 的列数

442
00:14:40,333 --> 00:14:41,483
Dr 也比较小

443
00:14:41,650 --> 00:14:43,250
从而实现一个 Position

444
00:14:43,250 --> 00:14:44,766
Embedding 的一个压缩

445
00:14:44,766 --> 00:14:47,083
那我们可以看到这里面的一个

446
00:14:47,083 --> 00:14:49,600
颜色圈圈的颜色不太一样

447
00:14:49,600 --> 00:14:50,800
那有圈圈的颜色

448
00:14:50,800 --> 00:14:52,283
就代表 Cache

449
00:14:52,283 --> 00:14:55,766
是真正的缓存起来的整体的 MLA

450
00:14:55,766 --> 00:14:59,650
实际上是采用了类似于 MQA 的思想

451
00:14:59,650 --> 00:15:01,450
而构建了所有的 head

452
00:15:01,683 --> 00:15:03,600
一个共享的 Cache 的变量

453
00:15:03,600 --> 00:15:04,966
那这两个 Cache 的变量

454
00:15:04,966 --> 00:15:08,083
就是我们刚才框框的两个内容

455
00:15:08,083 --> 00:15:09,850
这样才能大幅的降低了

456
00:15:09,850 --> 00:15:14,083
以前需要缓存大量的 k 跟 v 相关的 Cache

457
00:15:14,083 --> 00:15:16,333
我们现在只需要缓存 k

458
00:15:16,333 --> 00:15:19,683
r 跟 CT 相关的缓存就可以了

459
00:15:19,683 --> 00:15:20,250
这两个

460
00:15:20,250 --> 00:15:22,766
是所有的 head 进行一个共享

461
00:15:23,050 --> 00:15:24,683
那了解完我们刚才这个图之后

462
00:15:24,683 --> 00:15:26,333
我们看一下对应的公式

463
00:15:26,333 --> 00:15:29,000
跟图怎么是一一对应起来

464
00:15:29,366 --> 00:15:30,000
那我们现在

465
00:15:30,000 --> 00:15:32,166
很重要的就是每个 Transformer 成了

466
00:15:32,166 --> 00:15:34,883
现在就是根据我们刚才讲到

467
00:15:34,883 --> 00:15:38,566
只缓存蓝色方框的两个具体的向量

468
00:15:38,683 --> 00:15:42,683
CTK 跟 KTR 那 c k 整个维度

469
00:15:42,683 --> 00:15:44,050
在 Deepseek V2 里面

470
00:15:44,050 --> 00:15:44,933
DC 等于 4

471
00:15:44,933 --> 00:15:47,133
乘以 DH 等于 512KT

472
00:15:47,133 --> 00:15:49,050
维度也就更小了

473
00:15:49,166 --> 00:15:51,133
我们看一下里面的两个变量了

474
00:15:51,133 --> 00:15:53,000
DC 就是一个 mad 字

475
00:15:53,000 --> 00:15:54,600
压缩的具体的维度

476
00:15:54,600 --> 00:15:56,683
所以说 CT 现在变得很小

477
00:15:56,683 --> 00:15:58,133
而 KT

478
00:15:58,133 --> 00:16:00,333
就是经过 embending 里面

479
00:16:00,333 --> 00:16:02,533
也变得更加的小了

480
00:16:02,533 --> 00:16:04,250
也要对应我们这个图

481
00:16:04,483 --> 00:16:06,483
图里面的 KT 的一个销量很小

482
00:16:06,483 --> 00:16:09,883
然后 CTKR 也非常的小

483
00:16:09,933 --> 00:16:11,333
而原来

484
00:16:11,366 --> 00:16:12,450
原来有多大了

485
00:16:12,450 --> 00:16:13,933
我们简单的算一条数

486
00:16:13,933 --> 00:16:16,333
基本上就是 CDKV 是

487
00:16:16,333 --> 00:16:18,683
原来的 KV 是 7,000 多

488
00:16:18,683 --> 00:16:19,483
非常的夸张

489
00:16:19,483 --> 00:16:21,883
我们现在已经压缩到了 512 了

490
00:16:21,883 --> 00:16:23,133
那我们接下来来看一下

491
00:16:23,133 --> 00:16:24,450
整体的一个计算

492
00:16:24,450 --> 00:16:28,400
那这里面很重要的就公式的 37 跟 38

493
00:16:28,483 --> 00:16:29,883
其实整体的逻辑

494
00:16:29,883 --> 00:16:30,883
类似于 KV

495
00:16:30,933 --> 00:16:32,600
通过两个矩阵

496
00:16:32,600 --> 00:16:35,283
两个 w 矩阵

497
00:16:35,283 --> 00:16:37,166
做一层低秩的变换

498
00:16:37,166 --> 00:16:37,933
那这一步

499
00:16:37,933 --> 00:16:40,483
最重要的就是变换成为 CTK

500
00:16:40,600 --> 00:16:43,200
然后对 q 进行 concat 到一起

501
00:16:43,200 --> 00:16:45,166
把它组合起来

502
00:16:45,400 --> 00:16:45,850
那这一步

503
00:16:45,850 --> 00:16:47,883
最重要的就是对输入

504
00:16:47,883 --> 00:16:49,600
q 进行处理

505
00:16:49,600 --> 00:16:50,800
所以我们可以看到

506
00:16:50,800 --> 00:16:52,000
37 跟 38 了

507
00:16:52,000 --> 00:16:55,166
都是对了进行处理

508
00:16:55,166 --> 00:16:57,566
但是最核心的主要是算 KV

509
00:16:57,650 --> 00:16:59,483
所以我们现在往下看一下

510
00:16:59,483 --> 00:17:00,450
这家公司里面

511
00:17:00,450 --> 00:17:02,400
最核心的就是 41 了

512
00:17:02,400 --> 00:17:04,483
就算 CDKV41

513
00:17:04,483 --> 00:17:07,366
就对具体的输入黑灯 size HT

514
00:17:07,366 --> 00:17:08,650
作为低秩的压缩

515
00:17:08,650 --> 00:17:09,966
将低的维度

516
00:17:10,000 --> 00:17:13,166
经过一个 WDKV 变换之后

517
00:17:13,166 --> 00:17:16,850
压缩成为 DC 维度的一个 CT KV

518
00:17:16,933 --> 00:17:18,283
那 DeepSeek V3 里面

519
00:17:18,283 --> 00:17:20,050
d 是 7,000 多

520
00:17:20,050 --> 00:17:21,566
而 DC 是 512

521
00:17:21,566 --> 00:17:23,000
所以整体压缩的维度

522
00:17:23,000 --> 00:17:25,600
基本上压缩了 14 倍

523
00:17:25,600 --> 00:17:26,883
非常的夸张

524
00:17:27,450 --> 00:17:29,533
接下来我们看一下下面两条公式

525
00:17:29,533 --> 00:17:30,650
第一条就 42

526
00:17:30,650 --> 00:17:31,883
还有 45

527
00:17:31,883 --> 00:17:34,566
那两条公式都是两个具体的变化

528
00:17:34,650 --> 00:17:37,333
根据我们刚才的 41 的公式

529
00:17:37,333 --> 00:17:39,733
将 KV 的维度扩展回去

530
00:17:39,733 --> 00:17:40,483
d 也是

531
00:17:40,483 --> 00:17:41,166
每个 head

532
00:17:41,166 --> 00:17:42,933
有单独的自己的一个 k

533
00:17:43,050 --> 00:17:45,083
跟自己单独的一个 v

534
00:17:45,650 --> 00:17:47,766
通过我们刚才压缩后的一个向量

535
00:17:47,766 --> 00:17:48,800
扩展回去

536
00:17:48,800 --> 00:17:49,650
那这种方式

537
00:17:49,650 --> 00:17:50,450
非常类似于

538
00:17:50,450 --> 00:17:52,933
一个 LoRA 的一个实现的方式

539
00:17:52,933 --> 00:17:53,450
只不过

540
00:17:53,450 --> 00:17:54,933
我们整体的 k

541
00:17:54,966 --> 00:17:56,533
会把它 concat 到一起

542
00:17:56,533 --> 00:17:58,483
把我们刚才得到的一个 KT

543
00:17:58,600 --> 00:17:59,850
然后 v

544
00:17:59,850 --> 00:18:00,650
基本上

545
00:18:00,850 --> 00:18:03,050
压缩回去就等于这个 v 了

546
00:18:03,050 --> 00:18:03,883
那最后

547
00:18:03,883 --> 00:18:06,800
就是一个 softmax 的一个具体的计算了

548
00:18:07,800 --> 00:18:08,166
所以说

549
00:18:08,166 --> 00:18:08,600
基本上

550
00:18:08,600 --> 00:18:09,650
就没有太多秘密

551
00:18:09,650 --> 00:18:12,533
更多的是对一个加了一个 latent

552
00:18:12,566 --> 00:18:14,283
所以叫做 MLA

553
00:18:14,283 --> 00:18:16,133
不是叫 MHA 了

554
00:18:16,133 --> 00:18:16,733
那这里面

555
00:18:16,733 --> 00:18:19,166
刚才讲到的漏了一个位置编码

556
00:18:19,166 --> 00:18:20,200
Rotary inbending

557
00:18:20,200 --> 00:18:22,283
这两个位置编码的方式

558
00:18:22,366 --> 00:18:24,050
位置编码也是单独

559
00:18:24,050 --> 00:18:25,800
q 是单独的一个位置编码

560
00:18:25,800 --> 00:18:26,733
然后 k

561
00:18:26,733 --> 00:18:28,766
也是单独的一个位置编码

562
00:18:28,766 --> 00:18:31,250
两个内容进行一个合起核算

563
00:18:31,283 --> 00:18:31,883
那最后

564
00:18:31,883 --> 00:18:33,933
就把所有东西都 concat 到一起

565
00:18:33,933 --> 00:18:35,050
就把位置编码来

566
00:18:35,050 --> 00:18:36,400
跟 q concat 到一起

567
00:18:36,400 --> 00:18:39,766
还有 44 行把 k 跟 demending

568
00:18:39,766 --> 00:18:42,050
所以有 k 的一个上标是 r

569
00:18:42,333 --> 00:18:44,250
这个 q 的上标也是

570
00:18:44,250 --> 00:18:45,283
然后 concat 到一起

571
00:18:45,283 --> 00:18:48,166
然后给到我们最后的 softmax 进行计算

572
00:18:48,650 --> 00:18:49,566
那基本上我们

573
00:18:49,566 --> 00:18:52,650
现在已经了解完整个 MLA 的原理了

574
00:18:52,650 --> 00:18:54,933
下面我们来到一个最难的地方了

575
00:18:54,933 --> 00:18:56,966
或者我们今天的最核心的地方

576
00:18:56,966 --> 00:19:00,883
整个 DeepSeek 第一天所发布的一个代码仓

577
00:19:01,883 --> 00:19:04,050
我们现在来到了第三个内容

578
00:19:04,050 --> 00:19:05,883
大家可能会非常的关心

579
00:19:05,883 --> 00:19:07,683
ZOMI 也是尽我所能

580
00:19:07,683 --> 00:19:10,083
在可能今天晚上也就腾了

581
00:19:10,083 --> 00:19:11,850
大概一个半小时的时间

582
00:19:11,850 --> 00:19:13,400
去解读了一下这个代码

583
00:19:13,400 --> 00:19:14,933
那加了一些相关的注释

584
00:19:14,933 --> 00:19:16,133
我们现在来看一下

585
00:19:16,133 --> 00:19:19,050
Flash attention 相关的一个具体的内容

586
00:19:19,483 --> 00:19:20,333
首先核心

587
00:19:20,333 --> 00:19:22,250
就是我们要打开这一条链接

588
00:19:22,250 --> 00:19:24,283
就是 Deepseek 压的一个 Flash attention

589
00:19:24,283 --> 00:19:25,683
里面的 issue 有 26 个

590
00:19:25,683 --> 00:19:28,000
整个还有 6 个还没有上传

591
00:19:28,000 --> 00:19:30,800
那蛮有意思的就是整个大数是 7.3 k

592
00:19:30,800 --> 00:19:32,366
我们是半小时刷

593
00:19:32,366 --> 00:19:33,850
我们现在简单的刷一下

594
00:19:33,850 --> 00:19:36,766
哎呀已经一小时过后已经变成 7.4 k 了

595
00:19:36,766 --> 00:19:38,000
非常的夸张

596
00:19:38,000 --> 00:19:39,366
我最高的一个

597
00:19:39,366 --> 00:19:41,533
或者现在一个给大家分享的目录

598
00:19:41,533 --> 00:19:43,733
一个 up 的差

599
00:19:43,733 --> 00:19:45,400
也就只有一点多 k

600
00:19:45,400 --> 00:19:47,533
我们看一下它整体的 language 的比例

601
00:19:47,533 --> 00:19:48,050
蛮有意思

602
00:19:48,050 --> 00:19:50,050
就是 C++88.6%

603
00:19:50,450 --> 00:19:52,600
python 只占了 11.2%

604
00:19:52,600 --> 00:19:53,850
也就是 py

605
00:19:53,850 --> 00:19:55,600
还有相关的用例了

606
00:19:55,600 --> 00:19:56,400
非常的少

607
00:19:56,400 --> 00:19:58,283
CUDA 的代码占了 0.2 了

608
00:19:58,283 --> 00:19:59,733
更多的是 C++的代码

609
00:19:59,733 --> 00:20:01,683
为什么 C++的代码比较多

610
00:20:01,683 --> 00:20:03,600
不应该是 CUDA 的代码比较多了

611
00:20:03,600 --> 00:20:07,933
因为整个仓最核心的就是 Flash MLA

612
00:20:07,933 --> 00:20:09,933
使用的是英伟达提供

613
00:20:09,933 --> 00:20:12,283
一个 cutlass 的一个相关的库

614
00:20:12,933 --> 00:20:13,883
或者 API

615
00:20:13,883 --> 00:20:17,283
就 cutlass 实现的一个工作

616
00:20:17,283 --> 00:20:19,133
所以接下来代码会比较多

617
00:20:19,133 --> 00:20:19,850
我们简单的看一

618
00:20:19,850 --> 00:20:21,283
下它整体的 readme

619
00:20:21,283 --> 00:20:23,800
这里面就说到了用了一个 BF16

620
00:20:23,800 --> 00:20:26,200
还有一个 Paged 给 KV Cache

621
00:20:26,200 --> 00:20:29,133
然后 block 是 64 来去实现

622
00:20:29,133 --> 00:20:31,133
整体安装比较简单

623
00:20:31,133 --> 00:20:32,733
为什么 ZOMI 安装不了了

624
00:20:32,733 --> 00:20:36,083
因为它在一个 Hopper 架构里面去实现

625
00:20:36,083 --> 00:20:39,366
所以说 ZOMI 现在手上没有相关资源

626
00:20:39,366 --> 00:20:41,800
我手上只有升腾的机器

627
00:20:41,966 --> 00:20:43,533
想要用升腾的机器的人

628
00:20:43,533 --> 00:20:44,533
也可以问问我

629
00:20:44,533 --> 00:20:46,933
可能会有相关不同的特殊的渠道

630
00:20:56,800 --> 00:20:58,366
那我们现在回到这里面了

631
00:20:58,450 --> 00:21:00,250
整体的 Benchmark 比较好实现

632
00:21:00,250 --> 00:21:02,933
然后最重要的就算他说

633
00:21:02,933 --> 00:21:05,450
300GBPS 的一个 memory bound

634
00:21:05,450 --> 00:21:07,250
跟一个计算 bound

635
00:21:07,333 --> 00:21:09,366
整体来说是非常的夸张

636
00:21:09,366 --> 00:21:11,600
反正把我们单卡里面带宽

637
00:21:11,600 --> 00:21:16,050
还有一个嗯计算的 MFU

638
00:21:16,050 --> 00:21:17,083
打上去了

639
00:21:17,083 --> 00:21:18,166
做的非常的好

640
00:21:18,166 --> 00:21:20,000
那使用起来也就比较简单

641
00:21:20,000 --> 00:21:21,000
两个内容

642
00:21:21,000 --> 00:21:21,650
在 Python 层

643
00:21:21,650 --> 00:21:25,766
第一个就是获取 MLA 的一个 metadata

644
00:21:25,800 --> 00:21:28,650
也就是获取 ML 相关的一些信息

645
00:21:28,650 --> 00:21:29,933
特别是 tell 的信息

646
00:21:29,933 --> 00:21:30,883
还有 number split

647
00:21:30,883 --> 00:21:33,600
我们需要根据现在的一个序列的长度

648
00:21:33,600 --> 00:21:36,133
还有我们需要处理的数据的大小

649
00:21:36,133 --> 00:21:37,800
进行一个切分

650
00:21:38,000 --> 00:21:38,800
那切分完之后

651
00:21:38,800 --> 00:21:40,800
我们就正式的进行一个计算了

652
00:21:40,800 --> 00:21:43,733
就把 tile schedule meter data

653
00:21:43,733 --> 00:21:46,333
还有 number split 分为多少个块

654
00:21:46,333 --> 00:21:47,966
就分页的情况

655
00:21:47,966 --> 00:21:49,283
还有分块的情况

656
00:21:49,283 --> 00:21:51,766
给到 Flash MLA 的一个 keep case

657
00:21:51,766 --> 00:21:52,800
进行计算

658
00:21:52,800 --> 00:21:53,600
那可以看到了

659
00:21:53,600 --> 00:21:54,333
我们后面

660
00:21:54,333 --> 00:21:57,533
就正式的去打开对应的代码了

661
00:21:57,733 --> 00:22:00,050
现在我们打开一个 vs code

662
00:22:00,050 --> 00:22:01,650
点一下 readme

663
00:22:01,650 --> 00:22:03,650
我们就刚才看的一个内容嘛

664
00:22:03,650 --> 00:22:05,600
那整个工程目录

665
00:22:05,600 --> 00:22:06,650
就比较简单

666
00:22:06,650 --> 00:22:07,650
一个 text

667
00:22:07,650 --> 00:22:09,683
text 就是整个 MLA

668
00:22:09,683 --> 00:22:10,800
怎么去跑

669
00:22:10,800 --> 00:22:13,766
里面就给出了一个具体的用例

670
00:22:14,133 --> 00:22:16,733
这个用例就是把那个 MHA

671
00:22:16,733 --> 00:22:18,733
跟 MLA 进行了一个对比

672
00:22:18,733 --> 00:22:20,133
所以里面就有一个函数

673
00:22:20,133 --> 00:22:22,933
就叫做 CLA

674
00:22:23,000 --> 00:22:24,850
把 MHA 跟 MA

675
00:22:24,850 --> 00:22:26,250
进行一个对比

676
00:22:26,250 --> 00:22:29,400
然后看一下它们两个输出的一个误差

677
00:22:29,766 --> 00:22:31,966
text 我觉得可能不是我们最关心

678
00:22:31,966 --> 00:22:33,800
那我们看一下 Flash MLA

679
00:22:33,883 --> 00:22:36,200
这里面是一个 Python 的内容

680
00:22:36,200 --> 00:22:37,050
那 Python 的内容

681
00:22:37,050 --> 00:22:38,650
就我们刚才提到

682
00:22:38,650 --> 00:22:41,366
两个很核心的一个函数

683
00:22:41,366 --> 00:22:44,083
一个是 get MLA 的一个 metadata

684
00:22:44,166 --> 00:22:47,450
另外一个是 Flash MLA with KV Cache

685
00:22:47,450 --> 00:22:49,050
那 Flash MLA with KV Cache

686
00:22:49,050 --> 00:22:52,166
就是最核心的一个具体的计算了

687
00:22:52,166 --> 00:22:53,483
不过这里面的计算

688
00:22:53,483 --> 00:22:55,566
依赖的是 Flash MLA 杠

689
00:22:55,650 --> 00:22:58,933
扩大里面的 forward KV Cache 的 MLA

690
00:22:59,133 --> 00:23:00,600
那这里面最核心

691
00:23:00,600 --> 00:23:02,800
就是里面的这个内容了

692
00:23:02,850 --> 00:23:05,733
csrc 那里面

693
00:23:05,733 --> 00:23:07,483
就有 6 个文件

694
00:23:07,483 --> 00:23:08,966
Flash 的 API

695
00:23:08,966 --> 00:23:11,566
还有 Flash 的 forer MLA 的一个 BF16

696
00:23:11,566 --> 00:23:14,166
xxxx

697
00:23:14,483 --> 00:23:16,450
那这里面的所有的东西

698
00:23:16,450 --> 00:23:18,533
最核心的其实是在这里面

699
00:23:18,566 --> 00:23:22,333
这里面去实现

700
00:23:22,333 --> 00:23:23,200
所以整个代码

701
00:23:23,200 --> 00:23:25,050
这里面有 700 多行

702
00:23:25,050 --> 00:23:25,533
那 ZOMI

703
00:23:25,533 --> 00:23:27,283
加了很多相关的注释

704
00:23:27,283 --> 00:23:28,800
如果有需要的朋友

705
00:23:28,800 --> 00:23:30,766
也可以去 ZOMI 的一个夸克网盘

706
00:23:30,766 --> 00:23:31,850
去下载

707
00:23:31,850 --> 00:23:32,800
嗯免费把注

708
00:23:32,800 --> 00:23:34,333
释的都给你

709
00:23:35,450 --> 00:23:38,966
还有个 Flash MLA 定义了相关的数据结构

710
00:23:38,966 --> 00:23:41,083
还有一些工具的函数

711
00:23:41,083 --> 00:23:43,566
最核心的就是相关的数据结构

712
00:23:43,566 --> 00:23:46,366
都在一个 Flash MLA 里面去实现

713
00:23:46,366 --> 00:23:48,000
那

714
00:23:48,000 --> 00:23:49,533
是主要是用来做同步

715
00:23:49,533 --> 00:23:51,600
两个具体的状态

716
00:23:51,600 --> 00:23:54,166
所以说没有太多的其他内容了

717
00:23:54,166 --> 00:23:55,650
那 Softmax.h

718
00:23:55,650 --> 00:23:56,083
更多

719
00:23:56,083 --> 00:24:00,083
就是针对 Softmax 相关的一个计算

720
00:24:00,083 --> 00:24:02,333
因为整个 Softmax 嗯比较大

721
00:24:02,333 --> 00:24:04,800
所以说里面内部做了一些 reduce

722
00:24:04,800 --> 00:24:06,083
跟一个聚合

723
00:24:06,483 --> 00:24:09,133
还有 overdue 相关的一个具体的操作

724
00:24:09,133 --> 00:24:11,400
那这些操作其实并不难

725
00:24:11,400 --> 00:24:13,450
也不算非常的核心

726
00:24:13,450 --> 00:24:16,050
更重要的是对一个这个 reduce

727
00:24:16,050 --> 00:24:17,450
不是多卡的 reduce

728
00:24:17,450 --> 00:24:19,050
而单卡的 reduced

729
00:24:19,250 --> 00:24:20,366
所以大家要注意一下来

730
00:24:20,366 --> 00:24:22,133
就是单也不是单卡来

731
00:24:22,133 --> 00:24:24,366
就单节点的一个 reduce 的方式

732
00:24:24,366 --> 00:24:25,483
进行一个聚合

733
00:24:25,483 --> 00:24:26,566
充分的去利用了

734
00:24:26,566 --> 00:24:28,850
整个网络的带宽

735
00:24:28,883 --> 00:24:31,733
就在不同的 SM 不同的线程处里面

736
00:24:31,733 --> 00:24:33,050
进行一个 reduce

737
00:24:33,050 --> 00:24:36,450
还有一个 static

738
00:24:36,450 --> 00:24:37,733
其实也没有太多内容

739
00:24:37,733 --> 00:24:40,566
更多的是提供了一些定义

740
00:24:40,766 --> 00:24:42,966
那这些定义其实比较简单

741
00:24:42,966 --> 00:24:46,133
也是可能一些规范化的一些函数

742
00:24:46,133 --> 00:24:47,683
那最后还有一些

743
00:24:47,683 --> 00:24:48,966
那这些

744
00:24:48,966 --> 00:24:51,566
就是里面提供了一个工具

745
00:24:51,566 --> 00:24:53,400
相关的简单的计算

746
00:24:53,400 --> 00:24:55,650
例如分块后的 GEMM

747
00:24:55,650 --> 00:24:58,400
我们把一个具体分块完之后的成

748
00:24:58,400 --> 00:25:01,133
都丢给这个 GEMM 里面去实现

749
00:25:01,133 --> 00:25:02,200
还有一些可能

750
00:25:02,200 --> 00:25:04,166
数据格式的转换

751
00:25:04,166 --> 00:25:06,400
还有张亮的 layout 的布局

752
00:25:06,400 --> 00:25:08,200
就 reshape 相关的内容

753
00:25:08,200 --> 00:25:10,166
还有可能转 bf16

754
00:25:10,166 --> 00:25:12,800
还有数据类型的格式的转换

755
00:25:12,800 --> 00:25:14,800
都在一个里面

756
00:25:14,800 --> 00:25:15,800
那我们现在

757
00:25:15,800 --> 00:25:18,766
从第一个函数的入口进行打开

758
00:25:18,800 --> 00:25:19,483
那这里面

759
00:25:19,483 --> 00:25:20,650
我 ZOMI 觉得蛮有意思

760
00:25:20,650 --> 00:25:23,133
就是看一下最底下的是

761
00:25:23,133 --> 00:25:23,933
那

762
00:25:23,933 --> 00:25:26,000
就是把 C++的代码

763
00:25:26,483 --> 00:25:27,450
进行一个封装

764
00:25:27,450 --> 00:25:30,333
成为 Python 可以调的一个 API 的接口

765
00:25:30,333 --> 00:25:31,733
所以我们刚才看到

766
00:25:31,733 --> 00:25:33,850
整个 Flash MLA 里面

767
00:25:33,850 --> 00:25:35,566
这里面的对外的 INTERFACE

768
00:25:35,566 --> 00:25:36,933
提供的 API 的接口

769
00:25:37,000 --> 00:25:37,566
实际上

770
00:25:37,566 --> 00:25:40,533
是由这里面去提供的两个

771
00:25:40,533 --> 00:25:41,333
那这两个

772
00:25:41,333 --> 00:25:42,533
干了哪些工作了

773
00:25:42,533 --> 00:25:44,850
我们逐个的去看一下

774
00:25:45,166 --> 00:25:45,933
整个最核心

775
00:25:45,933 --> 00:25:48,250
就 MLA 的一个 metedata

776
00:25:48,250 --> 00:25:50,483
我们简单的再放大看一看

777
00:25:50,533 --> 00:25:51,766
整个 metedata

778
00:25:51,766 --> 00:25:54,083
就获取

779
00:25:54,133 --> 00:25:56,000
相关的函数

780
00:25:56,000 --> 00:25:57,966
用来对我们在计算之前

781
00:25:57,966 --> 00:26:02,450
进行分配完所有相关的一些嗯参数了

782
00:26:02,450 --> 00:26:03,483
就是方便

783
00:26:03,483 --> 00:26:05,966
我们在 Hopper 的架构里面的时候

784
00:26:05,966 --> 00:26:08,683
更好的对一些参数进行预分配

785
00:26:08,683 --> 00:26:10,133
或者对

786
00:26:10,133 --> 00:26:11,533
还有内存地址

787
00:26:11,533 --> 00:26:13,250
进行全面的预分配

788
00:26:13,250 --> 00:26:14,000
所以这里面

789
00:26:14,000 --> 00:26:16,400
就做了很多相关的工作

790
00:26:16,400 --> 00:26:17,766
还有相关的显存

791
00:26:18,050 --> 00:26:19,850
或者相关的 metadata

792
00:26:19,850 --> 00:26:20,733
一个预分配

793
00:26:20,733 --> 00:26:22,600
然后就调用内核

794
00:26:22,600 --> 00:26:25,166
去获取相关的一个 metadata

795
00:26:25,166 --> 00:26:26,566
然后再返回回来

796
00:26:26,566 --> 00:26:27,200
那这里面

797
00:26:27,200 --> 00:26:28,883
更多的是预工作

798
00:26:28,883 --> 00:26:30,050
那真正的工作

799
00:26:30,050 --> 00:26:31,800
是在一个

800
00:26:31,800 --> 00:26:34,650
MHA forward KV 一个 MLA

801
00:26:35,250 --> 00:26:36,850
那蛮有意思的就是这里面

802
00:26:36,850 --> 00:26:39,333
只提供了一个前向的传播的公式

803
00:26:39,333 --> 00:26:41,650
用于高效的去计算 MLA

804
00:26:41,850 --> 00:26:44,533
那真正的这里面的算了算的内容

805
00:26:44,533 --> 00:26:45,400
就在下面

806
00:26:45,400 --> 00:26:47,250
会调用这个 Flash forward

807
00:26:47,250 --> 00:26:49,650
MLA Kernel 这么一个函数

808
00:26:49,650 --> 00:26:51,800
所以说整个这个是对外的 API

809
00:26:51,800 --> 00:26:53,450
这个是核心的函数

810
00:26:53,450 --> 00:26:54,533
而一进来

811
00:26:54,533 --> 00:26:55,566
进来就蛮有意思

812
00:26:55,566 --> 00:26:58,366
就是去检查是不是 SM90

813
00:26:58,400 --> 00:26:59,650
如果是 SM90

814
00:26:59,650 --> 00:27:02,883
那 90 就是对应的 Hopper 架构里面

815
00:27:02,883 --> 00:27:04,166
具体的一个指令

816
00:27:04,166 --> 00:27:07,250
或者是整个英伟达的一个 cutlass

817
00:27:07,283 --> 00:27:08,566
一个具体的指令了

818
00:27:08,566 --> 00:27:09,483
所以我们可以看到

819
00:27:09,483 --> 00:27:09,966
这里面

820
00:27:09,966 --> 00:27:12,933
是的一个 Flash 的 math

821
00:27:12,933 --> 00:27:16,050
还有很多相关的一些 CUDA

822
00:27:16,050 --> 00:27:17,850
还有一个

823
00:27:17,966 --> 00:27:21,283
相关的 API

824
00:27:21,683 --> 00:27:22,566
不过没关系

825
00:27:22,566 --> 00:27:23,650
其实大家都是

826
00:27:23,650 --> 00:27:25,083
主要是利用了一个相关

827
00:27:25,083 --> 00:27:26,566
张量的一些定义

828
00:27:26,683 --> 00:27:27,766
那我们现在

829
00:27:27,766 --> 00:27:30,133
回到这一条函数里面

830
00:27:30,133 --> 00:27:33,050
一开始就去检查我们

831
00:27:33,050 --> 00:27:34,883
是否为 Hopper 架构了

832
00:27:34,883 --> 00:27:36,883
所以它专门针对的是 H100

833
00:27:37,083 --> 00:27:38,566
H800 深度的绑定

834
00:27:38,566 --> 00:27:40,083
进行一个加速

835
00:27:40,083 --> 00:27:42,600
那如果没有提供一个 v Cache

836
00:27:42,600 --> 00:27:43,600
那 KV Cache

837
00:27:43,600 --> 00:27:46,366
保持一致的一个向量的大小

838
00:27:46,366 --> 00:27:48,050
然后检查张量的输入

839
00:27:48,050 --> 00:27:49,450
数据的一个类型

840
00:27:49,450 --> 00:27:50,283
所以你会发现

841
00:27:50,283 --> 00:27:53,250
很多 touch check 相关的函数

842
00:27:53,250 --> 00:27:54,800
那了解完这些信息之后

843
00:27:54,800 --> 00:27:56,533
就输张量的信息了

844
00:27:56,533 --> 00:27:57,933
那这些张量的信息

845
00:27:57,933 --> 00:28:01,533
都是通过刚才的 metadata 获取

846
00:28:01,533 --> 00:28:03,766
相关的函数里面去试计

847
00:28:04,200 --> 00:28:07,000
了解完后的获选相关的数据之后

848
00:28:07,000 --> 00:28:08,333
下面就正式

849
00:28:08,333 --> 00:28:11,283
去调整一个 q 的 shape

850
00:28:11,366 --> 00:28:13,250
然后去适应 MLA

851
00:28:13,250 --> 00:28:14,683
一个具体的计算了

852
00:28:14,683 --> 00:28:16,966
那上面有很多

853
00:28:17,166 --> 00:28:19,883
最核心的就是前向传播的函数

854
00:28:19,883 --> 00:28:20,933
全部都设置好

855
00:28:20,933 --> 00:28:21,883
还有

856
00:28:21,883 --> 00:28:24,283
还有我们对应的一个 C++的指针

857
00:28:24,400 --> 00:28:26,166
全部都设置完之后

858
00:28:26,166 --> 00:28:28,683
当然还有后面一系列的检查呀

859
00:28:28,683 --> 00:28:30,850
然后才真正的去执行

860
00:28:30,850 --> 00:28:32,766
一个计算了

861
00:28:32,766 --> 00:28:34,766
也就是我们这里面 MHA

862
00:28:34,766 --> 00:28:37,766
相关的内容

863
00:28:37,766 --> 00:28:39,850
那我们现在来一点点点进来呀

864
00:28:43,933 --> 00:28:44,650
点不进来呀

865
00:28:44,650 --> 00:28:45,483
这个函数

866
00:28:45,483 --> 00:28:48,166
或者

867
00:28:48,566 --> 00:28:50,566
那我们人工的点进来吧

868
00:28:50,683 --> 00:28:52,450
那我们人工的点进来看一下

869
00:28:52,483 --> 00:28:54,400
真正的函数的运行

870
00:28:54,400 --> 00:28:55,883
就在这里面

871
00:28:55,883 --> 00:28:58,683
也就是整个 MLA 的一个前向传播

872
00:28:58,683 --> 00:28:59,200
这里面

873
00:28:59,200 --> 00:29:01,400
就支持 split KV

874
00:29:01,600 --> 00:29:06,366
split KV 对一个具体 MLA

875
00:29:06,366 --> 00:29:07,966
进行一个分块

876
00:29:07,966 --> 00:29:10,366
或者分页的 KV 缓存的管理

877
00:29:10,366 --> 00:29:11,166
所以整体

878
00:29:11,166 --> 00:29:14,733
就把这个名字就包装成为 Flash MLA

879
00:29:14,733 --> 00:29:16,133
进行一个对外

880
00:29:16,283 --> 00:29:17,200
同样一开始

881
00:29:17,200 --> 00:29:18,200
我们还是要

882
00:29:18,200 --> 00:29:20,733
对我们整个 Flash 进行一个显存

883
00:29:20,733 --> 00:29:21,966
进行一个

884
00:29:21,966 --> 00:29:22,366
然后

885
00:29:22,366 --> 00:29:24,400
就使用一个定义

886
00:29:24,400 --> 00:29:26,133
去真正的执行

887
00:29:26,366 --> 00:29:28,283
那后面还有很多一堆的参数了

888
00:29:28,283 --> 00:29:31,283
那大家可能去了解时候

889
00:29:31,283 --> 00:29:33,166
或者了解的时候

890
00:29:33,166 --> 00:29:33,800
你就会了解

891
00:29:33,800 --> 00:29:36,283
这些一个具体的命名的意义了

892
00:29:36,283 --> 00:29:37,333
我们现在来看一下

893
00:29:37,333 --> 00:29:40,450
整个入口里面的最核心的一个模块

894
00:29:40,450 --> 00:29:44,483
也就是 run Flash split 这个 FWMLA

895
00:29:44,533 --> 00:29:45,333
那基本上

896
00:29:45,333 --> 00:29:48,600
就是启动整个 GPU 的 kernel 行多

897
00:29:48,600 --> 00:29:49,400
头注意力

898
00:29:49,400 --> 00:29:50,800
特别是前一项的多

899
00:29:50,800 --> 00:29:53,133
MLA 的一个具体的内容

900
00:29:53,133 --> 00:29:53,733
那这里面

901
00:29:53,733 --> 00:29:55,883
其实也是单层的封装

902
00:29:56,200 --> 00:29:57,566
大部分都是做一些校验

903
00:29:57,566 --> 00:29:59,250
还有一个内存的分配

904
00:29:59,250 --> 00:30:01,400
和显存的分配相关的内容

905
00:30:01,400 --> 00:30:03,650
或者分块相关的一个内容

906
00:30:03,650 --> 00:30:05,250
那最核心的参数

907
00:30:05,250 --> 00:30:06,766
或者最核心的一个函数

908
00:30:06,766 --> 00:30:08,166
就是这里面

909
00:30:09,200 --> 00:30:12,450
在整个 Wifi spit forward MV 里面

910
00:30:12,450 --> 00:30:13,333
分开两段

911
00:30:13,333 --> 00:30:14,450
一段是上面

912
00:30:14,450 --> 00:30:15,883
一段是下面

913
00:30:15,966 --> 00:30:16,683
下面那段

914
00:30:16,683 --> 00:30:17,883
就是把结果

915
00:30:17,883 --> 00:30:20,050
所有都 concat 或者 combine 在一起

916
00:30:20,050 --> 00:30:21,800
说这面叫做 Flash

917
00:30:21,883 --> 00:30:24,450
然后 split KV

918
00:30:24,450 --> 00:30:27,133
就 MLA 的一个 KV 开始的分页

919
00:30:27,133 --> 00:30:29,250
你要把所有东西都 command 在一起

920
00:30:29,250 --> 00:30:30,766
又是结合在一起

921
00:30:30,766 --> 00:30:32,283
然后最终的输出

922
00:30:32,333 --> 00:30:35,133
那我们下面的还有一段

923
00:30:35,200 --> 00:30:36,166
上面的这一段

924
00:30:36,166 --> 00:30:37,766
就是这里为止

925
00:30:38,050 --> 00:30:41,250
我们需要去执行整个 split

926
00:30:41,250 --> 00:30:43,050
KV 的一个 MLA 的 kernel

927
00:30:43,050 --> 00:30:45,766
也就是对一个分页的缓存

928
00:30:45,766 --> 00:30:46,883
进行一个计算

929
00:30:47,000 --> 00:30:49,283
我们现在来分开两个函数去看

930
00:30:49,283 --> 00:30:51,000
这两个函数也就是最核心

931
00:30:51,000 --> 00:30:51,366
第一个

932
00:30:51,366 --> 00:30:55,000
就是 Flash MLA 的一个内核函数

933
00:30:55,000 --> 00:30:57,050
主要是负责启动计算线程

934
00:30:57,050 --> 00:30:59,283
还有跟一个分页进行分发

935
00:30:59,283 --> 00:31:02,083
并行的用于执行整个 MLA 机制

936
00:31:02,083 --> 00:31:04,733
支持 split k 的一个具体的优化

937
00:31:05,000 --> 00:31:05,566
基本上

938
00:31:05,566 --> 00:31:07,166
ZOMI 就加了很多的注释了

939
00:31:07,166 --> 00:31:07,650
一开始

940
00:31:07,650 --> 00:31:08,766
获得相关

941
00:31:08,766 --> 00:31:11,200
一个 metadata 的一个缩影和内容

942
00:31:11,200 --> 00:31:13,283
然后进入了一个主循环

943
00:31:13,283 --> 00:31:14,883
多个批次的去执行

944
00:31:14,883 --> 00:31:16,533
最后最核心的就是

945
00:31:16,533 --> 00:31:18,166
虽然我们在 For 里面去执行

946
00:31:18,166 --> 00:31:18,483
但是

947
00:31:18,483 --> 00:31:19,200
每一次

948
00:31:19,200 --> 00:31:21,483
是执行我们整体的一个注意机制

949
00:31:21,483 --> 00:31:22,650
里面的一行

950
00:31:22,883 --> 00:31:24,566
所以我们叫做 computer attention

951
00:31:24,600 --> 00:31:28,250
KV MLA 相关的一个内容

952
00:31:28,250 --> 00:31:31,050
根据分页情况进行执行

953
00:31:31,050 --> 00:31:32,366
那这个函数

954
00:31:32,366 --> 00:31:34,283
就是我们刚才转进来

955
00:31:34,333 --> 00:31:36,133
主要是

956
00:31:36,133 --> 00:31:38,050
最最最核心的一个计算

957
00:31:38,083 --> 00:31:40,683
负责处理注意力机制的计算

958
00:31:40,683 --> 00:31:43,650
还有规划的一个缓存的输入

959
00:31:43,650 --> 00:31:45,966
所以一进来也看到很多个

960
00:31:45,966 --> 00:31:47,600
还有分开很多个定

961
00:31:47,600 --> 00:31:50,733
然后很多个现成块进行一个配对

962
00:31:50,733 --> 00:31:53,250
然后定义张量的内存的布局

963
00:31:53,250 --> 00:31:54,883
然后进行一个逻辑

964
00:31:54,883 --> 00:31:57,333
具体的计算相关的内容

965
00:31:57,333 --> 00:31:58,166
当然了它里面

966
00:31:58,166 --> 00:32:01,133
就设计了两个缓冲区进行计算

967
00:32:01,133 --> 00:32:02,883
然后还没有注意力机制

968
00:32:02,883 --> 00:32:04,933
然后里面就套很多层 for 了

969
00:32:04,933 --> 00:32:06,650
包括一个

970
00:32:06,650 --> 00:32:07,533
一个 mask

971
00:32:07,533 --> 00:32:09,250
也是因果的一个 mask

972
00:32:09,400 --> 00:32:10,283
然后进行计算

973
00:32:10,283 --> 00:32:10,766
那最后

974
00:32:10,766 --> 00:32:13,566
就得到一个 Softmax

975
00:32:13,566 --> 00:32:14,400
然后进行计算

976
00:32:14,400 --> 00:32:16,600
然后同步线层输出

977
00:32:16,883 --> 00:32:18,800
基本上的内容非常的多

978
00:32:18,800 --> 00:32:20,333
那 ZOMI 就不在这里面

979
00:32:20,333 --> 00:32:23,083
一一跟大家去走读里面的代码了

980
00:32:23,083 --> 00:32:25,483
如果对里面的代码细节有兴趣的话

981
00:32:25,483 --> 00:32:25,933
可以看

982
00:32:25,933 --> 00:32:28,766
参考一下猪米做的一个代码的注释

983
00:32:28,766 --> 00:32:31,200
然后看一下里面的现场是怎么同步

984
00:32:31,200 --> 00:32:34,083
然后双缓冲区是怎么去实现

985
00:32:34,283 --> 00:32:34,600
这里面

986
00:32:34,600 --> 00:32:36,683
就实现了一个双模式的执行

987
00:32:36,683 --> 00:32:38,400
如果序列比较短

988
00:32:38,400 --> 00:32:39,800
我们就采用一个计算

989
00:32:39,800 --> 00:32:41,850
优化的一个函数去实现

990
00:32:41,850 --> 00:32:43,283
那如果序列比较长

991
00:32:43,283 --> 00:32:45,600
就会采用计算或者内存的优化

992
00:32:45,600 --> 00:32:47,083
来进行一个属实现

993
00:32:47,083 --> 00:32:49,250
所以这里面就用了两个

994
00:32:49,250 --> 00:32:51,683
然后多个进行一个线程同步

995
00:32:51,683 --> 00:32:53,050
那线程同步

996
00:32:53,050 --> 00:32:54,933
或者进程同步这个函数

997
00:32:54,933 --> 00:32:55,850
或者这个概念

998
00:32:55,850 --> 00:32:59,166
其实在很多的一个 DSA

999
00:32:59,166 --> 00:33:00,283
相关的一个模块

1000
00:33:00,283 --> 00:33:02,400
或 DSA 的一些芯片里面

1001
00:33:02,400 --> 00:33:04,133
是非常的重要

1002
00:33:04,250 --> 00:33:05,566
不过你会发现很多 DSA

1003
00:33:05,566 --> 00:33:07,166
其实没有这么一个概念

1004
00:33:07,166 --> 00:33:10,083
因为在 SIMD 里面很难做

1005
00:33:10,083 --> 00:33:11,850
主要是由做

1006
00:33:11,850 --> 00:33:16,450
在英伟达的 SIMT 相关的一些单指令

1007
00:33:16,450 --> 00:33:17,450
多线程里面

1008
00:33:17,450 --> 00:33:19,333
才会有一个同步线程

1009
00:33:19,333 --> 00:33:20,366
所以说

1010
00:33:20,366 --> 00:33:22,483
包括一些小伙伴

1011
00:33:22,483 --> 00:33:25,083
给 ZOMI 提供了一个 sense

1012
00:33:25,083 --> 00:33:26,566
能不能提供这个同步线层

1013
00:33:26,566 --> 00:33:27,966
其实很多时候

1014
00:33:28,000 --> 00:33:29,483
是因为硬件约束

1015
00:33:29,483 --> 00:33:30,683
而没有对外提供

1016
00:33:30,683 --> 00:33:32,483
这么一个同步线层的功能

1017
00:33:32,483 --> 00:33:34,366
不是说我们不想提供

1018
00:33:34,366 --> 00:33:36,250
很多时候你会发现硬件

1019
00:33:36,250 --> 00:33:36,966
或者

1020
00:33:36,966 --> 00:33:38,766
根据不同的硬件的加工的设计

1021
00:33:38,766 --> 00:33:39,733
它有不同的概念

1022
00:33:39,733 --> 00:33:41,083
你不能一味的去抄

1023
00:33:41,083 --> 00:33:44,200
因为答案回到刚才进入的一个函数

1024
00:33:44,200 --> 00:33:45,333
的一个主要函数

1025
00:33:45,366 --> 00:33:47,733
刚才讲到了整个内容里面

1026
00:33:47,733 --> 00:33:48,600
最核心的第一个

1027
00:33:48,600 --> 00:33:52,166
就是分页的 K 的一个进行计算

1028
00:33:52,166 --> 00:33:52,566
第二个

1029
00:33:52,566 --> 00:33:54,483
就把我们分页后的 KV

1030
00:33:54,483 --> 00:33:55,766
进行一个 combined

1031
00:33:55,766 --> 00:33:56,650
融合在一起

1032
00:33:56,650 --> 00:33:58,933
也是最核心的就这两个了

1033
00:33:59,083 --> 00:34:01,283
我们一个 split 后的 Q 开始

1034
00:34:01,283 --> 00:34:03,883
结果将多个快的一个输出

1035
00:34:03,883 --> 00:34:06,533
合并到最终的一个里面

1036
00:34:06,533 --> 00:34:08,333
所以说这个是一个后处理

1037
00:34:08,333 --> 00:34:10,683
怎么去加载我们整个 LCE 的数据

1038
00:34:10,683 --> 00:34:13,166
计算整个全局的 LCE

1039
00:34:13,200 --> 00:34:15,400
然后如果直视 Infinity

1040
00:34:15,400 --> 00:34:16,283
就视为 0

1041
00:34:16,483 --> 00:34:17,966
调试剂缩放的因子

1042
00:34:17,966 --> 00:34:18,600
最后把数

1043
00:34:18,600 --> 00:34:20,600
据的合并进行输出

1044
00:34:20,600 --> 00:34:23,650
还有当然的一些指针相关的控制

1045
00:34:23,650 --> 00:34:26,166
还有对于一个全局内存

1046
00:34:26,283 --> 00:34:26,600
那证明

1047
00:34:26,600 --> 00:34:28,966
对内存那个相关的一个指针

1048
00:34:28,966 --> 00:34:29,800
相关的内容

1049
00:34:29,800 --> 00:34:32,333
控制是写的非常的精妙

1050
00:34:32,333 --> 00:34:33,000
所以说

1051
00:34:33,000 --> 00:34:35,333
也欢迎大家真的好好读一读

1052
00:34:35,333 --> 00:34:36,883
这 700 多行代码

1053
00:34:36,883 --> 00:34:38,166
非常的精深

1054
00:34:38,166 --> 00:34:39,200
这 700 多行代码

1055
00:34:39,200 --> 00:34:40,766
说实话可能一个程序员

1056
00:34:40,766 --> 00:34:43,366
每天也就写五六十行

1057
00:34:43,366 --> 00:34:43,966
所以说

1058
00:34:43,966 --> 00:34:46,400
真正的把整个 700 多行的代码写出来

1059
00:34:46,400 --> 00:34:47,733
是非常的难

1060
00:34:47,733 --> 00:34:48,850
非常的深奥

1061
00:34:49,000 --> 00:34:49,966
那我们今天

1062
00:34:49,966 --> 00:34:53,166
基本上就走读完整个一个

1063
00:34:53,166 --> 00:34:57,766
或者我们整个 Flash MLA 对应的代码了

1064
00:34:57,766 --> 00:34:59,933
我们简单的回到 PPT 里面

1065
00:35:01,050 --> 00:35:02,800
刚才讲到了最核心的一个函数

1066
00:35:02,800 --> 00:35:05,366
就是 fast forward MLA.h

1067
00:35:05,400 --> 00:35:07,683
里面就实现了几个重要的功能

1068
00:35:07,683 --> 00:35:09,000
因为我们刚才带着大家

1069
00:35:09,000 --> 00:35:11,450
已经一起去组读了一下代码

1070
00:35:11,450 --> 00:35:12,850
那最核心的 4 个功能

1071
00:35:12,850 --> 00:35:13,766
ZOMI 做了一个简洁

1072
00:35:13,766 --> 00:35:14,083
第一个

1073
00:35:14,083 --> 00:35:17,200
就是共享内存的一个布局优化

1074
00:35:17,200 --> 00:35:19,533
根据输入的一个数据

1075
00:35:19,533 --> 00:35:20,850
一个 data 啦

1076
00:35:20,883 --> 00:35:22,200
输入数据的一个维度

1077
00:35:22,200 --> 00:35:24,850
选择合适的共享的内存的一个布局

1078
00:35:25,166 --> 00:35:28,733
来优化一个内存的一个 IO

1079
00:35:28,733 --> 00:35:29,333
那接着

1080
00:35:29,333 --> 00:35:31,650
就是矩阵程跟 softmaxs 的计算

1081
00:35:31,650 --> 00:35:33,533
那矩正程跟 softmaxs 的计算

1082
00:35:33,533 --> 00:35:35,400
切分成很多个 block

1083
00:35:35,400 --> 00:35:37,883
很多个小块进行一个分页

1084
00:35:37,883 --> 00:35:38,566
具体的分页

1085
00:35:38,566 --> 00:35:39,933
就用了 split k

1086
00:35:39,933 --> 00:35:41,533
一个具体的优化的算法

1087
00:35:41,533 --> 00:35:43,133
把我们整体的计算了

1088
00:35:43,133 --> 00:35:46,000
就切分到很多个快的计算

1089
00:35:46,000 --> 00:35:48,083
分布到多个线程里面去

1090
00:35:48,083 --> 00:35:50,600
提升整个 MFU

1091
00:35:50,733 --> 00:35:52,933
那最后就是结果的存储

1092
00:35:52,933 --> 00:35:53,966
那结果的存储

1093
00:35:53,966 --> 00:35:56,766
其实跟 split k 是相相关

1094
00:35:56,766 --> 00:35:57,850
将最后的结果

1095
00:35:57,850 --> 00:35:59,400
存储到全局的内存里面

1096
00:35:59,400 --> 00:36:01,200
或者一个中间缓存

1097
00:36:01,283 --> 00:36:03,050
那最重要的就是支持 split

1098
00:36:03,050 --> 00:36:03,450
跟 split

1099
00:36:03,450 --> 00:36:06,766
face split 一个显存或者存储的策略

1100
00:36:07,000 --> 00:36:09,483
那就一共有四个重要的特性

1101
00:36:09,483 --> 00:36:10,800
那这四个特性

1102
00:36:10,800 --> 00:36:13,850
都是非常的紧密的相结合

1103
00:36:13,850 --> 00:36:17,166
我们首先有个全局的内存的优化

1104
00:36:17,166 --> 00:36:18,483
获取 metadate

1105
00:36:18,650 --> 00:36:19,166
然后

1106
00:36:19,166 --> 00:36:21,483
对具体的核心进行计算

1107
00:36:21,483 --> 00:36:22,200
计算之前

1108
00:36:22,200 --> 00:36:24,050
我们会分开很多个 split

1109
00:36:24,200 --> 00:36:25,883
进行一个优化

1110
00:36:25,883 --> 00:36:28,200
那最后把结果存起来

1111
00:36:28,250 --> 00:36:29,566
这么几个过程

1112
00:36:31,000 --> 00:36:32,533
又到晚上两点钟了

1113
00:36:32,533 --> 00:36:35,933
录课明天早上还得 6 点钟去机场

1114
00:36:35,933 --> 00:36:39,250
我们现在来一个简单的总结和思考

1115
00:36:39,250 --> 00:36:40,766
那整个总结

1116
00:36:40,766 --> 00:36:44,050
就是整个 Flash MLA 的相关的特性

1117
00:36:44,050 --> 00:36:44,400
第一个

1118
00:36:44,400 --> 00:36:48,133
就是分页的 KV 开始的一个缓存管理

1119
00:36:48,333 --> 00:36:51,083
主要是基于 64 的一个报大小

1120
00:36:51,083 --> 00:36:53,283
进行一个显存管理

1121
00:36:53,283 --> 00:36:55,966
第二个就是异步的内存拷贝

1122
00:36:55,966 --> 00:36:58,133
最重要的就是利用了整个 Hopper 架构

1123
00:36:58,133 --> 00:36:59,400
一个 SM90

1124
00:36:59,683 --> 00:37:02,133
整体的一个 TMA 相关的内容

1125
00:37:02,133 --> 00:37:04,133
如果大家对 Hopper 加构不理解

1126
00:37:04,133 --> 00:37:06,400
也可以看回 ZOMI 在之前分享

1127
00:37:06,400 --> 00:37:09,333
一个英伟达相关的一些硬件

1128
00:37:09,333 --> 00:37:11,200
可以看回 ZOMI 之前分享

1129
00:37:11,200 --> 00:37:13,000
英伟达相关的视频

1130
00:37:13,166 --> 00:37:13,600
那最后

1131
00:37:13,600 --> 00:37:15,883
还有一个双模式的执行的引擎

1132
00:37:15,883 --> 00:37:18,566
或者双缓程区相关执行的内容

1133
00:37:19,200 --> 00:37:20,400
那综艺觉得其实蛮有意思

1134
00:37:20,400 --> 00:37:22,400
就是整个 Flash MLA

1135
00:37:22,400 --> 00:37:24,650
其实主要是提供了一个前向

1136
00:37:24,650 --> 00:37:25,966
后来我们推理的时候

1137
00:37:25,966 --> 00:37:26,800
在 Hopper 架

1138
00:37:26,800 --> 00:37:28,050
构的一个加速

1139
00:37:28,050 --> 00:37:28,683
那未来

1140
00:37:28,683 --> 00:37:30,566
肯定会有越来越多

1141
00:37:30,566 --> 00:37:32,933
支持易购的推理框架呀

1142
00:37:32,966 --> 00:37:36,533
去集成整个 Flash MLA 的能力

1143
00:37:36,566 --> 00:37:38,400
那这个对整个推理框架

1144
00:37:38,400 --> 00:37:40,850
整个推理的市场意味着什么

1145
00:37:40,850 --> 00:37:43,050
那对做推理的中间件

1146
00:37:43,050 --> 00:37:45,083
加速和推理框架的公司

1147
00:37:45,133 --> 00:37:46,850
又有哪些启示

1148
00:37:46,966 --> 00:37:49,850
未来中间件的公司还能走多远

1149
00:37:49,850 --> 00:37:52,600
还是直接都集成换方得了

1150
00:37:52,883 --> 00:37:54,050
还有最后一个问题

1151
00:37:54,050 --> 00:37:55,683
就是对算力到底是利好

1152
00:37:55,683 --> 00:37:56,933
还是什么格局

1153
00:37:56,933 --> 00:37:57,683
那这些问题

1154
00:37:57,683 --> 00:37:58,883
ZOMI 抛回给大家

1155
00:37:58,883 --> 00:38:00,250
那有兴趣的小伙伴

1156
00:38:00,250 --> 00:38:01,933
也可以上这里面的花课链接

1157
00:38:01,933 --> 00:38:03,283
去获取这个 PPT

1158
00:38:03,283 --> 00:38:05,566
或者给一个 GM 进行点赞了

1159
00:38:05,566 --> 00:38:06,450
今天内容

1160
00:38:06,450 --> 00:38:07,566
就先到这里为止了

1161
00:38:07,566 --> 00:38:08,083
谢谢各位

1162
00:38:08,083 --> 00:38:08,883
拜了个拜

