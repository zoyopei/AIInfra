1
00:00:00,000 --> 00:00:01,066
内容/录制:Z0MI 酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,066 --> 00:00:03,300
炸圈了炸圈了

3
00:00:03,400 --> 00:00:05,600
今天晚上 11 点多的时候准备下班

4
00:00:05,600 --> 00:00:07,133
然后打开朋友圈

5
00:00:07,233 --> 00:00:09,300
不小心看到朋友圈转发了

6
00:00:09,333 --> 00:00:13,666
都是 DeepSeek R1 这个网络模型的开源

7
00:00:14,133 --> 00:00:14,666
那于是

8
00:00:14,666 --> 00:00:16,666
ZOMI 也就连夜去看一下

9
00:00:16,666 --> 00:00:18,833
DeepSeek R1 到底有哪些不一样的点

10
00:00:18,833 --> 00:00:21,733
然后做了一个简单的深度解读

11
00:00:21,966 --> 00:00:23,333
那在这期视频里面的

12
00:00:23,333 --> 00:00:25,266
可能时间稍微长了一点点

13
00:00:25,433 --> 00:00:27,300
ZOMI 主要是跟大家分享 4 个内容

14
00:00:27,333 --> 00:00:28,000
第一个

15
00:00:28,000 --> 00:00:29,666
我们去看看

16
00:00:29,666 --> 00:00:30,766
虽然现在有点疲惫

17
00:00:30,800 --> 00:00:31,933
我们重点去看看

18
00:00:31,933 --> 00:00:34,266
DeepSeek R1 的一个技术文章的解读

19
00:00:34,433 --> 00:00:34,766
接着

20
00:00:34,800 --> 00:00:37,166
这篇文章里面很重要的提到两个点

21
00:00:37,166 --> 00:00:37,466
第一个

22
00:00:37,466 --> 00:00:39,766
就是 DeepSeek R1 的 zero 这个模型

23
00:00:39,800 --> 00:00:42,966
然后引进到 DeepSeek R1 这个模型

24
00:00:43,000 --> 00:00:44,366
那这两个模型里面

25
00:00:44,366 --> 00:00:46,566
最核心的一个强化学习的算法

26
00:00:46,733 --> 00:00:50,233
用的是幻方自己去研发的 GRPO

27
00:00:50,233 --> 00:00:53,133
也就是 Group Relative Policy optimization

28
00:00:53,166 --> 00:00:55,000
这个算法 ZOMI 我觉得还是蛮有意思的

29
00:00:55,000 --> 00:00:55,533
这里面

30
00:00:55,533 --> 00:00:56,366
在这个视频里面

31
00:00:56,366 --> 00:00:58,233
简单的解读一下这个算法

32
00:00:58,333 --> 00:00:59,633
那在最核心的内容

33
00:00:59,633 --> 00:00:59,866
还是

34
00:00:59,866 --> 00:01:02,866
引起我们对整个产业相关的一些思考

35
00:01:02,866 --> 00:01:04,166
和简单的小结

36
00:01:04,966 --> 00:01:05,600
那我相信

37
00:01:05,600 --> 00:01:07,400
今天晚上应该很多朋友

38
00:01:07,400 --> 00:01:08,800
都看了这么一个链接

39
00:01:08,800 --> 00:01:10,133
也就是深度求索幻方

40
00:01:10,133 --> 00:01:12,966
自己发布的一个相关的文章

41
00:01:12,966 --> 00:01:14,800
这篇公众号里面就说了

42
00:01:14,800 --> 00:01:15,733
我的性能

43
00:01:15,733 --> 00:01:16,833
已经对齐

44
00:01:16,933 --> 00:01:19,333
整体 Open AI 的 o1 的一个正式版本

45
00:01:19,333 --> 00:01:21,166
所以说我非常的牛逼

46
00:01:21,566 --> 00:01:22,233
那最后

47
00:01:22,233 --> 00:01:23,833
还直接把论文了

48
00:01:23,866 --> 00:01:24,733
这个不是论文了

49
00:01:24,766 --> 00:01:25,800
其实技术文章

50
00:01:25,800 --> 00:01:26,866
解读出来

51
00:01:26,866 --> 00:01:27,833
然后你们说了

52
00:01:27,833 --> 00:01:29,233
我还蒸馏了很多小模型

53
00:01:29,233 --> 00:01:30,566
供大家去使用的

54
00:01:30,600 --> 00:01:32,166
而且非常的有诚意

55
00:01:32,166 --> 00:01:33,400
还在 HuggingFace 上面

56
00:01:33,400 --> 00:01:35,633
发布了一系列的我的模型

57
00:01:35,633 --> 00:01:37,366
都开运给大家去使用了

58
00:01:37,400 --> 00:01:39,766
而且我还开放了许可协议

59
00:01:39,766 --> 00:01:41,000
另外的话在网上

60
00:01:41,033 --> 00:01:43,133
网上也可以就直接的去用

61
00:01:43,166 --> 00:01:44,000
ZOMI 也用了一下

62
00:01:44,000 --> 00:01:44,666
问了一个问题

63
00:01:44,666 --> 00:01:46,133
还是非常的有意思的

64
00:01:46,166 --> 00:01:48,266
而且整体的定价非常的便宜

65
00:01:48,266 --> 00:01:50,900
反正技术改变世界就招人

66
00:01:51,200 --> 00:01:53,333
那我们现在打开一个幻方的

67
00:01:53,333 --> 00:01:54,933
一个 R1 的链接来看看

68
00:01:54,933 --> 00:01:57,466
里面确实发布了非常多的一个文章

69
00:01:57,466 --> 00:01:58,733
还有相关的内容

70
00:01:58,800 --> 00:02:00,533
那这边的模型都可以下载

71
00:02:00,533 --> 00:02:02,066
确实非常的有诚意

72
00:02:02,166 --> 00:02:02,366
那

73
00:02:02,366 --> 00:02:04,733
了解完一些基本的官网的信息之后

74
00:02:04,733 --> 00:02:07,000
我们现在来到了第一个内容重点

75
00:02:07,000 --> 00:02:09,400
去看一下 DeepSeek R1 这个模型

76
00:02:09,400 --> 00:02:11,933
进行一个技术文章的解读

77
00:02:12,800 --> 00:02:14,666
ZOMI 的鼠标一向都很大

78
00:02:14,666 --> 00:02:16,300
因为上年纪喔

79
00:02:16,333 --> 00:02:17,600
没办法啦

80
00:02:29,433 --> 00:02:31,733
那我们现在来看一下这一篇技术文章

81
00:02:31,766 --> 00:02:33,566
里面讲了哪些东西

82
00:02:33,633 --> 00:02:34,166
很有意思的

83
00:02:34,200 --> 00:02:35,266
就 DeepSeek R1

84
00:02:35,266 --> 00:02:38,066
确实很有诚意的说了很多相关的事情

85
00:02:38,066 --> 00:02:38,633
那这里面

86
00:02:38,633 --> 00:02:41,033
最重要的就是介绍两个模型

87
00:02:41,366 --> 00:02:43,200
第一个就是 DeepSeek R1 的 zero

88
00:02:43,200 --> 00:02:45,066
第二个是 DeepSeek R1

89
00:02:45,366 --> 00:02:48,033
然后意思就是 DeepSeek R1 zero

90
00:02:48,033 --> 00:02:48,566
这个模型

91
00:02:48,600 --> 00:02:49,233
一开始

92
00:02:49,233 --> 00:02:53,266
就没有没有 without 直接使用 SFT

93
00:02:53,266 --> 00:02:55,066
而是直接用了 RL

94
00:02:55,166 --> 00:02:58,266
这么一个相关的算法来去实现的

95
00:02:58,266 --> 00:03:00,500
那效果是非常的好

96
00:03:00,533 --> 00:03:02,000
但是可圈可点

97
00:03:02,033 --> 00:03:04,833
因为有一些 poor readability

98
00:03:04,833 --> 00:03:06,866
还有 language missing 的问题

99
00:03:06,866 --> 00:03:08,933
也就是阅读起来很有困难

100
00:03:08,966 --> 00:03:10,166
还有一些英文

101
00:03:10,166 --> 00:03:11,433
英中文的一个混杂

102
00:03:11,433 --> 00:03:12,233
之后

103
00:03:12,400 --> 00:03:15,433
我们就重新的去搞了另外一个模型

104
00:03:15,433 --> 00:03:16,900
叫做 DeepSeek R1

105
00:03:16,966 --> 00:03:19,733
那 DeepSeek R1 又是基于 DeepSeek R1 zero

106
00:03:19,733 --> 00:03:21,533
上面多了几个内容

107
00:03:21,533 --> 00:03:23,833
第一个就是多阶段的训练

108
00:03:23,833 --> 00:03:26,133
第二个就是数据的冷启动

109
00:03:26,233 --> 00:03:28,833
在进行一个强化学习之前

110
00:03:29,166 --> 00:03:31,666
那于是就有了这两个模型了

111
00:03:31,666 --> 00:03:32,866
效果是非常好的

112
00:03:32,866 --> 00:03:34,333
很浅的这条线

113
00:03:34,366 --> 00:03:37,200
就是 DeepSeek V3 预训练的模型

114
00:03:37,266 --> 00:03:39,366
经过一个强化微调之后

115
00:03:39,400 --> 00:03:40,466
你可以看到

116
00:03:40,466 --> 00:03:43,100
整体 DeepSeek R1 的模型的效果

117
00:03:43,233 --> 00:03:45,566
确实已经精度非常高了

118
00:03:45,600 --> 00:03:47,800
而且在 CodeForces 里面的效果

119
00:03:47,800 --> 00:03:48,866
也是非常高的

120
00:03:48,866 --> 00:03:50,833
普通的预训练大模型的效果

121
00:03:50,866 --> 00:03:52,233
真的没有那么好

122
00:03:52,266 --> 00:03:54,166
那于是我们继续往下看一下

123
00:03:54,200 --> 00:03:55,133
它有哪些内容

124
00:03:55,133 --> 00:03:57,033
那在整个目录里面的

125
00:03:57,033 --> 00:04:00,633
ZOMI 觉得可能第二还有 4.2

126
00:04:00,633 --> 00:04:02,500
是非常的核心

127
00:04:02,533 --> 00:04:04,933
那我们今天也会重点的去看一看

128
00:04:04,933 --> 00:04:05,433
第二章

129
00:04:05,433 --> 00:04:07,700
所使用到的 approach 是相关的算法

130
00:04:07,800 --> 00:04:10,533
那最核心的就是 DeepSeek R1 Zero

131
00:04:10,600 --> 00:04:13,366
怎么去用一个强化学习进行训练的

132
00:04:13,366 --> 00:04:13,766
第二个

133
00:04:13,766 --> 00:04:15,433
就是 DeepSeek R1

134
00:04:15,800 --> 00:04:18,566
怎么去通过数据进行冷启动

135
00:04:18,633 --> 00:04:20,866
还有我们多阶段训练

136
00:04:20,866 --> 00:04:21,466
那最后

137
00:04:21,466 --> 00:04:24,733
还讨论了一些 unccessful 的一个 attempt

138
00:04:24,766 --> 00:04:26,333
也就是失败的案例

139
00:04:26,366 --> 00:04:29,066
为什么我们之前的一些尝试过的内容

140
00:04:29,066 --> 00:04:29,966
为什么不行

141
00:04:30,066 --> 00:04:30,933
那接下来

142
00:04:30,966 --> 00:04:34,433
我们重点来去看一下相关的 introduction

143
00:04:34,666 --> 00:04:36,133
在整个介绍里面

144
00:04:36,166 --> 00:04:37,933
就引用了很多相关的论文

145
00:04:37,933 --> 00:04:40,233
说实话他这里面就说到上面

146
00:04:40,233 --> 00:04:41,333
没有一篇文章

147
00:04:41,366 --> 00:04:44,200
none of this 是真正的有效的

148
00:04:44,200 --> 00:04:45,400
而真正的有效的

149
00:04:45,400 --> 00:04:47,533
是用我们直接基于

150
00:04:47,533 --> 00:04:50,166
那个 DeepSeek V3 的一个基础模型

151
00:04:50,200 --> 00:04:52,066
使用 GRPO 模型

152
00:04:52,066 --> 00:04:54,966
作用本整个强化学习的框架

153
00:04:55,000 --> 00:04:57,233
来提升我们模型的一个 text

154
00:04:57,233 --> 00:04:58,233
time 的 Reasoning

155
00:04:58,266 --> 00:04:59,533
推理的性能

156
00:04:59,600 --> 00:05:02,133
经过几千步的强化之后

157
00:05:02,166 --> 00:05:03,666
DeepSeek R1 zero

158
00:05:03,666 --> 00:05:05,300
在基准的数据集里面

159
00:05:05,333 --> 00:05:07,833
其实表现的非常的夸张

160
00:05:07,833 --> 00:05:12,433
从一个 AIME 的 15.6%直接提升到 71%

161
00:05:12,533 --> 00:05:14,166
所以 DeepSeek 就认为

162
00:05:14,166 --> 00:05:16,666
我们通过一个强化学习的方法

163
00:05:16,666 --> 00:05:19,933
就能解决或者就能复现 o1 的效果了

164
00:05:19,966 --> 00:05:22,966
而不是像之前很多上海交大，复旦

165
00:05:22,966 --> 00:05:25,000
搞了非常多的 summaries

166
00:05:25,000 --> 00:05:26,800
为这些算法

167
00:05:26,800 --> 00:05:29,466
或者这些 MTCS 的都没有用了

168
00:05:29,666 --> 00:05:30,066
那于是

169
00:05:30,066 --> 00:05:32,366
我们往下看一下他怎么去实现的

170
00:05:32,400 --> 00:05:34,000
在实现的过程当中

171
00:05:34,000 --> 00:05:35,266
其实 zero

172
00:05:35,266 --> 00:05:37,133
还是有一些可圈可点的问题的

173
00:05:37,166 --> 00:05:40,066
于是在 DeepSeek R1 里面

174
00:05:40,066 --> 00:05:41,766
就使用了数据冷启动

175
00:05:41,800 --> 00:05:43,533
还有多阶段的训练

176
00:05:43,533 --> 00:05:46,400
另外的话还使用了一些 SFT 的数据

177
00:05:46,400 --> 00:05:49,066
去给到我们的 RL checkpoint

178
00:05:49,066 --> 00:05:51,266
进行一个重新的 post-training

179
00:05:51,266 --> 00:05:53,066
那经过这种方法之后

180
00:05:53,066 --> 00:05:54,900
整体的效果非常的好

181
00:05:54,933 --> 00:05:57,733
取得非常惊人的一个成绩

182
00:05:57,733 --> 00:06:00,466
于是有了一些 contribution 相关的内容

183
00:06:00,466 --> 00:06:02,366
那简单的做了一个 summary

184
00:06:02,400 --> 00:06:04,033
我们确实很牛逼

185
00:06:04,033 --> 00:06:05,666
对业界贡献很多

186
00:06:05,833 --> 00:06:08,100
那讲完刚才的内容之后

187
00:06:08,133 --> 00:06:10,266
我们现在来到最核心的内容

188
00:06:10,266 --> 00:06:12,133
也就是整篇技术文章

189
00:06:12,166 --> 00:06:13,733
ZOMI 觉得很重要的

190
00:06:13,733 --> 00:06:17,166
就是它整体的 approch

191
00:06:17,166 --> 00:06:18,133
相关的方法

192
00:06:18,233 --> 00:06:19,500
在这篇文章里面

193
00:06:19,533 --> 00:06:21,166
其实主要是介绍三点

194
00:06:21,166 --> 00:06:22,366
第一点就有 DeepSeek R1 

195
00:06:22,366 --> 00:06:25,433
的 zero 它没有使用任何 SFT 的数据

196
00:06:25,433 --> 00:06:26,166
的情况下

197
00:06:26,200 --> 00:06:27,566
将强化学习

198
00:06:27,600 --> 00:06:29,733
直接应用到基础模型

199
00:06:29,733 --> 00:06:32,066
没有任何搜索树策略问题

200
00:06:32,333 --> 00:06:32,933
第二个点

201
00:06:32,933 --> 00:06:34,400
就是 DeepSeek R1

202
00:06:34,466 --> 00:06:37,266
直接使用长思维链的一个数据

203
00:06:37,266 --> 00:06:38,966
进行一个 SFT

204
00:06:39,366 --> 00:06:40,000
那最后

205
00:06:40,000 --> 00:06:42,400
就是将 DeepSeek R1 的推理能力

206
00:06:42,400 --> 00:06:44,733
提炼到我们的小模型里面

207
00:06:44,733 --> 00:06:46,866
那可能第一个点跟第二个点

208
00:06:46,866 --> 00:06:48,100
是最核心的

209
00:06:48,333 --> 00:06:50,733
也就是第三个点可能在整篇文章里面

210
00:06:50,733 --> 00:06:52,233
或者 ZOMI 不太关心

211
00:06:52,233 --> 00:06:53,833
我们现在重点打开第一个点

212
00:06:53,833 --> 00:06:54,833
跟第二个点

213
00:06:54,833 --> 00:06:57,066
那首先我们一开始还是重点

214
00:06:57,066 --> 00:06:59,866
去关注 DeepSeek R1 Zero 这个模型

215
00:06:59,866 --> 00:07:00,566
最核心的

216
00:07:00,600 --> 00:07:03,033
使用一个基于我们的预训的模型

217
00:07:03,033 --> 00:07:04,466
进行一个强化的

218
00:07:04,466 --> 00:07:06,166
那在整个过程当中

219
00:07:06,200 --> 00:07:08,366
重点的强调了好几回

220
00:07:08,633 --> 00:07:11,300
我们没有使用任何监督的方法

221
00:07:11,333 --> 00:07:13,033
或者监督的数据

222
00:07:13,066 --> 00:07:15,933
直接使用纯粹强化学习的方式

223
00:07:15,966 --> 00:07:18,766
也就是 pure reforcement learning

224
00:07:18,766 --> 00:07:22,200
让我们整个模型进行一个自学习

225
00:07:22,233 --> 00:07:25,100
自推理那这里面的强化学习算法

226
00:07:25,133 --> 00:07:28,133
就是使用我们之前发的一篇文章

227
00:07:28,133 --> 00:07:30,033
也就幻方之前发的一篇文章

228
00:07:30,166 --> 00:07:33,333
GRPO 这么一个算法来去实现

229
00:07:33,333 --> 00:07:34,533
那 GRPO 的算法

230
00:07:34,533 --> 00:07:36,566
ZOMI 会在后面的第三节

231
00:07:36,566 --> 00:07:38,533
跟大家一起去分享的

232
00:07:38,533 --> 00:07:39,400
那整体来说

233
00:07:39,400 --> 00:07:40,400
可以看到

234
00:07:40,800 --> 00:07:43,533
在整个模型里面很重要的就是 Reward Modeling

235
00:07:43,533 --> 00:07:45,066
怎么去评价这个模型

236
00:07:45,066 --> 00:07:46,333
到底好还是不好

237
00:07:46,400 --> 00:07:48,866
于是这里面就提出了两个点

238
00:07:49,200 --> 00:07:52,000
第一个就是准确性的奖励

239
00:07:52,000 --> 00:07:53,033
准确性的奖励

240
00:07:53,266 --> 00:07:55,666
明确的去奖励我们的模型

241
00:07:55,666 --> 00:07:56,733
可以评估的答案

242
00:07:56,766 --> 00:07:58,133
到底正确不正确

243
00:07:58,133 --> 00:07:58,733
第二个就

244
00:07:58,733 --> 00:08:00,366
是格式的奖励了

245
00:08:00,366 --> 00:08:01,933
那就是 Format rewards

246
00:08:01,933 --> 00:08:02,733
格式的奖励

247
00:08:02,733 --> 00:08:04,233
就是除了准确性以外

248
00:08:04,233 --> 00:08:06,366
还使用了一些格式性的奖励

249
00:08:06,733 --> 00:08:08,433
强制我们的一个大模型

250
00:08:08,433 --> 00:08:09,866
将思考的过程

251
00:08:09,866 --> 00:08:12,333
至于我们的 think and think 之间

252
00:08:12,366 --> 00:08:14,366
也就是我们打了一个 tag 标记

253
00:08:14,366 --> 00:08:15,966
然后让我们的模型

254
00:08:15,966 --> 00:08:18,033
尽可能的去思考

255
00:08:18,166 --> 00:08:21,000
在整个 R1 的训练的过程当中

256
00:08:21,000 --> 00:08:21,933
没有使用结果

257
00:08:21,933 --> 00:08:24,633
或者过程的一个奖励模型

258
00:08:24,766 --> 00:08:26,833
因为在真正训练过程中

259
00:08:26,833 --> 00:08:27,633
就发现

260
00:08:27,800 --> 00:08:30,266
使用了过程或者结果的奖励模型

261
00:08:30,266 --> 00:08:32,033
就会使得我们的神经网络

262
00:08:32,133 --> 00:08:33,600
会出现一个 reward hacking

263
00:08:33,766 --> 00:08:35,933
也就是奖励黑客问题

264
00:08:35,933 --> 00:08:37,133
就使得我们的模型

265
00:08:37,133 --> 00:08:40,166
陷入一个被动的不断的推理的过程

266
00:08:40,166 --> 00:08:41,966
或者一些很奇怪的推理过程

267
00:08:41,966 --> 00:08:43,733
就使得整个训练的过程

268
00:08:43,733 --> 00:08:45,333
变得非常的复杂

269
00:08:45,400 --> 00:08:47,466
所以整体来说非常的纯粹

270
00:08:47,466 --> 00:08:49,466
就用了一个 RL 的算法

271
00:08:49,733 --> 00:08:52,566
再来这里面讲了一些 training 的 template

272
00:08:52,566 --> 00:08:54,666
就我们训练的一些 template

273
00:08:54,666 --> 00:08:57,333
其实是没有对外太多的去公布的

274
00:08:57,366 --> 00:08:58,266
那但是蛮有意思的

275
00:08:58,266 --> 00:09:01,433
就讲了一些我们模型的一个 performance

276
00:09:01,433 --> 00:09:04,066
性能还有净化的一个过程

277
00:09:04,066 --> 00:09:04,666
那下面

278
00:09:04,666 --> 00:09:07,033
就是一些相关的进化的过程

279
00:09:07,033 --> 00:09:09,100
在每一个训练阶段里面

280
00:09:09,133 --> 00:09:10,633
基本上都会把 r1

281
00:09:10,633 --> 00:09:13,766
zero 我相关的 CKPT 切块拿出来

282
00:09:13,800 --> 00:09:15,033
去看一下它 accuracy

283
00:09:15,033 --> 00:09:17,233
可以看到在整个训练过程当中

284
00:09:17,333 --> 00:09:19,833
模型的效果是越来越好的

285
00:09:20,366 --> 00:09:21,633
而且再往下看一下

286
00:09:21,633 --> 00:09:22,566
蛮有意思的一点

287
00:09:22,600 --> 00:09:24,400
就是整个模型

288
00:09:24,433 --> 00:09:26,433
有了一个 self evolution

289
00:09:26,433 --> 00:09:29,500
也就是自我的进化的过程当中

290
00:09:29,533 --> 00:09:30,666
那所谓的自我的进化

291
00:09:30,666 --> 00:09:31,166
过程当中

292
00:09:31,200 --> 00:09:32,933
就是我们的网络模型

293
00:09:32,933 --> 00:09:33,466
一开始

294
00:09:33,466 --> 00:09:35,100
是基于一个预训练大模型

295
00:09:35,133 --> 00:09:36,966
进行一个微调的

296
00:09:36,966 --> 00:09:37,833
那预训练大模型

297
00:09:37,833 --> 00:09:38,633
你问他一个问题

298
00:09:38,633 --> 00:09:39,933
他可能回答一个问题

299
00:09:39,966 --> 00:09:41,800
他只能做多轮的回答

300
00:09:41,800 --> 00:09:43,233
但是他不会做推理

301
00:09:43,233 --> 00:09:45,366
也不会做 test time 的一个 reasoning

302
00:09:45,466 --> 00:09:46,466
就自我的思考

303
00:09:46,466 --> 00:09:47,500
自我的推理

304
00:09:47,533 --> 00:09:48,000
但是

305
00:09:48,000 --> 00:09:50,800
随着我们在 R1 的一个训练过程当中

306
00:09:50,800 --> 00:09:52,566
他的一个自我思考的时间

307
00:09:52,566 --> 00:09:53,433
就越来越长

308
00:09:53,433 --> 00:09:54,633
于是 DeepSeek 

309
00:09:54,633 --> 00:09:57,933
就称为这种是自我整体进化的过程

310
00:09:57,966 --> 00:09:59,033
那这个图

311
00:09:59,033 --> 00:10:00,366
还是非常的有意思

312
00:10:00,400 --> 00:10:02,866
那这也可以简单的停留下来去看一下

313
00:10:03,266 --> 00:10:03,766
那第二点

314
00:10:03,800 --> 00:10:05,933
就是整个模型

315
00:10:05,933 --> 00:10:07,000
在训练过程当中

316
00:10:07,000 --> 00:10:08,333
训练到一定程度

317
00:10:08,366 --> 00:10:10,466
会出现一个 a hah moment

318
00:10:10,466 --> 00:10:12,166
那什么叫做 a hah moment

319
00:10:12,200 --> 00:10:14,466
就是我问他一个问题

320
00:10:14,466 --> 00:10:15,766
问他一个逻辑的问题

321
00:10:15,800 --> 00:10:17,033
他会进行一个推理

322
00:10:17,066 --> 00:10:18,066
推理着推理着

323
00:10:18,066 --> 00:10:19,466
他可能出出现了 wait

324
00:10:19,466 --> 00:10:20,666
wait

325
00:10:20,666 --> 00:10:21,166
that an

326
00:10:21,200 --> 00:10:23,566
moment i can wait here

327
00:10:23,633 --> 00:10:25,533
有的时候我开始在思考了

328
00:10:25,566 --> 00:10:26,933
我可能发现诶

329
00:10:26,933 --> 00:10:28,800
我之前的推理可能不够细

330
00:10:28,800 --> 00:10:31,800
不够深我重新的进行一个推理

331
00:10:31,833 --> 00:10:32,300
于是

332
00:10:32,333 --> 00:10:34,366
幻幻就认为他出现了一个 a hah moment

333
00:10:34,366 --> 00:10:35,733
这个 a hah moment

334
00:10:35,800 --> 00:10:36,633
很重要的

335
00:10:36,633 --> 00:10:39,266
特别是对于一个自我进化的过程当中

336
00:10:39,666 --> 00:10:42,833
不过 DeepSeek R1 Zero

337
00:10:42,866 --> 00:10:44,566
确实也有一些问题的

338
00:10:44,633 --> 00:10:46,433
虽然它呈现一个非常强大的

339
00:10:46,433 --> 00:10:47,366
reasoning 的能力

340
00:10:47,400 --> 00:10:50,066
而且能够自主的去想象一些

341
00:10:50,066 --> 00:10:52,433
可能之前自己没有去推理过的问题

342
00:10:52,433 --> 00:10:54,766
不过整体来说它的可读性比较差

343
00:10:54,866 --> 00:10:56,966
语言会混杂到一起

344
00:10:57,333 --> 00:10:59,233
为了提高它的可读性

345
00:10:59,233 --> 00:10:59,766
于是

346
00:10:59,800 --> 00:11:01,333
就深入的去研究

347
00:11:01,333 --> 00:11:03,433
我们的 DeepSeek R1 这个模型了

348
00:11:03,633 --> 00:11:05,666
因此我们现在来到了第二个内容

349
00:11:05,666 --> 00:11:10,700
有 2.3 的 DeepSeek R1 reforcement learning with cold start

350
00:11:11,066 --> 00:11:13,066
一个冷启动强化学习

351
00:11:13,166 --> 00:11:13,600
不过

352
00:11:13,600 --> 00:11:15,733
在正式进入到下面内容之前

353
00:11:15,733 --> 00:11:17,566
其实还要去思考两个问题的

354
00:11:17,566 --> 00:11:17,866
第一个

355
00:11:17,866 --> 00:11:19,966
就是冷启动的过程当中

356
00:11:20,233 --> 00:11:22,700
我们能不能加入一些少量的数据

357
00:11:22,733 --> 00:11:24,833
去提高我们的模型的性能

358
00:11:24,833 --> 00:11:26,033
和收敛的速度

359
00:11:26,033 --> 00:11:26,766
那第二个

360
00:11:26,800 --> 00:11:28,533
怎么样才能够提升 zero 的

361
00:11:28,533 --> 00:11:29,366
一个可阅读性

362
00:11:29,366 --> 00:11:30,433
的友好性

363
00:11:30,433 --> 00:11:32,433
那一是奔着这两个问题

364
00:11:32,466 --> 00:11:33,300
虽然说实话

365
00:11:33,333 --> 00:11:35,066
朱敏看完整篇的文章觉得

366
00:11:35,066 --> 00:11:39,666
DeepSeek R1 zero 说实话它真的很 outstanding

367
00:11:39,666 --> 00:11:40,733
很有创举

368
00:11:40,766 --> 00:11:41,966
但是 DeepSeek R1

369
00:11:41,966 --> 00:11:45,000
更多的是解决可能过程当中的很多

370
00:11:45,000 --> 00:11:46,200
一些可读性的问题

371
00:11:46,200 --> 00:11:48,000
所以做了很多相关的 trick

372
00:11:48,200 --> 00:11:48,866
其实也不多了

373
00:11:48,866 --> 00:11:49,666
也就两个

374
00:11:49,666 --> 00:11:51,466
那我们接下来看一下相关的内容

375
00:11:51,466 --> 00:11:51,866
第一个

376
00:11:51,866 --> 00:11:53,366
就是冷启动了

377
00:11:53,400 --> 00:11:55,466
这里面 DeepSeek R1 zero

378
00:11:55,466 --> 00:11:57,900
跟 DeepSeek R1 不同的就是构建了很多

379
00:11:57,933 --> 00:12:00,600
就收集了很多长 CoT 的数据

380
00:12:00,600 --> 00:12:03,866
作为 RL 的一个微调的过程

381
00:12:03,866 --> 00:12:05,233
那在整个过程当中

382
00:12:05,233 --> 00:12:07,966
就讲了它为了提升可读性

383
00:12:08,000 --> 00:12:08,600
所以说

384
00:12:08,600 --> 00:12:12,066
他专门去收集了一些 COT 相关的数据

385
00:12:12,066 --> 00:12:12,833
那另外的话

386
00:12:12,833 --> 00:12:15,300
我们接下来来到了 2.3.2 了

387
00:12:15,333 --> 00:12:18,333
以推理为导向的一个具体的强化学习

388
00:12:18,833 --> 00:12:20,333
在整个训练过程中

389
00:12:20,366 --> 00:12:22,533
为了做数据的冷启动

390
00:12:22,533 --> 00:12:25,033
其实收集了很多 COT 的数据

391
00:12:25,033 --> 00:12:26,966
那这里面的很多的 COT 的数据

392
00:12:27,000 --> 00:12:29,366
主要是为了缓解语言混杂的问题

393
00:12:29,366 --> 00:12:31,133
所以在强化学习里面

394
00:12:31,133 --> 00:12:31,833
就引入了

395
00:12:31,833 --> 00:12:33,466
语言一致性的一个奖励

396
00:12:33,466 --> 00:12:35,100
也就是额外的奖励

397
00:12:35,333 --> 00:12:36,833
不过这种额外的奖励

398
00:12:36,833 --> 00:12:37,966
会导致

399
00:12:38,000 --> 00:12:40,366
我们的一些模型的性能有所下降

400
00:12:40,366 --> 00:12:41,566
但是这种奖励

401
00:12:41,566 --> 00:12:45,033
是非常符合人类的偏好的 human performance

402
00:12:45,033 --> 00:12:47,900
也就方更方便我们人类的去阅读

403
00:12:47,933 --> 00:12:50,200
所以就加入了这么一个奖励

404
00:12:50,200 --> 00:12:52,733
损失的一点相关的性能

405
00:12:52,766 --> 00:12:54,600
接着就是 2.3.2 了

406
00:12:54,833 --> 00:12:56,833
就拒绝采样和监督微调

407
00:12:56,833 --> 00:12:57,666
那这个时候

408
00:12:57,666 --> 00:12:59,933
就迎来了我们很多的 trick 了

409
00:12:59,966 --> 00:13:02,266
也就是分阶段的微调

410
00:13:02,266 --> 00:13:04,166
分阶段的训练的过程当中

411
00:13:04,233 --> 00:13:06,866
在整个 RL 训练的过程当中

412
00:13:06,866 --> 00:13:08,566
等我们的 RL 收敛的时候

413
00:13:08,600 --> 00:13:10,233
就会用我们的 RL

414
00:13:10,233 --> 00:13:11,366
一个 CKPT

415
00:13:11,633 --> 00:13:13,433
去收集下一轮的一个数据

416
00:13:13,433 --> 00:13:14,900
也就是产生 collect

417
00:13:14,933 --> 00:13:16,833
SFT 的一个具体的数据

418
00:13:16,833 --> 00:13:18,100
那这个数据的产生

419
00:13:18,133 --> 00:13:20,233
就是基于 RL 的一个 check point

420
00:13:20,233 --> 00:13:23,066
就是我们的断点进行一个拒绝采样

421
00:13:23,066 --> 00:13:24,133
去收集的

422
00:13:24,166 --> 00:13:26,133
然后从我们的推理

423
00:13:26,333 --> 00:13:28,466
生成了很多推理的轨迹

424
00:13:28,666 --> 00:13:29,466
相关的内容

425
00:13:29,466 --> 00:13:30,466
那具体的数据

426
00:13:30,466 --> 00:13:32,733
就是 600K 条

427
00:13:32,833 --> 00:13:34,866
600K 条相关推理的数据

428
00:13:34,866 --> 00:13:35,333
当然了

429
00:13:35,366 --> 00:13:37,600
这里面有一些非推理的数据的

430
00:13:37,600 --> 00:13:38,366
不然的话

431
00:13:38,366 --> 00:13:41,433
我们整个模型可能会变得很奇怪

432
00:13:41,433 --> 00:13:43,566
他能够处理一些简单的逻辑推理

433
00:13:43,600 --> 00:13:46,266
他也能够做一些简单的一些问答

434
00:13:46,266 --> 00:13:47,900
当然了它更多不是用来做问答

435
00:13:47,933 --> 00:13:49,200
而是做逻辑推理

436
00:13:49,200 --> 00:13:49,833
那接着

437
00:13:49,833 --> 00:13:52,266
就后面的内容可能就不是很重要了

438
00:13:52,266 --> 00:13:53,500
针对所有的强化学习

439
00:13:53,533 --> 00:13:55,600
我们做一些简单的蒸馏

440
00:13:55,600 --> 00:13:57,400
最后就是一些 experiences 啦

441
00:13:57,400 --> 00:13:58,766
就是我们的实验

442
00:13:58,766 --> 00:14:00,133
做一些简单的消融实验

443
00:14:00,133 --> 00:14:03,566
那最后的一个 unsuccessful 的一个 attempt

444
00:14:03,733 --> 00:14:04,600
也是不成功

445
00:14:04,600 --> 00:14:07,966
的尝试特别是一个过程的奖励 PRM

446
00:14:07,966 --> 00:14:09,966
其实很多一个强化学的算法

447
00:14:09,966 --> 00:14:12,400
或者 RLHF 的相关的算法

448
00:14:12,400 --> 00:14:13,800
会用到过程奖励

449
00:14:13,800 --> 00:14:15,600
但是整个过程奖励的方法

450
00:14:15,600 --> 00:14:16,733
在实践过程当中

451
00:14:16,733 --> 00:14:18,533
发现它的效果并不好

452
00:14:18,533 --> 00:14:20,600
会陷入一个奖励黑客

453
00:14:20,766 --> 00:14:23,766
也就是 heck reward 相关的问题

454
00:14:23,766 --> 00:14:24,233
另外的话

455
00:14:24,233 --> 00:14:26,866
就是 MCTS 的蒙特卡洛搜索树

456
00:14:27,133 --> 00:14:28,233
相关的算法

457
00:14:28,233 --> 00:14:30,500
那一开始的时候其实参考了很多论文

458
00:14:30,533 --> 00:14:32,533
可能都用蒙特卡洛搜索树

459
00:14:32,533 --> 00:14:33,733
但是最后发现

460
00:14:33,733 --> 00:14:34,733
蒙特卡罗搜索树

461
00:14:34,733 --> 00:14:37,833
会让整个模型去不断的探索

462
00:14:37,833 --> 00:14:39,700
而且模型挑起来的也很难

463
00:14:39,733 --> 00:14:42,633
也很难去做大规模的扩展

464
00:14:42,800 --> 00:14:43,966
因为算法越复杂

465
00:14:44,166 --> 00:14:46,966
越很难的去做一个 scaling out

466
00:14:47,166 --> 00:14:47,833
那这个

467
00:14:47,833 --> 00:14:49,300
有点像当年的 Bert

468
00:14:49,333 --> 00:14:50,666
为什么没有 T5 好

469
00:14:50,733 --> 00:14:53,066
T5 没有那个

470
00:14:53,066 --> 00:14:54,533
GPT1、2、3 好

471
00:14:54,566 --> 00:14:57,266
就是因为整个 Bert 的网络模型到 encorder

472
00:14:57,266 --> 00:14:59,233
encorder decoder 的架构

473
00:14:59,233 --> 00:14:59,966
整体训练

474
00:15:00,000 --> 00:15:00,933
越来越复杂

475
00:15:00,966 --> 00:15:03,400
纯粹采用 decoder 架构更方便

476
00:15:03,400 --> 00:15:04,133
那于是

477
00:15:04,133 --> 00:15:06,733
这个整个 o1 的训练也是一样的

478
00:15:06,766 --> 00:15:08,033
采用了 MCTS

479
00:15:08,033 --> 00:15:09,500
可能导致我们的模型

480
00:15:09,533 --> 00:15:10,466
变得越来复杂

481
00:15:10,533 --> 00:15:11,666
算法越来越复杂

482
00:15:11,766 --> 00:15:13,066
更不利于我们 scale out

483
00:15:13,066 --> 00:15:15,866
只有 scale out 或者我们的 scaling law

484
00:15:15,966 --> 00:15:18,366
它才会使我们的模型效果越来越好

485
00:15:18,366 --> 00:15:19,866
然后整篇文章

486
00:15:19,866 --> 00:15:21,533
基本上就分享到这里了

487
00:15:21,566 --> 00:15:22,333
后面的内容

488
00:15:22,333 --> 00:15:23,266
就是一些

489
00:15:23,266 --> 00:15:23,933
contribution

490
00:15:23,966 --> 00:15:25,433
还有相关的

491
00:15:25,433 --> 00:15:27,566
preferences 相关的引用

492
00:15:28,133 --> 00:15:29,366
我们现在来到了

493
00:15:29,366 --> 00:15:30,666
这个视频的第二部分

494
00:15:30,666 --> 00:15:31,833
整个 DeepSeek r1 Zero

495
00:15:31,833 --> 00:15:33,566
跟 DeepSeek R1 相关的

496
00:15:33,600 --> 00:15:34,466
整体的流程

497
00:15:34,466 --> 00:15:35,900
说实话刚才讲了很多的

498
00:15:35,933 --> 00:15:36,966
论文不过

499
00:15:36,966 --> 00:15:38,666
我们总比这里面简单

500
00:15:38,666 --> 00:15:39,266
画了个图

501
00:15:39,266 --> 00:15:39,933
画了这个图

502
00:15:39,966 --> 00:15:41,366
没少花我时间

503
00:15:41,666 --> 00:15:43,033
整体的 DeepSeek R1-Zero

504
00:15:43,033 --> 00:15:45,133
整体的训练策略其实比较简单

505
00:15:45,233 --> 00:15:47,066
就是基于 DeepSeek V3

506
00:15:47,066 --> 00:15:48,933
这么一个预训脑的模型

507
00:15:48,966 --> 00:15:52,433
经过一个强化学习的算法叫做 GRPO

508
00:15:52,466 --> 00:15:54,333
然后去训练

509
00:15:54,366 --> 00:15:57,866
得到我们的 DeepSeek R1-Zero 这么一个模型

510
00:15:57,866 --> 00:16:00,833
那 DeepSeek R1-Zero 的模型的效果已经很好了

511
00:16:00,833 --> 00:16:03,733
只是简单通过一个强化学习的算法

512
00:16:04,166 --> 00:16:06,466
没有使用到 SFT

513
00:16:06,466 --> 00:16:08,566
微调还有模特卡罗搜索树

514
00:16:08,633 --> 00:16:09,566
那整体的

515
00:16:09,600 --> 00:16:10,400
它的一个好处

516
00:16:10,400 --> 00:16:12,233
就是能够自我进化

517
00:16:12,233 --> 00:16:15,100
初步具备了 test-time reasoning 的相关的能力

518
00:16:15,133 --> 00:16:18,266
也就说 o 系列 O1 O3 相关的模型的能力

519
00:16:18,266 --> 00:16:19,833
不过一个比较大的问题

520
00:16:19,833 --> 00:16:21,533
就是整个 Reasoning

521
00:16:21,566 --> 00:16:22,733
推理的过程当中

522
00:16:22,733 --> 00:16:24,233
它的可读性比较差

523
00:16:24,533 --> 00:16:27,633
经常出现一些中英文混淆的问题

524
00:16:27,866 --> 00:16:31,966
于是为了解决刚才的两个问题

525
00:16:32,200 --> 00:16:33,266
幻方

526
00:16:33,266 --> 00:16:35,333
就推出了一个 DeepSeek R1

527
00:16:35,566 --> 00:16:38,333
重新的去思考怎么去解决这些问题

528
00:16:38,333 --> 00:16:38,733
于是

529
00:16:38,733 --> 00:16:41,166
就出现上面的这么一个数据集了

530
00:16:41,166 --> 00:16:43,266
我有一个 cold-start 的一个 data

531
00:16:43,266 --> 00:16:45,133
也是能启动的数据

532
00:16:45,200 --> 00:16:47,966
经过对我们的 DeepSeek V3

533
00:16:47,966 --> 00:16:50,800
这么一个预训练的大模型的一个微调

534
00:16:50,800 --> 00:16:51,633
之后

535
00:16:51,966 --> 00:16:54,466
那我们有了一个微调后的大模型

536
00:16:54,466 --> 00:16:55,733
使用这个大模型

537
00:16:55,766 --> 00:16:57,166
进行一个 GRPO

538
00:16:57,166 --> 00:16:58,466
一个强化的过程

539
00:16:58,533 --> 00:17:00,466
得到中间的一个过程

540
00:17:00,466 --> 00:17:02,433
它也就是 DeepSeek R1 的中间态

541
00:17:02,766 --> 00:17:04,433
那 DeepSeek R1 的过程当中

542
00:17:04,433 --> 00:17:07,733
使用了另外一个算法叫做 rejection simple

543
00:17:07,766 --> 00:17:09,433
也要拒绝采样的方式

544
00:17:09,433 --> 00:17:11,833
去得到一些 reasoning 的 data

545
00:17:11,833 --> 00:17:14,566
我们推理的数据还有

546
00:17:14,600 --> 00:17:17,266
专门收集了一些 Reasoning 的 data

547
00:17:17,366 --> 00:17:20,433
再对我们的 DeepSeek V3

548
00:17:20,433 --> 00:17:21,733
这么一个预训练的模型

549
00:17:21,766 --> 00:17:23,000
进行一个微调

550
00:17:23,000 --> 00:17:24,333
微调完之后

551
00:17:24,333 --> 00:17:26,766
再执行一个 GRPO 的强化

552
00:17:26,800 --> 00:17:29,066
那经过了好几轮的迭代

553
00:17:29,066 --> 00:17:31,266
就分阶段的预训练之后

554
00:17:31,266 --> 00:17:33,900
或者分阶段的 post-training 之后

555
00:17:33,933 --> 00:17:36,866
就得到了效果非常好的 DeepSeek R1

556
00:17:37,366 --> 00:17:38,666
有了 DeepSeek R1 之后

557
00:17:38,666 --> 00:17:41,233
我们这个模型的推理能力已经很好了

558
00:17:41,233 --> 00:17:43,333
所以我可以对 Qwen

559
00:17:43,433 --> 00:17:45,900
对 Llama 进行一个蒸馏

560
00:17:46,033 --> 00:17:46,766
蒸馏完之后

561
00:17:46,800 --> 00:17:49,600
就得到了一系列网上公布的千问

562
00:17:49,600 --> 00:17:54,433
还有 Llama destination 相关的一个 R1 的模型

563
00:17:54,433 --> 00:17:55,633
那整体的流程

564
00:17:55,633 --> 00:17:57,133
说实话比较简单

565
00:17:57,166 --> 00:17:59,966
但是他尝试的过程和尝试的内容

566
00:17:59,966 --> 00:18:01,200
非常的有趣

567
00:18:02,633 --> 00:18:04,366
下面我们来到第三个内容

568
00:18:04,400 --> 00:18:07,000
ZOMI 在这个时候已经非常的疲惫了

569
00:18:07,066 --> 00:18:08,933
已经到深夜 3、4 点了

570
00:18:09,133 --> 00:18:10,033
没办法

571
00:18:10,033 --> 00:18:11,300
还是得解读完了

572
00:18:11,333 --> 00:18:12,966
那在整个 GRPO 的算法

573
00:18:12,966 --> 00:18:14,600
来看一下整体的原理

574
00:18:14,600 --> 00:18:16,433
因为最核心的文章了

575
00:18:16,433 --> 00:18:18,466
或最核心的内容就 GRPO 了

576
00:18:18,466 --> 00:18:20,533
说实话 GRPO 这篇文章

577
00:18:20,633 --> 00:18:24,433
最早是发布于 DeepSeek math 这篇文章的

578
00:18:24,433 --> 00:18:26,466
那这篇模型

579
00:18:26,466 --> 00:18:27,233
或者这个模型

580
00:18:27,233 --> 00:18:29,300
主要是做一个 DeepSeek 的数学推理

581
00:18:29,333 --> 00:18:31,566
也就是做 Reasoning 的时候去用的

582
00:18:31,566 --> 00:18:32,866
所以建议大家

583
00:18:32,866 --> 00:18:35,533
真正的去打开看 DeepSeek R1 的时候

584
00:18:35,566 --> 00:18:37,866
还是先看一下这篇文章

585
00:18:38,266 --> 00:18:40,533
不过其实蛮多人去问 ZOMI

586
00:18:40,566 --> 00:18:43,000
为什么 ZOMI 你搞一个 AI Infra 的人

587
00:18:43,000 --> 00:18:44,333
对算法这么了解了

588
00:18:44,333 --> 00:18:46,600
或者经常解读一些算法的过程当中

589
00:18:46,800 --> 00:18:47,766
想当年

590
00:18:47,766 --> 00:18:49,533
我进入华为之前

591
00:18:49,533 --> 00:18:51,800
也是一个研究算法的人了

592
00:18:52,000 --> 00:18:54,566
若干年前还写了一个

593
00:18:54,566 --> 00:18:56,600
就当大家都还没开始研究

594
00:18:56,600 --> 00:18:58,200
深度学习的时候

595
00:18:58,466 --> 00:19:01,566
我已经在研究神都强化学习了

596
00:19:08,100 --> 00:19:10,166
那不过没关系

597
00:19:10,200 --> 00:19:10,933
我们讲讲

598
00:19:10,933 --> 00:19:13,266
为什么 ZOMI 对整个 RL 的知识

599
00:19:13,266 --> 00:19:15,166
会这么的敏感

600
00:19:15,200 --> 00:19:16,933
或者这么的熟悉的原因了

601
00:19:16,933 --> 00:19:19,600
我们看一下整个 RL 加大语言模型

602
00:19:19,633 --> 00:19:21,500
现在在大语言模型里面

603
00:19:21,533 --> 00:19:22,866
IOHF 的算法

604
00:19:22,866 --> 00:19:24,933
主要是分开两大技术路线的

605
00:19:25,066 --> 00:19:27,700
第一个就是以 PPO 为代表的 on policy

606
00:19:28,066 --> 00:19:31,300
那另外一个是以 DPO 为代表的 off policy

607
00:19:31,433 --> 00:19:33,833
那 off policy 跟 on policy 里面

608
00:19:33,833 --> 00:19:35,866
它其实也有自己的优缺点

609
00:19:35,866 --> 00:19:36,166
第一个

610
00:19:36,200 --> 00:19:37,600
我们看一下 on policy

611
00:19:37,600 --> 00:19:38,866
现在用的比较多的

612
00:19:38,866 --> 00:19:40,566
就是每次训练的过程中

613
00:19:40,600 --> 00:19:44,366
都会有一个 actor 跟 critics 作为反馈奖励

614
00:19:44,366 --> 00:19:45,966
然后有一个老师

615
00:19:45,966 --> 00:19:46,933
有个教练

616
00:19:47,200 --> 00:19:49,366
去将我们的模型变得越来越好

617
00:19:49,366 --> 00:19:49,833
那这个

618
00:19:49,833 --> 00:19:52,166
就强化学习的一个最基本的一个

619
00:19:52,200 --> 00:19:53,833
PPO 的原理算法

620
00:19:53,833 --> 00:19:55,100
那这种方式的优点

621
00:19:55,133 --> 00:19:57,800
就是训练模型的效率比较高

622
00:19:57,800 --> 00:20:01,000
然后有一个 actor 跟 critics 去做引导

623
00:20:01,000 --> 00:20:03,233
但是训练后的模型的能力

624
00:20:03,233 --> 00:20:05,966
可能不是说非常的足够

625
00:20:06,233 --> 00:20:08,566
因为太依赖于 actor 跟 critics 了

626
00:20:08,600 --> 00:20:10,733
那另外就是 off policy 了

627
00:20:10,733 --> 00:20:11,366
off policy

628
00:20:11,366 --> 00:20:12,833
其实是基于现有的标注情况

629
00:20:12,833 --> 00:20:13,733
进行分析的

630
00:20:13,800 --> 00:20:14,600
那这个时候

631
00:20:14,600 --> 00:20:15,933
可能训练过程中

632
00:20:15,933 --> 00:20:17,633
会导致我们的训练的样本

633
00:20:17,633 --> 00:20:19,866
可能跟我们的模型并不匹配

634
00:20:19,966 --> 00:20:20,766
那比较优势

635
00:20:20,766 --> 00:20:21,200
就是

636
00:20:21,200 --> 00:20:24,600
模型很快就能够达到比较好的效果

637
00:20:24,833 --> 00:20:25,366
但是

638
00:20:25,400 --> 00:20:27,366
它的整体的模型的训练效率

639
00:20:27,366 --> 00:20:28,233
还是非常的

640
00:20:28,233 --> 00:20:29,333
低的那当然了

641
00:20:29,366 --> 00:20:31,866
on policy 跟 on policy 都有自己的优缺点

642
00:20:31,933 --> 00:20:36,866
现在用的最多的是 on policy 的 PPO 算法

643
00:20:36,866 --> 00:20:38,533
那整个 on Policy 的 PPO 算法

644
00:20:38,566 --> 00:20:40,533
其实非常的消耗资源的

645
00:20:40,533 --> 00:20:42,600
因为它有一个老师

646
00:20:42,600 --> 00:20:43,966
也就所谓的 actor model

647
00:20:43,966 --> 00:20:44,366
另外

648
00:20:44,366 --> 00:20:48,266
它有一个专家或者我们的教练 critic model

649
00:20:48,266 --> 00:20:50,333
在整个 critic model 之间

650
00:20:50,366 --> 00:20:53,533
我们还会有一个 mode model 跟 reference model

651
00:20:53,533 --> 00:20:54,866
一共有四个模型

652
00:20:54,866 --> 00:20:56,966
去指导我们整个 PPU 的算法

653
00:20:57,000 --> 00:20:58,766
说实话这个 PPO 的算法

654
00:20:58,866 --> 00:21:00,766
训练起来很麻烦

655
00:21:00,800 --> 00:21:02,466
很难很痛苦

656
00:21:02,566 --> 00:21:03,466
那而且

657
00:21:03,466 --> 00:21:06,233
它刚才讲到了有 4 个模型吗

658
00:21:06,233 --> 00:21:08,500
所以他要消耗巨大的资源

659
00:21:08,533 --> 00:21:10,033
和计算的负担

660
00:21:10,066 --> 00:21:12,100
而且在整个大语言模型当中

661
00:21:12,133 --> 00:21:13,200
我们一般来说

662
00:21:13,200 --> 00:21:14,733
会对最后一个 Token

663
00:21:14,766 --> 00:21:16,800
对我们的奖励模型进行打分

664
00:21:16,833 --> 00:21:18,633
但是在整个训练的过程中

665
00:21:18,633 --> 00:21:20,333
对每一个 Token 进行一个标注

666
00:21:20,366 --> 00:21:21,533
其实是非常难的

667
00:21:21,800 --> 00:21:24,866
所以为了为了避免像 PPO 那样了

668
00:21:24,866 --> 00:21:26,266
使用额外的 Value model

669
00:21:26,266 --> 00:21:28,266
和解决我们刚才的问题

670
00:21:28,366 --> 00:21:31,200
所以幻方在 DeepSeek Math 里面

671
00:21:31,200 --> 00:21:33,766
就提出了 GRPO 这个算法

672
00:21:33,833 --> 00:21:34,900
对于同一个问题

673
00:21:34,933 --> 00:21:35,966
也就同一个 prompt

674
00:21:36,066 --> 00:21:37,433
我们采用多个

675
00:21:37,433 --> 00:21:41,066
采用输出的平均奖励作为整体的基线

676
00:21:41,366 --> 00:21:44,800
那我们现在来看一下整个 GRPO 的论文

677
00:21:44,933 --> 00:21:46,266
这么一个模型

678
00:21:46,266 --> 00:21:47,766
那在整个上面

679
00:21:47,800 --> 00:21:49,166
我们主要看看上面

680
00:21:49,166 --> 00:21:51,633
那这个 q 就是我们提的一个问题

681
00:21:51,633 --> 00:21:54,533
也就是我们的 prompt 输给我们的 policy model

682
00:21:54,566 --> 00:21:55,333
policy model

683
00:21:55,333 --> 00:21:57,966
就是大家可以理解为一个大语言模型

684
00:21:57,966 --> 00:22:00,600
或者它就是一个 DeepSeek 幻方的一个训

685
00:22:00,600 --> 00:22:02,266
练模型那这个模型

686
00:22:02,266 --> 00:22:03,333
我们叫做 actor

687
00:22:03,633 --> 00:22:04,066
另外的话

688
00:22:04,066 --> 00:22:05,766
我们还会有一个 critics

689
00:22:05,800 --> 00:22:07,266
也就是我们对应的 value model

690
00:22:07,400 --> 00:22:09,166
另外它还有刚才讲到的两个 model

691
00:22:09,166 --> 00:22:09,966
一个 reference model

692
00:22:09,966 --> 00:22:11,600
一个是 value model

693
00:22:11,633 --> 00:22:13,033
一共四个模型

694
00:22:13,033 --> 00:22:14,300
中间的训练流程

695
00:22:14,333 --> 00:22:16,366
或者是它 RL 的学习流程

696
00:22:16,366 --> 00:22:18,366
是非常的复杂的

697
00:22:18,733 --> 00:22:20,233
整体的效非常的复杂的

698
00:22:20,233 --> 00:22:21,166
效率不好

699
00:22:21,233 --> 00:22:21,666
于是

700
00:22:21,666 --> 00:22:24,900
我们是否可以去干掉我们的 crisis model

701
00:22:24,933 --> 00:22:26,466
让我们整个 actor

702
00:22:26,466 --> 00:22:29,666
就不再受到 crisis 的一个指导

703
00:22:29,666 --> 00:22:31,866
直接去对齐我们的 RM

704
00:22:31,866 --> 00:22:33,966
也对齐我们的 Reward Modeling

705
00:22:34,166 --> 00:22:34,800
这个想法

706
00:22:34,800 --> 00:22:37,033
其实一开始是没有这个 critics model

707
00:22:37,033 --> 00:22:37,366
但是

708
00:22:37,400 --> 00:22:40,133
为了我们整个强化学习的训练

709
00:22:40,133 --> 00:22:41,200
更加稳定

710
00:22:41,233 --> 00:22:42,866
方差比较少的时候

711
00:22:42,866 --> 00:22:45,866
所以才加上我们的 critics model 了

712
00:22:45,866 --> 00:22:48,100
但是你想去掉 critics model

713
00:22:48,133 --> 00:22:49,733
又想要模型的效果好

714
00:22:49,933 --> 00:22:50,966
那我觉得很难的

715
00:22:50,966 --> 00:22:51,766
怎么办

716
00:22:51,866 --> 00:22:52,833
在这里面

717
00:22:52,833 --> 00:22:54,833
直接退化成为 Policy model

718
00:22:54,833 --> 00:22:56,266
是不合理的

719
00:22:56,266 --> 00:22:58,500
因此在整个模型的关键的

720
00:22:58,533 --> 00:22:59,800
或者整个算法的关键

721
00:22:59,800 --> 00:23:01,133
是找到一个 baseline

722
00:23:01,133 --> 00:23:03,366
使得我们的整体网络模型

723
00:23:03,366 --> 00:23:04,600
训练的过程中

724
00:23:04,600 --> 00:23:05,633
趋于稳定

725
00:23:05,633 --> 00:23:07,266
特别是强化学习的训练

726
00:23:07,266 --> 00:23:09,766
其实是很难去掌控的

727
00:23:09,800 --> 00:23:10,666
这也是 ZOMI

728
00:23:10,666 --> 00:23:12,466
当初去研究强化学习的时候

729
00:23:12,466 --> 00:23:14,266
为什么强化学习没火起来

730
00:23:14,366 --> 00:23:15,733
到现在才火起来了

731
00:23:15,733 --> 00:23:16,866
整个 LLM

732
00:23:16,866 --> 00:23:18,733
它的泛化性还是非常的好的

733
00:23:18,800 --> 00:23:22,400
那回到我们整个 GRPO 这个算法里面

734
00:23:22,400 --> 00:23:23,633
GRPO 的优化函数

735
00:23:23,633 --> 00:23:24,766
整体的大的框架

736
00:23:24,800 --> 00:23:27,933
其实是跟 PPO 算法是相对一致的

737
00:23:27,966 --> 00:23:28,866
改变的地方

738
00:23:28,866 --> 00:23:30,333
就是在于我们的

739
00:23:30,366 --> 00:23:32,766
AI 的一个具体的计算方式

740
00:23:32,766 --> 00:23:34,666
那 AI 在整个 PPO 里面

741
00:23:34,666 --> 00:23:37,366
是要需要通过 critics 去参与计算

742
00:23:37,400 --> 00:23:37,966
那整体

743
00:23:37,966 --> 00:23:40,166
我们 R 加上 V

744
00:23:40,166 --> 00:23:41,466
nice 减去 vs

745
00:23:41,466 --> 00:23:41,900
但是

746
00:23:41,933 --> 00:23:45,133
整个 GIPU 里面就没有了 criticist

747
00:23:45,200 --> 00:23:47,133
直接采用暴力

748
00:23:47,133 --> 00:23:49,933
采用 n 次求均值这种方式

749
00:23:50,166 --> 00:23:50,466
然后

750
00:23:50,466 --> 00:23:53,100
再跟我们的 Policy model 跟 Reference model

751
00:23:53,133 --> 00:23:54,966
去计算 KL 散度

752
00:23:54,966 --> 00:23:57,233
这样的话使得我们整体的模型

753
00:23:57,266 --> 00:23:58,933
就是训练的更快

754
00:23:58,966 --> 00:24:01,666
少了一个 critics 模型的一个内存

755
00:24:01,666 --> 00:24:03,666
还有计算量也减下去了

756
00:24:03,666 --> 00:24:05,500
而且只有一个 Policy model

757
00:24:05,533 --> 00:24:07,466
没有两个 model 来回的训练

758
00:24:07,533 --> 00:24:09,833
整体训练的稳定性也增加了

759
00:24:12,566 --> 00:24:15,133
因此总结一下整个 GRPO 算法

760
00:24:15,133 --> 00:24:17,966
就是没有额外的一个 Policy model 了

761
00:24:17,966 --> 00:24:18,933
criticis model

762
00:24:18,933 --> 00:24:20,866
跟奖励模型能够快速的对齐

763
00:24:20,866 --> 00:24:22,766
而且通过一个 KL 散度

764
00:24:22,800 --> 00:24:25,466
就是去处理我们的一个 loss

765
00:24:26,066 --> 00:24:26,966
听不懂没关系了

766
00:24:27,000 --> 00:24:28,866
只是简单的解读一下算法了

767
00:24:28,866 --> 00:24:30,366
后面已经 ZOMI

768
00:24:30,400 --> 00:24:32,333
也是希望能够深度的解读一下

769
00:24:32,333 --> 00:24:33,833
整个 RLHF 用到的

770
00:24:33,833 --> 00:24:35,733
强化学习用到的相关的算法了

771
00:24:35,866 --> 00:24:37,566
那现在我们来到了最后一个内容

772
00:24:37,600 --> 00:24:39,400
看一下对产业的整体的思考

773
00:24:39,400 --> 00:24:40,933
和相关的小结

774
00:24:41,333 --> 00:24:41,833
那这里面

775
00:24:41,833 --> 00:24:43,466
我们还是把这个图

776
00:24:43,466 --> 00:24:44,866
放大来去看一下了

777
00:24:44,866 --> 00:24:45,933
那整个 deepSeek

778
00:24:45,966 --> 00:24:47,333
总理觉得还是蛮有意思的

779
00:24:47,333 --> 00:24:48,533
首先 R1-Zero

780
00:24:48,533 --> 00:24:51,266
我觉得是一个算法的探索器

781
00:24:51,366 --> 00:24:52,733
怎么通过强化学习

782
00:24:52,733 --> 00:24:55,133
去实现我们的一个 test time reasoning

783
00:24:55,733 --> 00:24:57,566
那用什么 RL 算法

784
00:24:57,566 --> 00:24:59,066
可能就变得很核心了

785
00:24:59,066 --> 00:25:01,466
那这里面就幻方法直接采用他

786
00:25:01,466 --> 00:25:05,100
之前去训练 DeepSeek Math 相关的

787
00:25:05,133 --> 00:25:06,466
GRPO 相关的算法

788
00:25:06,533 --> 00:25:07,866
使得整个 R1 zero

789
00:25:07,866 --> 00:25:09,633
获得一个自进化的能力

790
00:25:09,633 --> 00:25:10,100
所以这个

791
00:25:10,133 --> 00:25:11,233
还是蛮有意思的

792
00:25:11,233 --> 00:25:11,933
那第二个问题

793
00:25:11,966 --> 00:25:13,800
就是我们在训练的过程当中

794
00:25:13,800 --> 00:25:15,933
怎么去设计整个奖励

795
00:25:16,200 --> 00:25:18,633
自己奖励的过程中其实挺难的

796
00:25:18,833 --> 00:25:21,433
于是就抛弃了一个过程奖励

797
00:25:21,533 --> 00:25:23,666
直接使用我们的一个准确性

798
00:25:23,666 --> 00:25:25,733
来去引导模型的最终的效果

799
00:25:26,066 --> 00:25:26,733
另外的话

800
00:25:26,766 --> 00:25:27,833
test time reasoning

801
00:25:27,833 --> 00:25:28,966
怎么去做搜索

802
00:25:29,000 --> 00:25:29,600
那这里面

803
00:25:29,600 --> 00:25:30,766
直接放弃搜索

804
00:25:30,766 --> 00:25:31,533
不搜索了

805
00:25:31,533 --> 00:25:33,733
我们通过 RL 的算法来去实现

806
00:25:33,766 --> 00:25:36,233
所以做到真正的一个 RL

807
00:25:36,233 --> 00:25:38,500
加大语言模型的开源的探索

808
00:25:38,533 --> 00:25:42,466
这也是 ZOMI 认为整个 R1 最核心的内容

809
00:25:42,466 --> 00:25:44,633
我们是对 RL 在大语言模型的

810
00:25:44,633 --> 00:25:47,133
一个最核心的探索内容

811
00:25:47,933 --> 00:25:50,400
那有了一个初期的探索之后

812
00:25:50,400 --> 00:25:51,866
我们真正的或者幻方

813
00:25:51,866 --> 00:25:53,033
来到了 R1

814
00:25:53,133 --> 00:25:55,466
怎么去提升 R1 的模型的效果

815
00:25:55,466 --> 00:25:57,966
于是经历了刚才讲到的 R1 Zero

816
00:25:58,000 --> 00:25:59,433
我的自我进化之后

817
00:25:59,533 --> 00:26:02,733
现在通过多阶段的 post training

818
00:26:02,766 --> 00:26:04,566
去获得更好的效果

819
00:26:04,566 --> 00:26:06,200
不过 ZOMI 更觉得

820
00:26:06,266 --> 00:26:08,966
R1 更多的是工程跟数据上的一个 trick

821
00:26:09,166 --> 00:26:10,200
先玩一下 trick

822
00:26:10,366 --> 00:26:12,166
真正具有创新意义的

823
00:26:12,166 --> 00:26:14,966
还是 R1 Zero 这么一个阶段

824
00:26:15,666 --> 00:26:18,966
那回看一下整个大模型的发展

825
00:26:19,033 --> 00:26:20,566
预测一下未来的趋势中

826
00:26:20,600 --> 00:26:22,333
你觉得几个点还蛮有意思的

827
00:26:22,333 --> 00:26:23,666
首先我们可以看到

828
00:26:23,666 --> 00:26:24,900
大模型的推理

829
00:26:24,933 --> 00:26:26,133
不管是多模态的推理

830
00:26:26,133 --> 00:26:27,466
还是大语言模型的推理

831
00:26:27,566 --> 00:26:29,466
基本上都走向集群了

832
00:26:29,566 --> 00:26:31,466
未来推理的云端的解决方案

833
00:26:31,466 --> 00:26:33,333
肯定会越来越多的

834
00:26:33,466 --> 00:26:35,100
推理的解决方案有了

835
00:26:35,133 --> 00:26:36,066
我们肯定会有

836
00:26:36,066 --> 00:26:37,166
推理的框架

837
00:26:37,533 --> 00:26:37,966
所以说

838
00:26:37,966 --> 00:26:40,200
我们之前看到的 MMDeploy

839
00:26:40,233 --> 00:26:42,700
或者其他的大于模型的推理框架

840
00:26:42,766 --> 00:26:44,433
用 C++来实现

841
00:26:44,566 --> 00:26:47,666
未来肯定会逐步的让位给像 vLLM

842
00:26:47,666 --> 00:26:48,366
SGLang

843
00:26:48,400 --> 00:26:51,366
等用 Python 来去实现的推理框架的

844
00:26:51,366 --> 00:26:53,633
这也是为什么现在这些推理框架

845
00:26:53,633 --> 00:26:57,066
在业界那么越来越多的火，云端

846
00:26:57,066 --> 00:27:00,066
真的不在乎那几微秒的时间

847
00:27:00,266 --> 00:27:02,666
而且用性可能占了一个大头

848
00:27:02,966 --> 00:27:03,800
那另外的话

849
00:27:03,800 --> 00:27:06,233
因为我们迎来一个 test time 的一个

850
00:27:06,233 --> 00:27:06,700
Reasoning

851
00:27:06,733 --> 00:27:09,333
所以我们整体的模型的训练的序列

852
00:27:09,333 --> 00:27:10,733
也会越来越长

853
00:27:10,733 --> 00:27:12,566
我们会把 COT 的数据

854
00:27:12,933 --> 00:27:15,766
这回我们整体的一个预训练模型

855
00:27:15,766 --> 00:27:17,066
进行一个微调的

856
00:27:17,066 --> 00:27:19,633
所以模型的训练肯定会越来越长

857
00:27:19,733 --> 00:27:21,766
而且是分阶段变长

858
00:27:21,800 --> 00:27:23,766
去进行一个运训练的

859
00:27:23,766 --> 00:27:27,066
跟幻方的 DeepSeek V3 的训练的过程差不多

860
00:27:27,066 --> 00:27:29,533
最后一个大模型的发展

861
00:27:29,566 --> 00:27:31,266
ZOMI 觉得肯定

862
00:27:31,433 --> 00:27:32,766
未来预训练模型

863
00:27:32,800 --> 00:27:34,566
会结合 O 系列的模型

864
00:27:34,566 --> 00:27:37,333
把整个大模型的一个推理的能力

865
00:27:37,333 --> 00:27:39,733
反补回去我们的预训练模型里面

866
00:27:39,733 --> 00:27:41,400
使得我们的预训练模型的能力

867
00:27:41,400 --> 00:27:42,666
也进一步的提升

868
00:27:42,733 --> 00:27:44,033
所以 O 系列的模型

869
00:27:44,033 --> 00:27:46,100
也就幻方的 R 系列的模型

870
00:27:46,166 --> 00:27:47,566
跟 V 系列的模型

871
00:27:47,566 --> 00:27:49,733
他们肯定是互相博弈

872
00:27:49,733 --> 00:27:50,966
互相学习的

873
00:27:51,866 --> 00:27:52,533
那最后

874
00:27:52,566 --> 00:27:54,866
我们还是要看回产业的

875
00:27:54,966 --> 00:27:56,033
那对产业的影响

876
00:27:56,033 --> 00:27:57,333
总理觉得还是蛮有意思的

877
00:27:57,366 --> 00:27:57,933
一开始

878
00:27:57,933 --> 00:28:00,066
我们其实不一定很乐观

879
00:28:00,066 --> 00:28:02,900
预计整个 2025 年到 2026 年

880
00:28:03,133 --> 00:28:05,233
整个 scaling 能不能继续的膨胀

881
00:28:05,233 --> 00:28:08,233
不过从幻方这个 R1 的模型来看

882
00:28:08,266 --> 00:28:10,333
R1 都到了 671B 了

883
00:28:10,366 --> 00:28:12,533
模型越大效果越好

884
00:28:12,766 --> 00:28:14,733
整个 pre training

885
00:28:14,766 --> 00:28:15,633
预训练的模型

886
00:28:15,633 --> 00:28:17,266
scaling 还是在继续的

887
00:28:17,533 --> 00:28:18,000
第二个

888
00:28:18,000 --> 00:28:19,533
就是 R1 的 Zero

889
00:28:19,633 --> 00:28:21,100
直接使用 RL

890
00:28:21,400 --> 00:28:23,233
在 post training 过程当中

891
00:28:23,233 --> 00:28:25,033
直接去后训练的

892
00:28:25,066 --> 00:28:27,066
直接抛弃了 SFT

893
00:28:27,200 --> 00:28:28,633
而且 R1

894
00:28:28,633 --> 00:28:30,966
使用了一个 Reasoning 的 COT

895
00:28:31,000 --> 00:28:32,266
进行 SFT 的

896
00:28:32,333 --> 00:28:33,366
所以未来

897
00:28:33,366 --> 00:28:35,733
post training 加 test time 的 scaling

898
00:28:35,800 --> 00:28:36,766
也在继续

899
00:28:36,766 --> 00:28:38,266
也就是我们的 scaling law

900
00:28:38,633 --> 00:28:40,033
还在的延续

901
00:28:40,366 --> 00:28:41,433
那第三个

902
00:28:41,433 --> 00:28:43,166
未来因为有 O1

903
00:28:43,200 --> 00:28:46,666
或者 R1 这类的一个逻辑推理的模型

904
00:28:46,833 --> 00:28:48,933
所以我们可能会在垂直领域

905
00:28:49,566 --> 00:28:51,333
会出现越来越多的 AIAgent

906
00:28:51,333 --> 00:28:54,400
卷离线的推理场景

907
00:28:54,433 --> 00:28:55,566
跟推理芯片

908
00:28:55,600 --> 00:28:56,666
走向何方

909
00:28:56,666 --> 00:29:00,100
因为我们未来肯定会迎来越来越多的

910
00:29:00,133 --> 00:29:02,200
云端的解决方案

911
00:29:02,266 --> 00:29:04,433
这也是 ZOMI 对未来的推理

912
00:29:04,566 --> 00:29:05,966
有点担心的原因

913
00:29:05,966 --> 00:29:06,833
对训练

914
00:29:06,833 --> 00:29:10,266
因为现在训练的服务器都来做推理的

915
00:29:10,266 --> 00:29:11,366
解决方案

916
00:29:11,666 --> 00:29:12,533
推理的芯片

917
00:29:12,566 --> 00:29:16,433
推理的场景未来会不断的增加吗

918
00:29:16,433 --> 00:29:19,500
还是会有序稳定的增加

919
00:29:19,833 --> 00:29:20,533
这个问题

920
00:29:20,566 --> 00:29:21,266
抛给各位

921
00:29:21,266 --> 00:29:22,500
那在最后

922
00:29:22,933 --> 00:29:23,966
已经基本上结束了

923
00:29:23,966 --> 00:29:26,666
这一期视频的内容和解读了

924
00:29:26,666 --> 00:29:27,833
那这整个 PPT

925
00:29:27,833 --> 00:29:29,133
是开源在这条链接

926
00:29:29,166 --> 00:29:30,566
欢迎大家去取用

927
00:29:30,566 --> 00:29:31,733
那相关的引用

928
00:29:31,733 --> 00:29:34,666
也是留给读者去做一些简单的思考了

929
00:29:34,666 --> 00:29:35,300
那这里面

930
00:29:35,333 --> 00:29:36,200
就到这里为止

931
00:29:36,200 --> 00:29:36,666
谢谢各位

932
00:29:36,666 --> 00:29:37,466
拜了个拜

933
00:29:37,600 --> 00:29:38,800
累的不行了

934
00:29:38,866 --> 00:29:39,900
赶紧睡觉

935
00:29:40,333 --> 00:29:40,933
卷的不行了

936
00:29:40,933 --> 00:29:41,866
卷的不行了

937
00:29:41,866 --> 00:29:43,533
AI 系统的全套知识都在这里

938
00:29:43,566 --> 00:29:45,200
欢迎跟着目录进行学习

939
00:29:45,600 --> 00:29:46,533
给我一键三连

940
00:29:46,533 --> 00:29:47,733
给我一键三连

941
00:29:48,200 --> 00:29:48,866
谢谢各位

942
00:29:48,866 --> 00:29:49,900
拜了个拜

