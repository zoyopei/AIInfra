1
00:00:00,000 --> 00:00:02,600
内容/录制:Z0MI 酱，视频后期/字幕:梁嘉铭 

2
00:00:02,600 --> 00:00:03,266
hello 大家好

3
00:00:03,300 --> 00:00:06,500
我是那个睡得晚晚起得早早的 ZOMI

4
00:00:08,033 --> 00:00:10,266
距离这一期的视频

5
00:00:10,300 --> 00:00:12,500
其实已经过去了一年了

6
00:00:12,500 --> 00:00:12,966
一年前

7
00:00:12,966 --> 00:00:14,833
ZOMI 已经录了一个第一个内容

8
00:00:14,833 --> 00:00:16,433
就是整个 transformer 架构

9
00:00:16,633 --> 00:00:18,266
transformer 的前世今生

10
00:00:18,300 --> 00:00:19,500
那有兴趣的小伙伴

11
00:00:19,500 --> 00:00:22,133
也可以翻回 ZOMI 之前

12
00:00:22,233 --> 00:00:24,400
1 年前正是这一个时间节点

13
00:00:24,400 --> 00:00:26,466
分享的一些 transformer 的内容

14
00:00:26,500 --> 00:00:28,700
那因为工作需要

15
00:00:28,700 --> 00:00:29,166
最近

16
00:00:29,166 --> 00:00:32,100
ZOMI 就陆续的分析了一个 transformer

17
00:00:32,133 --> 00:00:32,933
一些 Token

18
00:00:32,933 --> 00:00:33,566
Embedding

19
00:00:33,566 --> 00:00:35,566
还有 Attention 相关的内容

20
00:00:35,566 --> 00:00:35,833
接着

21
00:00:35,833 --> 00:00:38,266
我们将会来分享一个 Attention 的变种

22
00:00:38,300 --> 00:00:39,633
各种各样的变种

23
00:00:39,633 --> 00:00:41,066
那了解完所有的变种之后

24
00:00:41,100 --> 00:00:43,466
我们将会来到了一个现在

25
00:00:43,466 --> 00:00:45,466
包括 DeepSeek 和 Kimi 还是豆包哪个

26
00:00:45,500 --> 00:00:48,700
大模型最关心的就是长序列的问题

27
00:00:48,800 --> 00:00:50,466
那了解完整个 transformer 的架构

28
00:00:50,500 --> 00:00:51,533
基本的原理

29
00:00:51,533 --> 00:00:52,700
之后我们来到了

30
00:00:52,700 --> 00:00:55,300
整个大模型的参数怎么去配

31
00:00:55,300 --> 00:00:57,933
我们这些参数之间是什么关系

32
00:00:57,933 --> 00:01:00,366
例如我的一个 Tokenizer，Embedding

33
00:01:00,366 --> 00:01:02,400
还有一个 hidden size

34
00:01:02,400 --> 00:01:04,066
NLA kv cache 的头

35
00:01:04,233 --> 00:01:05,966
各种各样的相关的概念

36
00:01:05,966 --> 00:01:06,800
就会在这里面

37
00:01:06,800 --> 00:01:08,433
重点的去澄清了

38
00:01:08,566 --> 00:01:09,100
 

39
00:01:09,133 --> 00:01:11,966
我们今天还是来到了第二个内容

40
00:01:11,966 --> 00:01:14,833
核心机制的 Tokenizer

41
00:01:14,833 --> 00:01:17,800
那这里面我们要思考几个问题

42
00:01:18,000 --> 00:01:20,566
就是大家都说 Token Token 非常重要

43
00:01:20,566 --> 00:01:22,033
现在大模型收费

44
00:01:22,033 --> 00:01:23,300
都是按多少 Token

45
00:01:23,333 --> 00:01:24,366
input Token 多少

46
00:01:24,366 --> 00:01:26,200
output Token 多少来进行收费

47
00:01:26,200 --> 00:01:28,400
现在我们在正式以下的内容之前

48
00:01:28,400 --> 00:01:30,600
跟大家一起去讨论

49
00:01:30,600 --> 00:01:31,566
带着这几个问题

50
00:01:31,566 --> 00:01:33,800
进行一个思考往下走的那第

51
00:01:33,800 --> 00:01:34,233
一个问题

52
00:01:34,233 --> 00:01:35,833
就是到底 Token

53
00:01:35,966 --> 00:01:38,433
Tokenizer 之间是什么关系

54
00:01:38,433 --> 00:01:40,900
那我们经常讲 Token 词元

55
00:01:40,933 --> 00:01:43,033
这个 Tokenizer 到底意味着什么

56
00:01:43,100 --> 00:01:44,166
那第二个问题就是

57
00:01:44,333 --> 00:01:45,700
为什么 transformer 的模型

58
00:01:45,733 --> 00:01:49,566
需要对文本进行 Tokenization

59
00:01:49,633 --> 00:01:51,400
那这里面引用了 3 个名词

60
00:01:51,400 --> 00:01:53,233
都是 Tokenizer 的一个衍生词

61
00:01:53,233 --> 00:01:54,200
一个 Tokenizer

62
00:01:54,200 --> 00:01:55,300
一个 Tokenization

63
00:01:55,600 --> 00:01:57,366
它们之间到底什么关系

64
00:01:57,766 --> 00:01:58,500
那了解完之后

65
00:01:58,533 --> 00:01:59,900
我们看一下 Token

66
00:01:59,900 --> 00:02:00,400
实际上

67
00:02:00,400 --> 00:02:02,666
我们可以理解 Token 为一个词元

68
00:02:02,700 --> 00:02:04,500
那我们通过某些算法

69
00:02:04,500 --> 00:02:06,433
把词元进行分割

70
00:02:06,433 --> 00:02:09,166
那这些分割的方法我们叫 Tokenizer

71
00:02:09,233 --> 00:02:11,266
分割完的单词我们叫 Tokenization

72
00:02:11,366 --> 00:02:13,166
那为什么我们这些单词

73
00:02:13,166 --> 00:02:16,166
不直接按空格或者直接分词

74
00:02:16,166 --> 00:02:17,766
那例如我们在英文里面

75
00:02:17,766 --> 00:02:19,366
直接按英文就好了

76
00:02:19,366 --> 00:02:20,600
在这句话

77
00:02:20,600 --> 00:02:22,600
就是 ZOMI 最近

78
00:02:22,600 --> 00:02:24,866
就是美国对华为芯片

79
00:02:24,900 --> 00:02:26,400
昇腾芯片的打压嘛

80
00:02:26,400 --> 00:02:29,433
所以说出台了一个美国的管制法规

81
00:02:29,500 --> 00:02:32,166
那我把它翻译成一个英文

82
00:02:32,266 --> 00:02:35,033
应该是把美国的法规翻译成中文

83
00:02:35,033 --> 00:02:36,166
那为什么我们英文

84
00:02:36,166 --> 00:02:38,666
不直接按空格进行分词

85
00:02:38,766 --> 00:02:40,600
还是真的直接按空格进行分词

86
00:02:40,600 --> 00:02:42,233
丢给大模型进行训练

87
00:02:42,366 --> 00:02:43,200
那我们中文

88
00:02:43,200 --> 00:02:44,866
会按什么方式来进行分词

89
00:02:44,900 --> 00:02:45,600
这蛮有意思

90
00:02:45,600 --> 00:02:47,500
我们有很多相关的概念

91
00:02:47,533 --> 00:02:47,966
这里面

92
00:02:47,966 --> 00:02:50,500
就跟大家一起去探讨和澄清

93
00:02:50,600 --> 00:02:50,900
可能

94
00:02:50,900 --> 00:02:52,800
很多人并不关心这里面的细节

95
00:02:52,800 --> 00:02:53,600
没关系

96
00:02:53,600 --> 00:02:55,566
我们简单来看一下今天的视频

97
00:02:55,566 --> 00:02:56,633
可能会有点流水账

98
00:02:56,633 --> 00:02:57,500
有点长

99
00:02:57,533 --> 00:02:58,300
那我们整体

100
00:02:58,300 --> 00:03:00,400
应该是分开 5 个内容

101
00:03:00,400 --> 00:03:00,833
那第一个

102
00:03:00,833 --> 00:03:02,366
就是 Tokenizer 的基本概念

103
00:03:02,366 --> 00:03:03,166
就我们怎么把

104
00:03:03,166 --> 00:03:04,966
Token 变成 Tokenization

105
00:03:05,100 --> 00:03:06,333
变成怎么分词

106
00:03:06,433 --> 00:03:07,466
那了解完基本概念之后

107
00:03:07,500 --> 00:03:09,600
我们看一下整体的完整的流程

108
00:03:09,600 --> 00:03:10,866
从第一步到第二步

109
00:03:10,900 --> 00:03:12,733
到我们分完词之后

110
00:03:12,733 --> 00:03:13,733
他应该长什么样子

111
00:03:13,733 --> 00:03:16,233
我们就会在这里面跟大家去展示

112
00:03:16,333 --> 00:03:17,300
那展示完之后

113
00:03:17,300 --> 00:03:18,766
就会深入的看一下

114
00:03:18,766 --> 00:03:21,800
里面进行分词的那个算法

115
00:03:21,800 --> 00:03:22,566
一个深度解析

116
00:03:22,566 --> 00:03:23,666
看一下现在

117
00:03:23,733 --> 00:03:26,333
大模型都用什么分词的算法

118
00:03:26,333 --> 00:03:27,233
英文用什么

119
00:03:27,233 --> 00:03:28,400
中文用什么

120
00:03:28,433 --> 00:03:29,266
可能法文

121
00:03:29,300 --> 00:03:31,300
还有各种各样的文都用什么

122
00:03:31,300 --> 00:03:32,500
这里面蛮有意思

123
00:03:32,500 --> 00:03:33,333
分完词之后

124
00:03:33,333 --> 00:03:35,400
怎么评估它到底好还是不好呀

125
00:03:35,433 --> 00:03:37,233
怎么个分词算法好

126
00:03:37,233 --> 00:03:37,833
所以这里面

127
00:03:37,833 --> 00:03:39,966
我们来到了一个评估指标

128
00:03:40,000 --> 00:03:41,000
那了解完一切之后

129
00:03:41,000 --> 00:03:41,866
我们可能简单

130
00:03:41,900 --> 00:03:44,700
跟大家一起去做一个代码的编写

131
00:03:44,700 --> 00:03:46,433
那因为是实时的编写

132
00:03:46,433 --> 00:03:47,966
代码如果有错报错

133
00:03:47,966 --> 00:03:50,000
大家不要去耻笑 ZOMI

134
00:03:50,000 --> 00:03:52,166
毕竟 ZOMI 是好久没有编码了

135
00:03:52,166 --> 00:03:56,200
从一个研发到架构师再到现在的生态

136
00:03:56,200 --> 00:04:01,233
我们现在马上的进入到第一个内容

137
00:04:02,633 --> 00:04:03,666
我们这里面

138
00:04:03,700 --> 00:04:06,500
保持热情又不失礼貌

139
00:04:06,700 --> 00:04:08,300
比较无聊的跟大家去分享一下

140
00:04:08,300 --> 00:04:10,000
Tokenizer 的一个基本概念

141
00:04:10,433 --> 00:04:13,466
首先我们了解一下什么是词元

142
00:04:13,500 --> 00:04:15,400
Token 那词元这个名字

143
00:04:15,400 --> 00:04:17,600
也是去年黄仁勋

144
00:04:17,600 --> 00:04:19,000
在他的一个大会里面

145
00:04:19,000 --> 00:04:20,000
就说了哎

146
00:04:20,000 --> 00:04:22,366
Token 其实在中文里面叫词元

147
00:04:22,366 --> 00:04:22,900
那这里面

148
00:04:22,933 --> 00:04:25,433
我们还是要简单的看一下语言模型

149
00:04:25,433 --> 00:04:28,433
为什么要将我们看到的这些文本

150
00:04:28,533 --> 00:04:29,833
转换成为数字

151
00:04:29,833 --> 00:04:32,233
那这个其实跟 ASCII 编码

152
00:04:32,233 --> 00:04:33,566
其实比较类似

153
00:04:33,566 --> 00:04:35,066
把单字符映

154
00:04:35,100 --> 00:04:37,900
射到语义的单元

155
00:04:37,900 --> 00:04:39,333
不管你看不懂没关系

156
00:04:39,333 --> 00:04:40,833
我们现在来思考一个问题

157
00:04:40,833 --> 00:04:44,100
就是 Token 跟单词之间的关系

158
00:04:44,133 --> 00:04:46,733
一个 Token 到底能代表几个单词

159
00:04:46,733 --> 00:04:48,133
或者汉字

160
00:04:48,166 --> 00:04:49,466
那我们这里面

161
00:04:49,500 --> 00:04:52,033
就简单的跟大家一起去看一看了

162
00:04:52,033 --> 00:04:53,833
首先英文里面的 Token

163
00:04:53,833 --> 00:04:55,100
有点类似于拼图

164
00:04:55,133 --> 00:04:56,133
那看不懂没关系

165
00:04:56,133 --> 00:04:56,700
我们看一下

166
00:04:56,700 --> 00:04:57,233
一个 Token

167
00:04:57,233 --> 00:04:58,800
我们简单的总结

168
00:04:58,800 --> 00:05:03,466
大约就对应 0.5 个单词或者 3-4 个字母

169
00:05:03,533 --> 00:05:05,000
那为什么不都是单词

170
00:05:05,000 --> 00:05:07,300
因为在我们真正切词的时候

171
00:05:07,333 --> 00:05:09,800
或者我们正在 Tokenizer 的算法的时候

172
00:05:09,800 --> 00:05:12,033
它确实被切成各种各样的字母

173
00:05:12,033 --> 00:05:15,066
例如 unhappy 可能会切成 “un”

174
00:05:15,100 --> 00:05:16,100
还有 “happi”

175
00:05:16,100 --> 00:05:17,233
还有“ness”

176
00:05:17,300 --> 00:05:18,333
那三个 Token

177
00:05:18,333 --> 00:05:19,966
就好像把它拼起来

178
00:05:19,966 --> 00:05:20,900
所以说一个 Token

179
00:05:20,933 --> 00:05:23,566
我们可能对应的是一个单词

180
00:05:23,566 --> 00:05:24,600
或者半个单词

181
00:05:24,600 --> 00:05:26,366
或者 3 到 5 个字母

182
00:05:26,366 --> 00:05:27,166
那这里面

183
00:05:27,166 --> 00:05:28,633
我们就说到了 OpenAI

184
00:05:28,633 --> 00:05:30,166
就说到了 1,000 个 Token

185
00:05:30,166 --> 00:05:33,500
大约能装下 750 个英文单词

186
00:05:33,533 --> 00:05:34,500
那我们后面

187
00:05:34,500 --> 00:05:35,733
也会重点的去看一下

188
00:05:35,733 --> 00:05:37,233
为什么会这么去做

189
00:05:37,233 --> 00:05:38,833
大家记住就行了

190
00:05:38,833 --> 00:05:40,833
一个 Token 是 0.75 个单词

191
00:05:41,066 --> 00:05:42,966
按照我们现在的 Token 的算法

192
00:05:42,966 --> 00:05:43,566
实际上

193
00:05:43,566 --> 00:05:45,866
我们输入的英文的单词

194
00:05:45,900 --> 00:05:46,966
可能会省点钱

195
00:05:46,966 --> 00:05:49,433
输入中文可能会费点钱

196
00:05:49,633 --> 00:05:52,166
那我们中文里面来看一下汉字

197
00:05:52,166 --> 00:05:53,266
就相当于打包了

198
00:05:53,300 --> 00:05:54,366
在中文里面

199
00:05:54,366 --> 00:05:58,400
一个 Token 相当于对应 1 到 1.8 个汉字

200
00:05:58,400 --> 00:06:00,700
因为我们汉字经常会切词

201
00:06:00,733 --> 00:06:01,600
例如你好世界

202
00:06:01,600 --> 00:06:04,033
我们可能会偏切成 你 好

203
00:06:04,033 --> 00:06:06,633
然后 世 界 可能会组成一个单词

204
00:06:06,766 --> 00:06:08,100
那这里面我们就变

205
00:06:08,133 --> 00:06:08,966
成了一个 Token

206
00:06:08,966 --> 00:06:11,900
会对应 1 到 1.8 个字汉字

207
00:06:12,166 --> 00:06:13,000
那不管怎么样

208
00:06:13,000 --> 00:06:15,500
我们简单的感性的来去看一下

209
00:06:15,533 --> 00:06:16,600
Tokenizer

210
00:06:16,600 --> 00:06:18,766
就是 Token 的算法

211
00:06:18,766 --> 00:06:19,400
核心任务

212
00:06:19,400 --> 00:06:21,633
就是把一个文本

213
00:06:21,633 --> 00:06:25,233
映射成为对应的一个 ID 的序

214
00:06:25,233 --> 00:06:28,566
是每一个单词都有一个对应的 ID 序

215
00:06:28,566 --> 00:06:30,833
这个就是对应的序列了

216
00:06:30,833 --> 00:06:31,500
就是 this

217
00:06:31,533 --> 00:06:32,966
可能是 2023

218
00:06:32,966 --> 00:06:34,033
那为什么 2023

219
00:06:34,033 --> 00:06:35,566
我们怎样后面去看

220
00:06:35,600 --> 00:06:36,966
那最后

221
00:06:36,966 --> 00:06:37,900
有了 Tokenizer

222
00:06:37,933 --> 00:06:39,500
变成 ID 序之后

223
00:06:39,500 --> 00:06:41,333
就给到大模型进行训练了

224
00:06:41,333 --> 00:06:42,000
embedding

225
00:06:42,000 --> 00:06:45,166
就是我们大模型的第一层输入

226
00:06:45,166 --> 00:06:47,000
那就是 input

227
00:06:47,100 --> 00:06:48,733
切成词之后

228
00:06:48,733 --> 00:06:49,700
变成序列之后

229
00:06:49,700 --> 00:06:51,833
就给整个大模型进行输入了

230
00:06:51,833 --> 00:06:54,266
那右边的这个就是拉马的经典结构

231
00:06:54,300 --> 00:06:55,533
就是我们整个 Transformer

232
00:06:55,533 --> 00:06:56,800
一个具体输入

233
00:06:57,966 --> 00:07:00,266
刚才我们似乎学到了一点东西

234
00:07:00,300 --> 00:07:03,033
我似乎学到了一些没有用的知识

235
00:07:05,900 --> 00:07:08,300
我们看一下 Tokenizer 的一个完整流程

236
00:07:08,300 --> 00:07:09,900
就刚才整个过程了

237
00:07:09,900 --> 00:07:11,633
切词的过程其实是 Tokenizer

238
00:07:11,633 --> 00:07:12,966
我们整个 Tokenizer 里面

239
00:07:12,966 --> 00:07:14,600
其实需要进行预处理

240
00:07:14,600 --> 00:07:16,466
一般分为 4 个步骤

241
00:07:16,500 --> 00:07:17,100
第一个步骤

242
00:07:17,100 --> 00:07:18,766
就是 Normalization

243
00:07:18,766 --> 00:07:20,066
把我们一个单词

244
00:07:20,100 --> 00:07:21,800
我们刚只是跳跃的讲

245
00:07:21,800 --> 00:07:23,533
就是一句完整的话

246
00:07:23,533 --> 00:07:25,433
然后变成我们切完词之后

247
00:07:25,433 --> 00:07:26,266
最后

248
00:07:26,300 --> 00:07:28,833
把这些对应到一个序列 id

249
00:07:28,833 --> 00:07:29,500
那中间

250
00:07:29,533 --> 00:07:30,633
可能还有很多过程

251
00:07:30,633 --> 00:07:32,500
第一个过程就是 normalization

252
00:07:32,533 --> 00:07:34,100
对文本进行清洗

253
00:07:34,100 --> 00:07:35,833
还有一些数字的处理

254
00:07:35,900 --> 00:07:36,433
那第二个

255
00:07:36,433 --> 00:07:38,900
就是叫做 pre-Tokenization 就

256
00:07:38,933 --> 00:07:39,800
是我们基于规则

257
00:07:39,800 --> 00:07:42,566
对我们进行的一些简单的切分

258
00:07:42,566 --> 00:07:43,200
那第三个

259
00:07:43,200 --> 00:07:44,000
就是 modeling

260
00:07:44,000 --> 00:07:45,633
这个 modeling 就是我们后面

261
00:07:45,633 --> 00:07:48,000
下个内容会讲的核心的算法

262
00:07:48,000 --> 00:07:49,433
BPR work pieces

263
00:07:49,433 --> 00:07:52,066
等句子等子词切分的算法

264
00:07:52,133 --> 00:07:54,333
那有了这些算法切完之后

265
00:07:54,333 --> 00:07:56,000
我们就会进行第四步

266
00:07:56,000 --> 00:07:57,766
Step4，post-Tokenizer

267
00:07:57,766 --> 00:07:58,700
那那有个 Pre

268
00:07:58,733 --> 00:07:59,733
有个 post

269
00:07:59,733 --> 00:08:01,900
其实做一些特殊的标志位

270
00:08:01,900 --> 00:08:03,833
那我们这里面可以看一下

271
00:08:03,833 --> 00:08:05,166
具体有什么差别

272
00:08:05,166 --> 00:08:07,800
接下来会逐个的去展开

273
00:08:07,800 --> 00:08:09,566
首先刚才讲到第一步

274
00:08:09,566 --> 00:08:11,600
就是 Normalization 归一化

275
00:08:11,766 --> 00:08:14,166
说实话我们所有的任何句子

276
00:08:14,166 --> 00:08:15,033
或者任何单词

277
00:08:15,033 --> 00:08:16,200
或者任何 Token

278
00:08:16,200 --> 00:08:17,266
或者任何东西

279
00:08:17,300 --> 00:08:18,733
输给我们大模型之前

280
00:08:18,733 --> 00:08:21,300
肯定还是要进行一个预处理

281
00:08:21,333 --> 00:08:23,333
例如去掉一些没有用的字符了

282
00:08:23,333 --> 00:08:24,400
额外的空白

283
00:08:24,400 --> 00:08:26,766
还有我们经常看到的回车键

284
00:08:26,900 --> 00:08:28,800
这些我们还是要去掉

285
00:08:28,800 --> 00:08:30,666
另外的话我们会统一

286
00:08:30,700 --> 00:08:31,966
标准化一些写法

287
00:08:31,966 --> 00:08:33,100
例如大小的统一

288
00:08:33,133 --> 00:08:34,333
数字格式的统一

289
00:08:34,333 --> 00:08:37,366
还有一些字符的标准的统一

290
00:08:37,400 --> 00:08:40,033
统一用某些字符进行处理

291
00:08:40,033 --> 00:08:40,866
因为不然的话

292
00:08:40,900 --> 00:08:43,233
我们看到一段对话里面

293
00:08:43,233 --> 00:08:45,266
有可能用那个半圆形

294
00:08:45,300 --> 00:08:46,566
有可能用全圆形

295
00:08:46,600 --> 00:08:47,766
有可能用这种

296
00:08:47,766 --> 00:08:48,666
有可能用这种

297
00:08:48,700 --> 00:08:50,000
是吧我们一句话里面

298
00:08:50,000 --> 00:08:50,666
这就不对了

299
00:08:50,700 --> 00:08:52,733
所以我们还是要进行一个预处理

300
00:08:52,766 --> 00:08:55,166
那当然我们还会有一些安全跟规范化

301
00:08:55,166 --> 00:08:57,500
例如里面说脏话了是不得去掉

302
00:08:57,533 --> 00:08:59,333
那这里面是数据预处理的内容

303
00:08:59,333 --> 00:08:59,766
但是

304
00:08:59,766 --> 00:09:02,200
我们在 Token 的预处理过程当中

305
00:09:02,200 --> 00:09:05,066
也会做一些相关安全跟规范性的工作

306
00:09:05,166 --> 00:09:06,033
那来接着

307
00:09:06,033 --> 00:09:07,700
我们来到了第二个内容

308
00:09:07,700 --> 00:09:09,433
STEP2 就预处理的第二部分

309
00:09:09,433 --> 00:09:11,200
就 Pre-Tokenization

310
00:09:11,233 --> 00:09:12,633
那这里面 Pre-Tokenization

311
00:09:12,633 --> 00:09:14,866
主要是做一些基于简单规则

312
00:09:14,900 --> 00:09:18,100
一些初步的文本的分割

313
00:09:18,166 --> 00:09:20,066
那我们可以简单的理解为

314
00:09:20,100 --> 00:09:21,433
就是把文本

315
00:09:21,433 --> 00:09:24,066
进行拆分为一些比较小的单元

316
00:09:24,100 --> 00:09:26,166
例如句子跟词语

317
00:09:26,166 --> 00:09:26,700
那英文

318
00:09:26,733 --> 00:09:29,733
我们可能会直接用空格来进行分割

319
00:09:29,733 --> 00:09:32,766
那中文我们可能就没有这一步了

320
00:09:32,766 --> 00:09:35,833
那直接用算法进行一个分词了

321
00:09:36,033 --> 00:09:38,666
那刚才我们比较感性的看待 hello

322
00:09:38,700 --> 00:09:40,766
how are you today

323
00:09:40,800 --> 00:09:44,100
我们接着分完词之后就会变成 hello，

324
00:09:44,133 --> 00:09:44,533
how， are ，you

325
00:09:44,533 --> 00:09:47,766
用每一个都变成一个具体的单词嘛

326
00:09:47,766 --> 00:09:49,466
然后现在我们来到了第三个内容

327
00:09:49,500 --> 00:09:50,566
看一下 modeling

328
00:09:50,566 --> 00:09:52,500
也就是一个核心算法

329
00:09:52,933 --> 00:09:53,900
在第三步的时候

330
00:09:53,900 --> 00:09:57,300
我们首先要选定对应的一个分词

331
00:09:57,300 --> 00:09:58,233
Tokenization 的算法

332
00:09:58,233 --> 00:09:58,500
当然

333
00:09:58,533 --> 00:10:00,566
Tokenization 的算法我们后面会讲到

334
00:10:00,566 --> 00:10:01,833
有 BPE WordPiece

335
00:10:01,833 --> 00:10:05,300
Unigram 还有 SentencePiece 很多

336
00:10:05,400 --> 00:10:07,366
使用了对应的算法之后

337
00:10:07,366 --> 00:10:08,033
那这些算法

338
00:10:08,033 --> 00:10:10,666
我们需要输入大量的文本的数据

339
00:10:10,700 --> 00:10:11,733
然后根据模型

340
00:10:11,733 --> 00:10:14,433
算法就会生成词汇表了 

341
00:10:14,433 --> 00:10:17,066
所以我们经常会看到 vocabulary

342
00:10:17,133 --> 00:10:18,566
一个词汇表

343
00:10:18,566 --> 00:10:20,266
接着就可以根据词汇表

344
00:10:20,300 --> 00:10:23,566
将文本拆分成对应的 Token 了

345
00:10:23,733 --> 00:10:26,000
看这里面可能有点抽象

346
00:10:26,000 --> 00:10:27,766
我们打开浏览器

347
00:10:28,300 --> 00:10:28,933
那我们现在

348
00:10:28,933 --> 00:10:31,900
打开 hugging face 里面的 Qwen3

349
00:10:31,900 --> 00:10:34,166
然后看一下 file and version

350
00:10:34,333 --> 00:10:35,766
里面就有很多相关

351
00:10:35,766 --> 00:10:36,800
例如我们可以看到

352
00:10:36,800 --> 00:10:37,600
切完词之后

353
00:10:37,600 --> 00:10:40,900
我们就会生成一个 Tokenizer.json 和

354
00:10:40,933 --> 00:10:42,533
Tokenizer_config.json

355
00:10:42,566 --> 00:10:44,300
还有 Vocab.json

356
00:10:44,433 --> 00:10:46,966
这三个文件都非常的重要

357
00:10:46,966 --> 00:10:50,033
我们后面将会在代码演示里面

358
00:10:50,033 --> 00:10:52,566
去讲讲这三个文件分别有什么作用

359
00:10:53,133 --> 00:10:53,733
那我们现在

360
00:10:53,733 --> 00:10:57,200
还是回到对应的一个全流程里面

361
00:10:57,200 --> 00:10:59,866
我们 Tokenization 一共有 4 步

362
00:10:59,900 --> 00:11:01,033
那刚才讲到

363
00:11:01,033 --> 00:11:02,833
会根据模型算法来选定

364
00:11:02,833 --> 00:11:04,400
然后产生一个词汇表

365
00:11:04,400 --> 00:11:06,266
然后真正的进行拆分 Token

366
00:11:06,333 --> 00:11:07,833
那词汇表就非常重要

367
00:11:07,833 --> 00:11:09,233
但是我们现在一般来说

368
00:11:09,233 --> 00:11:11,766
如果默认用 BPE 这个算法

369
00:11:11,766 --> 00:11:14,233
基本上词汇表也会固定

370
00:11:14,233 --> 00:11:15,066
因为整个一届

371
00:11:15,100 --> 00:11:16,600
已经做了很多大量的示范了

372
00:11:16,600 --> 00:11:18,800
就不需要整个流程重新走一遍了

373
00:11:18,800 --> 00:11:19,966
但是一开始的时候

374
00:11:19,966 --> 00:11:21,966
或者你想真的自己去端到端训练

375
00:11:21,966 --> 00:11:22,500
一个大模型

376
00:11:22,533 --> 00:11:24,800
连词你都自己去训

377
00:11:24,800 --> 00:11:26,033
那恭喜你

378
00:11:26,100 --> 00:11:28,233
你需要走这么几步

379
00:11:28,300 --> 00:11:29,900
但是现在训练大模型

380
00:11:29,900 --> 00:11:32,566
基本上就把一个第一步

381
00:11:32,566 --> 00:11:34,266
然后直接跳过最后一步

382
00:11:34,300 --> 00:11:36,033
经过一个 modeling BPE 算法

383
00:11:36,033 --> 00:11:37,033
然后丢进去

384
00:11:37,033 --> 00:11:38,300
我们刚才看到

385
00:11:38,333 --> 00:11:41,700
一个 vocabulary 跟 Tokenization 就结束了

386
00:11:41,700 --> 00:11:43,733
就输出一个 Token

387
00:11:43,733 --> 00:11:45,133
之后的一个 ID

388
00:11:45,166 --> 00:11:47,600
那这个也是现在的一个常见的做法

389
00:11:47,933 --> 00:11:48,400
不管怎么样

390
00:11:48,400 --> 00:11:49,966
我们是看回最后一步

391
00:11:49,966 --> 00:11:52,066
就是 post Tokenization 了

392
00:11:52,100 --> 00:11:54,166
那这里面可能会做的更多的是

393
00:11:54,166 --> 00:11:56,900
于是添加一些特殊的 Token 的标志位

394
00:11:56,933 --> 00:11:59,700
例如整个句子的起始跟结束

395
00:11:59,700 --> 00:12:00,733
包括我一个

396
00:12:00,733 --> 00:12:02,100
在现在推理模型里面

397
00:12:02,100 --> 00:12:05,133
会加入一个 think 跟 end think

398
00:12:05,233 --> 00:12:07,400
就是停止思考相关的内容

399
00:12:07,400 --> 00:12:09,866
当然我们还有一些注意力的编码

400
00:12:09,900 --> 00:12:12,600
还有包括填充一些 padding

401
00:12:12,600 --> 00:12:14,500
或者填充一些没有用的 Token 进去

402
00:12:14,533 --> 00:12:16,433
方便我们做一个截断

403
00:12:16,600 --> 00:12:18,766
那了解完之后我们看一下

404
00:12:18,766 --> 00:12:20,233
里面我们刚才讲到了

405
00:12:20,233 --> 00:12:22,433
其实有两个概念

406
00:12:22,500 --> 00:12:24,533
第一个就是关键的参数

407
00:12:24,533 --> 00:12:26,033
词汇表的大小

408
00:12:26,033 --> 00:12:28,466
让我们看回浏览器

409
00:12:28,500 --> 00:12:29,366
我们可以看到

410
00:12:29,366 --> 00:12:31,900
vocabulary 这个词汇表还是蛮大

411
00:12:31,933 --> 00:12:35,100
2.78 mb 那虽然看上去很小

412
00:12:35,100 --> 00:12:36,600
但是里面的词汇表

413
00:12:36,600 --> 00:12:40,500
已经有 15 万个单词或者 15 万个词了

414
00:12:40,600 --> 00:12:41,900
那我们就可以看到了

415
00:12:41,933 --> 00:12:44,333
大词表跟小词表的区别

416
00:12:44,333 --> 00:12:47,000
15 万跟 5 万的词表到底有什么区别

417
00:12:47,166 --> 00:12:49,266
那比较明显的就是大词表

418
00:12:49,300 --> 00:12:50,733
可以覆盖更多的词

419
00:12:50,733 --> 00:12:53,533
小词表查询效率就更高

420
00:12:53,533 --> 00:12:54,900
所以说但现在

421
00:12:54,900 --> 00:12:56,100
基本上大家

422
00:12:56,100 --> 00:12:58,133
会根据模型去区分

423
00:12:58,133 --> 00:13:00,300
我们可以看到 DeepSeek

424
00:13:00,300 --> 00:13:02,966
它整体的词汇表大概是 10 万

425
00:13:02,966 --> 00:13:07,866
但是千问这个词汇表大概是 15 万

426
00:13:07,900 --> 00:13:09,900
那可以看到最大的区别就是

427
00:13:09,900 --> 00:13:11,700
DeepSeek 这个十万的词汇表

428
00:13:11,700 --> 00:13:13,600
大概只覆盖中英文

429
00:13:13,600 --> 00:13:15,266
但是千问的这个词汇表

430
00:13:15,300 --> 00:13:17,833
可以发盖 50 多种语言

431
00:13:17,866 --> 00:13:20,633
当然了 deep think 更多的聚焦在思考嘛

432
00:13:20,633 --> 00:13:21,400
然后千问

433
00:13:21,400 --> 00:13:24,000
更多的在各种各样的能力都能处理

434
00:13:24,000 --> 00:13:25,833
所以说词汇表的大小

435
00:13:25,833 --> 00:13:26,966
也需要进行权衡

436
00:13:26,966 --> 00:13:27,600
到底用 10 万

437
00:13:27,600 --> 00:13:29,066
5 万还是 15 万

438
00:13:29,100 --> 00:13:30,700
还是 20 万

439
00:13:30,700 --> 00:13:31,233
那这个

440
00:13:31,233 --> 00:13:33,966
就大家需要去权衡自己的算法了

441
00:13:33,966 --> 00:13:36,066
那我们现在来看一下具体的一个例子

442
00:13:36,100 --> 00:13:37,833
打开这条链接

443
00:13:38,533 --> 00:13:40,000
那我们可以看到

444
00:13:40,000 --> 00:13:40,833
现在一打开

445
00:13:40,833 --> 00:13:42,700
就是 Tokenization 的整体的算法

446
00:13:42,733 --> 00:13:46,366
我们一开始输入的就是 You are a helpful assistant

447
00:13:46,366 --> 00:13:48,433
那蛮有意思

448
00:13:48,433 --> 00:13:50,566
这里面就会把整个句子

449
00:13:50,566 --> 00:13:53,366
就像我们刚才经过了 post Tokenization

450
00:13:53,366 --> 00:13:54,966
就加入很多的这种

451
00:13:54,966 --> 00:13:56,300
所谓的标志位

452
00:13:56,433 --> 00:13:57,033
那不管了

453
00:13:57,033 --> 00:13:58,700
你不管它是什么标志位

454
00:13:58,733 --> 00:14:00,233
其实最核心的就是这句话

455
00:14:00,233 --> 00:14:02,400
You are a helpful assistant

456
00:14:02,833 --> 00:14:04,266
那这里面我们可以看到

457
00:14:04,300 --> 00:14:07,200
就真正的就会把它切成一个一个单词

458
00:14:07,200 --> 00:14:08,966
里面经过了 Token 之后

459
00:14:08,966 --> 00:14:09,866
我们每个单词

460
00:14:09,900 --> 00:14:10,933
经过查表

461
00:14:10,933 --> 00:14:13,833
查词汇表就会变成这么一个序列了

462
00:14:13,833 --> 00:14:14,866
那我们可以看到

463
00:14:14,900 --> 00:14:17,633
如果我覆盖两个 system 会怎么样

464
00:14:17,633 --> 00:14:19,666
system 那可以看到 13

465
00:14:19,700 --> 00:14:21,233
17360 是 system

466
00:14:21,300 --> 00:14:22,500
17360 是 system

467
00:14:22,500 --> 00:14:24,366
它基本上就查词汇表了

468
00:14:24,366 --> 00:14:26,866
我们首先通过 Tokenization 的算法

469
00:14:26,933 --> 00:14:29,733
把我们这句话呀切成一个个单词

470
00:14:29,733 --> 00:14:31,700
然后通过我们刚才的几个步骤

471
00:14:31,700 --> 00:14:33,366
变成这么一堆东西

472
00:14:33,400 --> 00:14:34,233
加了标志位

473
00:14:34,233 --> 00:14:36,400
经过 post Tokenization

474
00:14:36,400 --> 00:14:38,566
和 Pre Tokenization 的处理之后

475
00:14:38,566 --> 00:14:40,066
就变成这一坨东西了

476
00:14:40,400 --> 00:14:42,400
处理完之后就变成我们最右的了

477
00:14:42,400 --> 00:14:44,066
所以说蛮有意思

478
00:14:44,100 --> 00:14:47,100
我们再加个比较有意思的单词

479
00:14:47,100 --> 00:14:50,633
unhappiness 那蛮有意思

480
00:14:50,633 --> 00:14:51,466
就 unhappiness

481
00:14:51,500 --> 00:14:52,800
就分成两个了

482
00:14:53,000 --> 00:14:55,100
UNH 是吧分成一个

483
00:14:55,133 --> 00:14:59,166
然后 APPINESS 又分为一个 Token

484
00:14:59,666 --> 00:15:00,966
所以说这里面

485
00:15:00,966 --> 00:15:04,566
我们一共有 1234566 个单词

486
00:15:04,566 --> 00:15:06,466
但是消耗了 18 个 Token

487
00:15:06,800 --> 00:15:07,500
所以不要觉得

488
00:15:07,533 --> 00:15:09,400
我可能输入的 Token 没那么多

489
00:15:09,400 --> 00:15:11,866
但是我最后结算的时候怎么这么多

490
00:15:11,966 --> 00:15:13,100
哎你去算钱的话

491
00:15:13,133 --> 00:15:14,533
它是按 Token 来算

492
00:15:14,533 --> 00:15:16,833
不是按你有个多少个单词来算

493
00:15:16,833 --> 00:15:18,233
我们注意这个点呐

494
00:15:19,533 --> 00:15:20,533
看到这里为止

495
00:15:20,533 --> 00:15:22,366
如果你对 Token 的算法的实

496
00:15:22,366 --> 00:15:24,566
现并没有那么的感兴趣

497
00:15:24,566 --> 00:15:26,633
其实已经可以结束这个视频了

498
00:15:26,633 --> 00:15:28,766
接下来我们已经会深入一些

499
00:15:28,833 --> 00:15:30,300
更加无聊的内容

500
00:15:30,333 --> 00:15:31,800
就是分词的算法

501
00:15:31,800 --> 00:15:33,100
一个深度的解析

502
00:15:33,133 --> 00:15:35,000
讲一下我们有哪些分词算法

503
00:15:35,000 --> 00:15:35,566
那我们看一下

504
00:15:35,566 --> 00:15:38,500
整个分词算法其实有一个演进史

505
00:15:38,533 --> 00:15:39,200
那一开始

506
00:15:39,200 --> 00:15:40,766
我们是基于一个单词

507
00:15:40,766 --> 00:15:42,633
所以叫做 word based

508
00:15:42,700 --> 00:15:44,600
那按照单词进行分词

509
00:15:44,600 --> 00:15:45,500
例如英文里面

510
00:15:45,533 --> 00:15:47,166
today is Sunday

511
00:15:47,166 --> 00:15:47,866
那我们这个

512
00:15:47,900 --> 00:15:49,133
就会根据空格

513
00:15:49,133 --> 00:15:49,800
空格

514
00:15:49,800 --> 00:15:52,166
标点符号进行分割

515
00:15:52,166 --> 00:15:53,300
那比较明确嘛

516
00:15:53,333 --> 00:15:53,966
那第二个

517
00:15:53,966 --> 00:15:57,400
就是基于那个字符

518
00:15:57,533 --> 00:15:58,400
那比较明确

519
00:15:58,400 --> 00:16:00,466
就是按照单词进行分词

520
00:16:00,500 --> 00:16:01,900
就有点粗了

521
00:16:01,900 --> 00:16:03,000
就所以说大家诶

522
00:16:03,000 --> 00:16:05,666
是不是可以基于那个字符进行分词

523
00:16:05,700 --> 00:16:07,800
所以叫做 character based

524
00:16:07,966 --> 00:16:08,633
那这里面

525
00:16:08,633 --> 00:16:09,366
我们看一下

526
00:16:09,366 --> 00:16:10,233
也是同样

527
00:16:10,233 --> 00:16:11,833
today is Sunday

528
00:16:11,866 --> 00:16:12,266
这里面

529
00:16:12,300 --> 00:16:14,133
就每个单词分开

530
00:16:14,600 --> 00:16:17,400
那后来大家觉得这种方式太细了

531
00:16:17,500 --> 00:16:20,000
所以就搞了一个子词的分词

532
00:16:20,000 --> 00:16:21,100
那所谓的子词

533
00:16:21,133 --> 00:16:22,733
就是 Subword based

534
00:16:22,733 --> 00:16:26,533
现在不管是 Bert 还是一开始

535
00:16:26,533 --> 00:16:27,900
应该是最原始的 GPT

536
00:16:27,900 --> 00:16:28,833
还是 birth 了

537
00:16:28,833 --> 00:16:29,966
还是 Llama

538
00:16:29,966 --> 00:16:31,766
还是现在的一个千问

539
00:16:31,800 --> 00:16:33,666
都基于子词进行分词

540
00:16:33,700 --> 00:16:36,800
例如还是刚才的那个句子

541
00:16:36,800 --> 00:16:38,600
today is Sunday

542
00:16:38,600 --> 00:16:39,100
那这里面

543
00:16:39,133 --> 00:16:42,133
我们就会把它切成 today ，is ， sun，day

544
00:16:42,333 --> 00:16:43,600
把它分成一个单词

545
00:16:43,600 --> 00:16:45,000
day 也分成一个单词

546
00:16:45,000 --> 00:16:45,700
那这种方式

547
00:16:45,733 --> 00:16:47,233
就是子词了

548
00:16:47,233 --> 00:16:48,633
那我们看一下子词里面

549
00:16:48,633 --> 00:16:50,000
其实还会继续划分

550
00:16:50,100 --> 00:16:50,933
那不管怎么样

551
00:16:50,933 --> 00:16:51,966
我们先来看一下

552
00:16:51,966 --> 00:16:54,666
每一个内容的一个优缺点

553
00:16:54,766 --> 00:16:56,633
首先基于 word base

554
00:16:56,633 --> 00:16:59,500
就是非常直观的符合一个看法

555
00:16:59,533 --> 00:16:59,966
但是

556
00:16:59,966 --> 00:17:03,533
这样会导致词汇表爆炸

557
00:17:03,566 --> 00:17:05,533
那经常会出现一个 OOV

558
00:17:05,533 --> 00:17:06,433
所谓的 OOV

559
00:17:06,433 --> 00:17:08,000
就是 out of vocabulary 了

560
00:17:08,000 --> 00:17:09,866
就我们刚才讲到的大词表

561
00:17:09,900 --> 00:17:11,966
我们全世界有这么多单词

562
00:17:12,000 --> 00:17:13,033
都按这么分

563
00:17:13,033 --> 00:17:15,066
怎么可能分得过来嘛

564
00:17:15,100 --> 00:17:16,533
我一个单词里面

565
00:17:16,533 --> 00:17:18,033
就经常导致大模型

566
00:17:18,033 --> 00:17:20,000
没有办法很好地学习里面的知识

567
00:17:20,000 --> 00:17:24,066
例如 dog 跟 dogs 好像它是一个意思

568
00:17:24,100 --> 00:17:27,900
happy 跟 unhappy 好像是一个意思

569
00:17:27,900 --> 00:17:31,166
但是 happy 跟 unhappy 它是有关联关系

570
00:17:31,166 --> 00:17:34,100
你不可以把 happy 跟 unhappy 完全区别出来

571
00:17:34,366 --> 00:17:35,433
所以说这个时候

572
00:17:35,433 --> 00:17:38,366
我们整个基于 word base 的分词

573
00:17:38,366 --> 00:17:41,666
就没有很好的去表示语义嘛

574
00:17:41,900 --> 00:17:44,633
来接着我们看一下基于 character based

575
00:17:44,633 --> 00:17:45,200
那这个

576
00:17:45,200 --> 00:17:47,966
就是导致词汇表极小

577
00:17:47,966 --> 00:17:48,666
特别的小

578
00:17:48,700 --> 00:17:51,333
所有英文单词就那么 26 个字母

579
00:17:51,333 --> 00:17:53,300
但是最大的问题就是

580
00:17:53,366 --> 00:17:56,066
丢失了语义的信息

581
00:17:56,100 --> 00:17:58,300
所以说很少人去用 character based

582
00:17:58,333 --> 00:18:01,100
这里面只是随机这么一说

583
00:18:01,100 --> 00:18:03,000
那现在用的更多的就是 subword-base

584
00:18:03,000 --> 00:18:03,766
subword-base 里面

585
00:18:03,766 --> 00:18:05,800
又分为很多种算法了

586
00:18:05,800 --> 00:18:07,400
那比较核心的思想

587
00:18:07,400 --> 00:18:09,633
就是将一些高频的词

588
00:18:09,733 --> 00:18:11,800
那我们刚才的一个 Sunday

589
00:18:11,800 --> 00:18:14,366
sun 跟 day 呀拆分出来

590
00:18:14,366 --> 00:18:15,900
就把高频的词

591
00:18:15,933 --> 00:18:18,433
切分成为更细粒度的子词

592
00:18:18,500 --> 00:18:19,900
那低频的这些词

593
00:18:19,900 --> 00:18:22,000
就保留完整的词

594
00:18:22,000 --> 00:18:22,866
例如 is 

595
00:18:22,866 --> 00:18:24,500
is 你就没必要拆嘛

596
00:18:24,533 --> 00:18:25,633
me 呀，i 呀

597
00:18:25,633 --> 00:18:26,500
我呀，你呀

598
00:18:26,533 --> 00:18:27,766
他呀

599
00:18:27,766 --> 00:18:29,000
这些没必要拆

600
00:18:29,000 --> 00:18:30,766
所以说低频的词

601
00:18:30,766 --> 00:18:33,100
就保存完整的一个词

602
00:18:33,100 --> 00:18:34,400
那这个优势

603
00:18:34,400 --> 00:18:37,466
就是解决了 out of vocabulary 的问题

604
00:18:37,500 --> 00:18:39,300
使得词汇表

605
00:18:39,300 --> 00:18:42,166
基本上固定到 5 万到 15 万之间

606
00:18:42,166 --> 00:18:43,000
那现在大模型

607
00:18:43,000 --> 00:18:45,166
基本上词汇表就是这么大

608
00:18:45,500 --> 00:18:47,533
那我们看回这个网站

609
00:18:47,533 --> 00:18:48,966
这里面的 vocabulary

610
00:18:48,966 --> 00:18:53,233
基本上就会固定在 1.5M 到 3M 之间

611
00:18:53,233 --> 00:18:56,266
就我们刚才讲到的 5 万到 15 万之间

612
00:18:56,300 --> 00:18:59,133
那典型的是 word-base 的算法

613
00:18:59,166 --> 00:19:00,566
主要是三种

614
00:19:00,566 --> 00:19:02,466
第一种就是 BPE

615
00:19:02,500 --> 00:19:04,800
也就是(Byte Pair Encoding)

616
00:19:04,800 --> 00:19:06,100
第二种就是 WordPiece

617
00:19:06,133 --> 00:19:06,600
第三种

618
00:19:06,600 --> 00:19:07,833
就是 Unigram

619
00:19:08,133 --> 00:19:10,200
那现在所有的大模型

620
00:19:10,200 --> 00:19:12,833
用的最多的就是 BPE

621
00:19:12,833 --> 00:19:14,433
不管是中文还是英文

622
00:19:14,433 --> 00:19:15,233
反正

623
00:19:15,233 --> 00:19:18,300
整个业界已经达成共识了用 BPE 算法

624
00:19:18,400 --> 00:19:18,966
所以说后面

625
00:19:18,966 --> 00:19:20,900
我们也会重点的去跟大家解读一下

626
00:19:20,933 --> 00:19:22,000
这个算法啦

627
00:19:22,733 --> 00:19:25,900
哎我们现在马上来到了这个 BPE 算法

628
00:19:25,900 --> 00:19:28,433
也就(Byte Pair Encoding)

629
00:19:28,433 --> 00:19:28,900
这个算法

630
00:19:28,933 --> 00:19:32,000
其实大家可以参考一下这一篇论文了

631
00:19:32,000 --> 00:19:33,966
忘记了是什么时候写的了

632
00:19:34,000 --> 00:19:34,666
那不管怎么样

633
00:19:34,700 --> 00:19:35,566
我们简单的看一下

634
00:19:35,566 --> 00:19:37,566
这篇论文的一个算法流程

635
00:19:37,566 --> 00:19:40,366
首先我们构建初始化的词表

636
00:19:40,366 --> 00:19:41,566
我们按字符的力度

637
00:19:41,566 --> 00:19:43,966
将每一个单词进行一个切分了

638
00:19:43,966 --> 00:19:45,566
单独一个 Token

639
00:19:45,633 --> 00:19:47,166
那接着我们去识别

640
00:19:47,166 --> 00:19:49,600
最频繁出现的一些标记的对

641
00:19:49,633 --> 00:19:51,100
那需要对语料

642
00:19:51,133 --> 00:19:52,333
进行抽样的扫描

643
00:19:52,333 --> 00:19:53,433
因为现在的语料太多了

644
00:19:53,433 --> 00:19:54,800
不可能所有都扫描

645
00:19:54,800 --> 00:19:56,700
所以说抽样的扫描

646
00:19:56,733 --> 00:19:59,733
然后找出那频率最高的 Token 对

647
00:19:59,966 --> 00:20:00,466
那接着

648
00:20:00,500 --> 00:20:03,633
我们就会合并这些频繁出现的 Token

649
00:20:03,633 --> 00:20:05,900
对两两进行合并

650
00:20:05,933 --> 00:20:07,100
然后合并完之后

651
00:20:07,100 --> 00:20:08,766
再覆盖掉新的词表里

652
00:20:08,766 --> 00:20:10,166
面进行一个递归

653
00:20:10,166 --> 00:20:11,500
再两两合并

654
00:20:11,700 --> 00:20:14,233
那最后一步就是重复 2 和 3 了

655
00:20:14,400 --> 00:20:15,466
不断的扫描

656
00:20:15,500 --> 00:20:16,433
不断的合并

657
00:20:16,433 --> 00:20:17,200
不断的扫描

658
00:20:17,200 --> 00:20:18,266
不断的合并

659
00:20:18,533 --> 00:20:19,233
直到

660
00:20:19,233 --> 00:20:21,900
指定词汇表大小或者 Token 对

661
00:20:21,933 --> 00:20:24,133
不能两两在频繁的合并

662
00:20:24,233 --> 00:20:25,900
或者出现为止吧

663
00:20:25,933 --> 00:20:27,166
我一般来这么做

664
00:20:27,166 --> 00:20:29,500
那刚才讲的太抽象了

665
00:20:29,533 --> 00:20:32,966
我们简单的以一个例子来看一下

666
00:20:32,966 --> 00:20:34,233
hello world peace

667
00:20:34,600 --> 00:20:36,500
世界和平

668
00:20:36,533 --> 00:20:37,633
那这么一个句子

669
00:20:37,633 --> 00:20:38,433
我们首先

670
00:20:38,433 --> 00:20:40,866
会把这么一个句子进行一个拆分

671
00:20:40,900 --> 00:20:42,733
在词库表里面查到

672
00:20:42,733 --> 00:20:43,533
然后 hello

673
00:20:43,533 --> 00:20:44,766
就出现了 6 次

674
00:20:44,766 --> 00:20:45,900
what 出现了 8 次

675
00:20:45,933 --> 00:20:47,800
pieces 出现了两次

676
00:20:47,800 --> 00:20:48,366
不管怎么样

677
00:20:48,366 --> 00:20:51,466
我们将所有的单词拆分为每个单字

678
00:20:51,600 --> 00:20:54,200
所以我们可以看到 h 有出现了 6 次

679
00:20:54,200 --> 00:20:55,300
e 出现了 10 次

680
00:20:55,333 --> 00:20:56,800
以此类推

681
00:20:57,200 --> 00:20:57,633
那接着

682
00:20:57,633 --> 00:20:59,066
我们频繁

683
00:20:59,100 --> 00:21:02,200
合并最频繁出现的一个单字了

684
00:21:02,200 --> 00:21:05,366
那例如 l 跟 o 就会频繁的出现

685
00:21:05,366 --> 00:21:07,033
所以我们把它进行合并

686
00:21:07,566 --> 00:21:08,133
接着

687
00:21:08,133 --> 00:21:11,533
我们进行回到刚才的这一步

688
00:21:11,600 --> 00:21:13,000
再进行频繁的合并

689
00:21:13,000 --> 00:21:16,166
那就发现了 LOE 出现了 10 次

690
00:21:16,166 --> 00:21:19,800
那反复迭代直到条件满足

691
00:21:20,566 --> 00:21:22,600
所以我们就回到了一开始

692
00:21:22,600 --> 00:21:23,666
一个简单的例子

693
00:21:23,700 --> 00:21:27,733
把 unhappiness 就直接翻拆分为三个 Token

694
00:21:27,733 --> 00:21:30,033
un，happi，ness

695
00:21:30,033 --> 00:21:30,866
三个 Token

696
00:21:30,900 --> 00:21:31,966
就通过这种方式

697
00:21:31,966 --> 00:21:34,500
BPE 的算法来进行实现

698
00:21:34,933 --> 00:21:37,800
当然了我们还举了另外一个例子

699
00:21:37,800 --> 00:21:38,833
那不管怎么样

700
00:21:38,833 --> 00:21:39,866
基本上都是这样

701
00:21:39,900 --> 00:21:41,033
the highest mountain

702
00:21:41,033 --> 00:21:44,200
最高的一个山是什么

703
00:21:44,200 --> 00:21:46,066
于是我们进行一个排序

704
00:21:46,100 --> 00:21:46,700
排序之后

705
00:21:46,700 --> 00:21:47,800
进行一个叠代

706
00:21:47,800 --> 00:21:50,366
然后把一个单词或者是 subword

707
00:21:50,366 --> 00:21:52,700
提取出来基本上都是以这个思路

708
00:21:54,100 --> 00:21:55,433
毕竟 transformer 的原理

709
00:21:55,433 --> 00:21:57,366
还是相对比较简单

710
00:21:57,366 --> 00:21:58,433
只是打开了一些细节

711
00:21:58,433 --> 00:21:59,300
我们现在来看到

712
00:21:59,333 --> 00:22:01,500
Tokenizer 的一个评估指标

713
00:22:01,500 --> 00:22:02,300
讲了这么多 zomi

714
00:22:02,300 --> 00:22:02,966
不想讲了

715
00:22:02,966 --> 00:22:05,233
不过为了坚持完成这个系列

716
00:22:05,233 --> 00:22:07,100
还是得咬咬牙

717
00:22:07,133 --> 00:22:09,700
你们也咬咬牙多支持一下 zomi 了

718
00:22:09,800 --> 00:22:12,800
坚持才是胜利

719
00:22:12,966 --> 00:22:14,300
首先 Tokenizer 的评估指标

720
00:22:14,333 --> 00:22:15,233
我们刚才讲到了

721
00:22:15,233 --> 00:22:16,833
首先我们看一下一个

722
00:22:16,833 --> 00:22:18,033
词汇表的覆盖率

723
00:22:18,033 --> 00:22:20,866
也就是能不能处理很多种不同的语言

724
00:22:20,900 --> 00:22:22,766
所以我们会看词汇表

725
00:22:22,800 --> 00:22:24,000
我们 15 万个词

726
00:22:24,000 --> 00:22:26,033
到底能覆盖多少种语言

727
00:22:26,033 --> 00:22:27,600
它的一个专业术语

728
00:22:27,600 --> 00:22:30,100
例如人工智能有没有在词汇表

729
00:22:30,133 --> 00:22:32,800
人工智能它是一个整个单词

730
00:22:32,800 --> 00:22:34,166
而不是分开两个

731
00:22:34,166 --> 00:22:38,000
所谓的人工跟智能

732
00:22:38,100 --> 00:22:40,433
它是个完整的单词

733
00:22:40,433 --> 00:22:42,400
所以说我们要术语的专业性

734
00:22:42,533 --> 00:22:42,833
第二个

735
00:22:42,833 --> 00:22:44,700
就是 Token 的数量

736
00:22:44,733 --> 00:22:47,366
过长的话会导致计算浪费

737
00:22:47,366 --> 00:22:48,900
过短会丢失信息

738
00:22:48,933 --> 00:22:51,633
例如人工智能非常聪明

739
00:22:51,833 --> 00:22:53,900
它不可能变成一个词

740
00:22:53,933 --> 00:22:55,166
或者变成一个 Token

741
00:22:55,166 --> 00:22:56,300
绝对会拆分成为

742
00:22:56,333 --> 00:23:00,200
人工智能非常聪明

743
00:23:00,400 --> 00:23:01,600
那这么去拆

744
00:23:01,600 --> 00:23:02,166
所以说

745
00:23:02,166 --> 00:23:04,833
避免我们过长的一个浪费

746
00:23:04,900 --> 00:23:06,533
那另外就是 OOV

747
00:23:06,533 --> 00:23:08,166
一个查询率

748
00:23:08,333 --> 00:23:10,800
out of vocabulary 的一个 wait 了

749
00:23:10,800 --> 00:23:12,700
就是超过词表大小

750
00:23:12,733 --> 00:23:13,900
或者词表查不到

751
00:23:14,933 --> 00:23:16,900
我们现在来到了最后一个内容

752
00:23:16,900 --> 00:23:18,633
Tokenizer 一个具体的实践

753
00:23:18,633 --> 00:23:19,566
那这个实践

754
00:23:19,566 --> 00:23:22,166
我们是基于一个千问 38B

755
00:23:22,166 --> 00:23:23,300
一个大模型

756
00:23:23,333 --> 00:23:24,600
进行一个分词

757
00:23:24,700 --> 00:23:25,733
那在分词之前

758
00:23:25,733 --> 00:23:27,800
我们要了解一个几个粗浅的概念

759
00:23:27,866 --> 00:23:31,933
首先千问 3 使用的是字节级的 BPE 算法

760
00:23:31,933 --> 00:23:32,766
那这个算法 

761
00:23:32,766 --> 00:23:34,766
我刚才已经跟大家介绍过了

762
00:23:34,766 --> 00:23:35,800
简单的介绍后面

763
00:23:35,800 --> 00:23:38,066
会有一个这个算法的手撸

764
00:23:38,100 --> 00:23:39,333
端到端实践

765
00:23:39,700 --> 00:23:40,133
那接着

766
00:23:40,133 --> 00:23:42,800
我们看一下一个所谓的词表

767
00:23:42,800 --> 00:23:44,433
说实话词表这么大

768
00:23:44,433 --> 00:23:45,700
四五万个词

769
00:23:45,766 --> 00:23:46,833
那这里面怎么去实现

770
00:23:46,833 --> 00:23:48,666
主要是基于开源的分词框架

771
00:23:48,700 --> 00:23:49,900
也就是 tik Token

772
00:23:50,200 --> 00:23:54,566
这个 c l 100 k 的基础词表作为初始化

773
00:23:54,566 --> 00:23:55,466
把更多的语言

774
00:23:55,500 --> 00:23:57,833
还有一些专业的词汇加进去

775
00:23:58,366 --> 00:24:00,400
当然了可能还会有一些 MCP 的词汇

776
00:24:00,400 --> 00:24:01,433
也会加进去

777
00:24:01,433 --> 00:24:03,466
还有最新的一个 think 跟 end think

778
00:24:03,500 --> 00:24:06,200
推理模型的相关的东西加进去

779
00:24:06,366 --> 00:24:08,000
那有了这个词表之后

780
00:24:08,000 --> 00:24:09,100
我们对中文

781
00:24:09,133 --> 00:24:10,833
需要进行一些扩展

782
00:24:10,833 --> 00:24:12,500
刚才讲到的高频的中文词

783
00:24:12,533 --> 00:24:14,166
成语最终

784
00:24:14,166 --> 00:24:17,800
把词汇表拓展到 15 万 Token

785
00:24:18,000 --> 00:24:19,266
那这也是现在

786
00:24:19,300 --> 00:24:21,966
整个千问 3 的一个 Token 的情况

787
00:24:22,500 --> 00:24:23,500
那最后

788
00:24:23,500 --> 00:24:24,366
还会蛮有意思

789
00:24:24,366 --> 00:24:25,300
就数字

790
00:24:25,333 --> 00:24:25,933
这里面

791
00:24:25,933 --> 00:24:26,833
就刚才讲到

792
00:24:26,833 --> 00:24:31,000
会有一个 Pre-Tokenization 的一个过程

793
00:24:31,033 --> 00:24:32,966
那这个预处理的过程当中

794
00:24:32,966 --> 00:24:34,600
因为方便我们进行一个

795
00:24:34,600 --> 00:24:35,833
数理逻辑的推导

796
00:24:35,833 --> 00:24:39,266
所以会把这里面的数字也拆分出来

797
00:24:39,300 --> 00:24:41,733
提升模型对推理任务的一个泛化能力

798
00:24:41,733 --> 00:24:44,166
这个是千问 3 所做的事情

799
00:24:44,333 --> 00:24:45,000
那同样

800
00:24:45,000 --> 00:24:45,700
我们现在

801
00:24:45,733 --> 00:24:48,733
来看看一开始的文本的输入

802
00:24:48,766 --> 00:24:51,466
怎么变成一个最后的 Token 的 ID

803
00:24:52,033 --> 00:24:52,700
那我们现在

804
00:24:52,733 --> 00:24:55,133
就打开 Jupyter Notebook

805
00:24:55,133 --> 00:24:56,333
去尝试用一下

806
00:24:56,333 --> 00:24:59,333
hugging face 里面的一个 auto Tokenizer

807
00:24:59,633 --> 00:25:00,266
那首先

808
00:25:00,300 --> 00:25:02,133
我们需要 from

809
00:25:03,233 --> 00:25:04,866
transformer

810
00:25:07,500 --> 00:25:10,133
oh no 可能名字错了

811
00:25:10,133 --> 00:25:12,533
。。。。。

812
00:25:12,533 --> 00:25:14,600
手撸代码很容易出错

813
00:25:14,600 --> 00:25:14,900
然后

814
00:25:14,933 --> 00:25:17,300
我们现在加载一个千问的 Tokenizer

815
00:25:20,833 --> 00:25:23,500
嘿加搭载的是千问 7B

816
00:25:23,533 --> 00:25:27,133
也就是对应的这里面的千问

817
00:25:27,133 --> 00:25:28,566
千问 3-8b

818
00:25:28,633 --> 00:25:29,600
我们改一改

819
00:25:29,633 --> 00:25:32,166
千问 3-8b 的模型

820
00:25:32,633 --> 00:25:34,400
好加载完之后

821
00:25:34,400 --> 00:25:35,066
我们接下来

822
00:25:35,100 --> 00:25:37,833
就要对这个模型进行一个

823
00:25:37,833 --> 00:25:40,600
编码了将文本转为 Token ID

824
00:25:45,533 --> 00:25:48,400
执行可以看到编码结果就把

825
00:25:49,133 --> 00:25:51,500
哎呀，漏了漏了~

826
00:25:55,633 --> 00:25:57,300
好我们执行可以看到

827
00:25:57,333 --> 00:25:58,033
基本上

828
00:25:58,033 --> 00:26:00,500
就把我们输入的 text

829
00:26:00,566 --> 00:26:03,200
变成 Token 的 ID 了

830
00:26:03,200 --> 00:26:06,400
那我们现在来进行一个解码的尝试

831
00:26:07,033 --> 00:26:09,666
把 Token ID 转回文本

832
00:26:19,033 --> 00:26:20,033
可以看到我们这里面

833
00:26:20,033 --> 00:26:21,066
把解码结果

834
00:26:21,100 --> 00:26:25,033
就转为你正在看 ZOMI 无聊的视频了

835
00:26:25,166 --> 00:26:25,666
那同样

836
00:26:25,700 --> 00:26:28,933
我们尝试看一下其中这个单词

837
00:26:28,933 --> 00:26:30,300
表示的是什么意思

838
00:26:30,300 --> 00:26:31,100
这个 Token

839
00:26:33,733 --> 00:26:35,766
那基本上拿到一个就是你了

840
00:26:35,766 --> 00:26:37,800
然后确实他不一定是一对应

841
00:26:37,800 --> 00:26:39,800
但有可能对应的概率比较大

842
00:26:40,133 --> 00:26:42,133
然后我们多尝试一个吧

843
00:26:43,700 --> 00:26:44,900
哎这个试试看一下

844
00:26:44,900 --> 00:26:46,400
试试这么小的是哪个

845
00:26:47,000 --> 00:26:48,833
哎原来是 i

846
00:26:48,900 --> 00:26:50,533
那这个 i 蛮有意思

847
00:26:50,533 --> 00:26:51,766
他把 ZOMI 这个 i

848
00:26:51,766 --> 00:26:54,266
就是单独提取出来了

849
00:26:54,300 --> 00:26:56,733
就没把 ZOMI 能够完全识别出来

850
00:26:56,733 --> 00:26:58,200
那我们现在来看一下

851
00:26:58,200 --> 00:27:00,966
一个具体的 Token

852
00:27:07,766 --> 00:27:09,966
那么现在我们可以看到下面

853
00:27:09,966 --> 00:27:11,666
这个是不是挺有意思

854
00:27:11,733 --> 00:27:15,133
就是切词就一开始的时候

855
00:27:15,133 --> 00:27:17,600
怎么去把具体的 Token 的单词

856
00:27:17,600 --> 00:27:20,033
就这句话你正在看这么无聊的视频

857
00:27:20,033 --> 00:27:22,166
变成这一坨我完全看不懂

858
00:27:22,166 --> 00:27:22,800
当然了我们可以

859
00:27:22,800 --> 00:27:23,500
看到这里面

860
00:27:23,533 --> 00:27:25,166
实际上都对不上

861
00:27:25,166 --> 00:27:26,966
周敏能看得懂的只有 MI

862
00:27:26,966 --> 00:27:28,766
还有整个 transformer 的英文

863
00:27:28,766 --> 00:27:29,300
这里面

864
00:27:29,333 --> 00:27:32,366
其实最主要的问题就是码率不对

865
00:27:32,366 --> 00:27:32,966
但实际上

866
00:27:32,966 --> 00:27:34,500
它能够正常的去识别

867
00:27:34,533 --> 00:27:37,233
就输入跟输出

868
00:27:37,333 --> 00:27:40,733
然后把输出这堆 Token 还原回去

869
00:27:40,733 --> 00:27:43,900
输入只是我们简单的打出来

870
00:27:43,900 --> 00:27:45,900
它不一定能够正常的显示

871
00:27:45,933 --> 00:27:46,833
这也是为什么

872
00:27:46,833 --> 00:27:50,166
我们打开这里面的词汇表

873
00:27:50,166 --> 00:27:50,966
就发现哎

874
00:27:50,966 --> 00:27:53,866
里面有很多的一些单词

875
00:27:54,133 --> 00:27:57,766
可能不是我们能够真正能够认识到

876
00:27:57,766 --> 00:27:58,400
就这里面

877
00:27:58,400 --> 00:28:00,700
你找不到中文是因为它对应的码率

878
00:28:00,733 --> 00:28:01,933
显示不对

879
00:28:01,966 --> 00:28:02,466
但这里面

880
00:28:02,500 --> 00:28:04,366
实际上有大量的中文

881
00:28:04,366 --> 00:28:07,233
一些专业的名词在这里面

882
00:28:08,166 --> 00:28:08,700
同样

883
00:28:08,733 --> 00:28:12,366
我们把 Tokenizer 点 Jason 也打开

884
00:28:12,366 --> 00:28:13,966
跟大家一起去看一看

885
00:28:13,966 --> 00:28:14,700
那这边马有色

886
00:28:14,733 --> 00:28:17,400
就会加很多这种所谓的标志符了

887
00:28:17,400 --> 00:28:19,600
在一个 post

888
00:28:19,600 --> 00:28:21,500
一个预处理和后处理的过程中

889
00:28:21,533 --> 00:28:22,700
也就是真正

890
00:28:22,700 --> 00:28:23,133
蛮有意思

891
00:28:23,133 --> 00:28:24,500
就是看一下 think

892
00:28:24,500 --> 00:28:25,800
在我们现在的推理模型

893
00:28:25,800 --> 00:28:26,666
有很多的 think

894
00:28:26,700 --> 00:28:30,100
跟一个 end think 相关的内容都加进来了

895
00:28:30,100 --> 00:28:32,400
然后我们再看一下 Tokenizer configure

896
00:28:32,500 --> 00:28:33,100
那 configure

897
00:28:33,100 --> 00:28:34,300
这些就是配置了

898
00:28:34,300 --> 00:28:36,766
就我们 Tokenizer 的一些读取的时候

899
00:28:36,766 --> 00:28:37,500
真正处理的时候

900
00:28:37,533 --> 00:28:39,200
它会做一些配置

901
00:28:39,200 --> 00:28:39,900
所以说配置

902
00:28:39,933 --> 00:28:41,533
我们没必要去看太多

903
00:28:41,533 --> 00:28:43,766
更多的是看 Tokener 的内容

904
00:28:43,766 --> 00:28:47,266
跟 vocabulary 的一个内容就可以了

905
00:28:47,300 --> 00:28:48,633
经过一个简单的尝试

906
00:28:48,633 --> 00:28:51,000
我们基于千问 3-8b 的模型

907
00:28:51,000 --> 00:28:52,033
把一个文本

908
00:28:52,033 --> 00:28:54,800
到一个 input Tokens ID

909
00:28:54,800 --> 00:28:55,400
就输出了

910
00:28:55,400 --> 00:28:56,766
我们现在来到了最

911
00:28:56,766 --> 00:28:57,600
后的内容

912
00:28:57,700 --> 00:28:59,433
看一下总结跟思考

913
00:28:59,500 --> 00:29:00,333
那每一节 ZOMI

914
00:29:00,333 --> 00:29:02,533
都还是要跟大家一起做一个总结

915
00:29:02,533 --> 00:29:04,433
首先我们回顾一下 Tokenizer

916
00:29:04,433 --> 00:29:05,066
实际上

917
00:29:05,100 --> 00:29:06,733
是整个 transformerr 大模型

918
00:29:06,733 --> 00:29:08,566
理解文本的第一道

919
00:29:08,566 --> 00:29:11,566
对模型的输入变成一堆

920
00:29:11,600 --> 00:29:14,600
就刚才的这种所谓的数字

921
00:29:14,600 --> 00:29:17,200
那计算机不会去识别语言

922
00:29:17,200 --> 00:29:20,066
更多的是识别这些数字跟数字

923
00:29:20,100 --> 00:29:23,033
就序列跟序列之间的一个关系

924
00:29:24,100 --> 00:29:24,700
那第二个

925
00:29:24,700 --> 00:29:25,966
就是 subword

926
00:29:25,966 --> 00:29:26,866
就是 BPE

927
00:29:26,900 --> 00:29:28,033
这种算法

928
00:29:28,133 --> 00:29:28,633
实际上

929
00:29:28,633 --> 00:29:30,766
已经现在成为整个大模型的主流了

930
00:29:30,766 --> 00:29:33,066
它能够很好地去平衡语义的表达

931
00:29:33,100 --> 00:29:34,733
跟计算的效率

932
00:29:35,333 --> 00:29:37,000
那我们现在还是要思考一个问题

933
00:29:37,000 --> 00:29:38,666
就是为什么 output Token 的价格

934
00:29:38,700 --> 00:29:40,900
比 input Token 的价格更贵

935
00:29:40,933 --> 00:29:41,433
那下面

936
00:29:41,433 --> 00:29:42,100
就是

937
00:29:42,133 --> 00:29:44,800
我在那个阿里云的一个官网里面

938
00:29:44,800 --> 00:29:45,600
去截取

939
00:29:45,600 --> 00:29:48,666
你会发现千问的一个

940
00:29:48,700 --> 00:29:50,800
输入的 Token 实际上

941
00:29:50,800 --> 00:29:53,900
是比输出的 Token 要便宜

942
00:29:54,000 --> 00:29:54,833
原因在哪

943
00:29:54,833 --> 00:29:56,600
为什么输出的 Token 更贵

944
00:29:56,900 --> 00:29:57,633
那这个内容

945
00:29:57,633 --> 00:29:58,700
希望大家去思考

946
00:29:58,733 --> 00:30:00,900
当然了它不是因为 Token 引起

947
00:30:00,900 --> 00:30:03,000
更多是因为算力引起

948
00:30:03,000 --> 00:30:04,000
那今天的内容

949
00:30:04,000 --> 00:30:04,766
就到这里为止

950
00:30:04,766 --> 00:30:05,366
谢谢各位

951
00:30:05,366 --> 00:30:06,633
拜了个拜
