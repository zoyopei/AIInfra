1
00:00:00,000 --> 00:00:01,433
内容/录制:Z0MI 酱，视频后期/字幕:梁嘉铭

2
00:00:01,433 --> 00:00:02,666
hello 大家好

3
00:00:02,700 --> 00:00:04,400
我是那个春光好暖

4
00:00:04,400 --> 00:00:05,100
我好懒

5
00:00:05,100 --> 00:00:07,466
吃完一碗又一碗的 ZOMI

6
00:00:11,166 --> 00:00:11,433
今天

7
00:00:11,433 --> 00:00:13,700
我们还是在变形金刚 transformer 里面

8
00:00:13,733 --> 00:00:16,033
去看看 transformer 的一个核心机制

9
00:00:16,033 --> 00:00:18,633
就是第一层 Embedding

10
00:00:18,733 --> 00:00:21,633
那回到整个 transformer 的整体架构里面

11
00:00:21,633 --> 00:00:23,433
我们之前已经分享了前世今生

12
00:00:23,433 --> 00:00:26,100
还有整个核心机制的词元 token

13
00:00:26,500 --> 00:00:29,033
token 之后输入第一层就是 embedding 了

14
00:00:29,033 --> 00:00:30,700
我们现在来到了这一个内容

15
00:00:30,733 --> 00:00:31,933
看一下今天

16
00:00:31,933 --> 00:00:34,500
主要是跟大家分享的内容有三个

17
00:00:34,500 --> 00:00:35,166
整个 embedding

18
00:00:35,166 --> 00:00:36,233
其实我们现在

19
00:00:36,233 --> 00:00:40,266
基本上基本上基本上百分之 999 了

20
00:00:40,300 --> 00:00:43,333
然后都会用 ROPE rotate embedding

21
00:00:43,333 --> 00:00:44,900
旋转位置编码

22
00:00:44,900 --> 00:00:46,033
但是整个视频里面

23
00:00:46,033 --> 00:00:48,633
还是想跟大家一起去探讨一下

24
00:00:48,800 --> 00:00:50,433
整个 Embedding 的发展过程

25
00:00:50,433 --> 00:00:53,000
从绝对位置编码到相对位置编码

26
00:00:53,000 --> 00:00:53,900
再到我们现在

27
00:00:53,933 --> 00:00:57,033
旋转位置编码的一个发展历程

28
00:00:57,100 --> 00:00:58,166
那可能在后面

29
00:00:58,166 --> 00:00:59,266
ZOMI 还会录一个视频

30
00:00:59,300 --> 00:01:03,366
去跟大家手撸一个旋转位置编码

31
00:01:03,366 --> 00:01:05,600
一个代码出来

32
00:01:05,600 --> 00:01:06,166
那 whatever

33
00:01:06,166 --> 00:01:06,866
我们现在

34
00:01:06,900 --> 00:01:09,533
来看一下整个 embedding 的核心任务

35
00:01:09,533 --> 00:01:11,900
看一下 embedding 到底是什么

36
00:01:11,900 --> 00:01:13,600
那如果不了解的同学

37
00:01:13,600 --> 00:01:14,966
其实比较明确

38
00:01:15,000 --> 00:01:15,366
首先

39
00:01:15,366 --> 00:01:17,400
大模型输入是一堆的 text
 
40
00:01:17,400 --> 00:01:18,633
不管是单模态

41
00:01:18,633 --> 00:01:20,500
大语言模型还是多模态

42
00:01:20,566 --> 00:01:22,100
都会经过一个 tokenizer

43
00:01:22,133 --> 00:01:24,233
也就是把它变成一个个词元

44
00:01:24,233 --> 00:01:26,300
词元就是一个

45
00:01:26,333 --> 00:01:27,933
vocabulary 的一个 ID

46
00:01:27,966 --> 00:01:28,833
那接着之后

47
00:01:28,833 --> 00:01:30,966
我们就会把词元真正的 token

48
00:01:30,966 --> 00:01:32,566
丢给 embedding 层

49
00:01:32,566 --> 00:01:34,466
embedding 层经过第一层

50
00:01:34,500 --> 00:01:37,566
也就是右边的这一小层了

51
00:01:37,566 --> 00:01:40,300
input embedding 经过这一层之后

52
00:01:40,300 --> 00:01:42,200
才会给我们整个大模型

53
00:01:42,200 --> 00:01:43,200
transformer 的架构

54
00:01:43,200 --> 00:01:44,666
也就是自注意力机制

55
00:01:44,700 --> 00:01:46,600
里面的这些计算

56
00:01:46,733 --> 00:01:48,133
那在丢进去之前

57
00:01:48,133 --> 00:01:49,700
就会经过一个 embedding

58
00:01:49,700 --> 00:01:53,566
之后就输出这么一个矩阵出来了

59
00:01:53,633 --> 00:01:55,233
所以说 embedding 的作用

60
00:01:55,233 --> 00:01:57,166
就是将我们离散的输入的 ID

61
00:01:57,166 --> 00:01:59,166
转换成为连续的向量

62
00:01:59,166 --> 00:02:01,166
有下面的这个向量表

63
00:02:01,166 --> 00:02:01,900
通过查表

64
00:02:01,933 --> 00:02:05,533
为每个符号赋予具有语义信息的向量

65
00:02:05,600 --> 00:02:06,366
那 whatever

66
00:02:06,366 --> 00:02:07,266
我们简单的理解

67
00:02:07,300 --> 00:02:10,800
就是把一个输入的 token 的 ID

68
00:02:11,100 --> 00:02:13,900
变成连续的向量

69
00:02:13,900 --> 00:02:15,000
这么一个过程

70
00:02:16,233 --> 00:02:18,800
如果大家对 embedding 不是很感兴趣

71
00:02:18,800 --> 00:02:20,200
说实话你了解完

72
00:02:20,200 --> 00:02:22,900
刚才跟大家分享的那个内容之后

73
00:02:22,933 --> 00:02:24,166
你就可以停止了

74
00:02:24,166 --> 00:02:24,666
这个视频

75
00:02:24,700 --> 00:02:26,966
往下已经没有任何绝对的意义了

76
00:02:26,966 --> 00:02:28,633
那如果有兴趣的小伙伴

77
00:02:28,633 --> 00:02:29,866
我们往下继续看一下

78
00:02:29,900 --> 00:02:30,533
首先第一个

79
00:02:30,533 --> 00:02:31,766
我们看一下

80
00:02:31,766 --> 00:02:33,000
绝对位置编码

81
00:02:33,000 --> 00:02:34,566
叫做 APE

82
00:02:34,566 --> 00:02:37,500
absolutely position Embedding

83
00:02:37,633 --> 00:02:38,433
英文有点差

84
00:02:38,433 --> 00:02:40,500
我们看一下所谓的绝对位置编码

85
00:02:40,533 --> 00:02:41,900
里面主要的原理

86
00:02:41,900 --> 00:02:44,566
就是为序列当中的每个位置

87
00:02:44,566 --> 00:02:45,700
每一个位置

88
00:02:45,733 --> 00:02:49,000
分配一个固定的一个向量了

89
00:02:49,000 --> 00:02:50,600
那我们直接可以表示

90
00:02:50,600 --> 00:02:52,833
成为绝对的位置的索引

91
00:02:52,833 --> 00:02:53,366
那这里面

92
00:02:53,366 --> 00:02:53,833
实际上

93
00:02:53,833 --> 00:02:55,200
分为两种算法

94
00:02:55,200 --> 00:02:57,000
第一种就是可学习的编码

95
00:02:57,000 --> 00:02:57,433
第二种

96
00:02:57,433 --> 00:03:00,400
就是这个单词我也不知道怎么读

97
00:03:00,400 --> 00:03:01,800
三角函数

98
00:03:01,900 --> 00:03:04,900
然后我们分开两个学习的方式

99
00:03:04,900 --> 00:03:06,633
第一种就是可学习的编码

100
00:03:06,633 --> 00:03:09,300
第二种就是正旋波的编码

101
00:03:09,466 --> 00:03:11,300
两种那不管了

102
00:03:11,333 --> 00:03:13,366
我们后面会展开的适用的任务

103
00:03:13,366 --> 00:03:15,033
就是整个绝对位置编码主

104
00:03:15,033 --> 00:03:17,266
要是一些短序列任务

105
00:03:17,300 --> 00:03:19,633
以前在 RNN 时代

106
00:03:19,633 --> 00:03:20,966
可能会用的比较多

107
00:03:20,966 --> 00:03:22,366
到了 transformer 架构的时候

108
00:03:22,366 --> 00:03:24,166
因为序列越来越长了

109
00:03:24,166 --> 00:03:25,166
绝对位置编码

110
00:03:25,166 --> 00:03:27,300
其实不符合一个规则

111
00:03:27,400 --> 00:03:29,133
所以说他的问题比较敏感

112
00:03:29,133 --> 00:03:30,500
就问题比较明确

113
00:03:30,500 --> 00:03:32,500
对位置非常的敏感

114
00:03:32,500 --> 00:03:33,500
特别是小任务

115
00:03:33,500 --> 00:03:36,000
所以需要精确的定位到上下文

116
00:03:36,000 --> 00:03:36,633
这种方式

117
00:03:36,633 --> 00:03:38,400
就不适合我们现在大量的语料

118
00:03:38,400 --> 00:03:40,000
大量的长上下文

119
00:03:40,200 --> 00:03:41,433
那我们刚才讲到了

120
00:03:41,433 --> 00:03:42,433
分开两个嘛

121
00:03:42,433 --> 00:03:43,166
绝对位置编码

122
00:03:43,166 --> 00:03:43,466
第一个

123
00:03:43,500 --> 00:03:45,200
就可学习的绝对位置编码

124
00:03:45,300 --> 00:03:46,366
主要的原理就是

125
00:03:46,366 --> 00:03:48,066
我们初始化的时候

126
00:03:48,100 --> 00:03:49,933
就设置一个位置

127
00:03:49,933 --> 00:03:51,166
或者设置一个矩阵

128
00:03:51,166 --> 00:03:52,366
可以学习

129
00:03:52,366 --> 00:03:53,166
然后模型

130
00:03:53,166 --> 00:03:55,466
去感知到每个词向量的一个绝对位置

131
00:03:55,500 --> 00:03:57,700
然后让模型自主的去学习

132
00:03:57,700 --> 00:03:58,733
早期的 bert

133
00:03:58,733 --> 00:03:59,966
大家可以看回去

134
00:03:59,966 --> 00:04:02,133
那可能就会用这种方式了

135
00:04:02,166 --> 00:04:02,533
但是

136
00:04:02,533 --> 00:04:04,800
最大的问题又不具备长外推性

137
00:04:04,800 --> 00:04:05,466
推理的时候

138
00:04:05,500 --> 00:04:06,433
如果我训练的时候

139
00:04:06,433 --> 00:04:09,066
是 512，512 的一个序列长度

140
00:04:09,100 --> 00:04:09,933
那推理的时候

141
00:04:09,933 --> 00:04:11,400
也只能 512 了

142
00:04:11,400 --> 00:04:11,833
第二个

143
00:04:11,833 --> 00:04:13,233
就是正弦波

144
00:04:13,233 --> 00:04:15,300
也就是我们叫做三角式

145
00:04:15,333 --> 00:04:16,733
一个绝对位置编码

146
00:04:16,766 --> 00:04:18,100
所以的三角式

147
00:04:18,133 --> 00:04:19,700
我们看这条公式比较明确了

148
00:04:19,700 --> 00:04:22,000
就是通过不同的一个频率

149
00:04:22,000 --> 00:04:23,033
一个是正弦

150
00:04:23,033 --> 00:04:24,100
一个是余弦

151
00:04:24,133 --> 00:04:26,433
生成未知编码

152
00:04:26,433 --> 00:04:27,433
那偶数

153
00:04:27,433 --> 00:04:29,233
我们看到偶数 2i

154
00:04:29,233 --> 00:04:30,566
就用的是正弦

155
00:04:30,633 --> 00:04:31,866
基数 2i 加一

156
00:04:31,900 --> 00:04:33,433
用的就是余弦

157
00:04:33,433 --> 00:04:35,266
那这个就是 position

158
00:04:35,300 --> 00:04:36,233
这个 d

159
00:04:36,233 --> 00:04:37,000
dimension

160
00:04:37,000 --> 00:04:38,700
模型的一个维度了

161
00:04:38,733 --> 00:04:40,833
我们可以视为 embedding 的维度

162
00:04:40,833 --> 00:04:41,566
那基本上就

163
00:04:41,566 --> 00:04:43,200
按这条公式来去计算

164
00:04:43,200 --> 00:04:45,566
我们后面也会单独的跟大家看看代码

165
00:04:45,566 --> 00:04:47,833
这里面就不详细的展开了

166
00:04:47,833 --> 00:04:49,666
三角式的绝对位置编码

167
00:04:49,700 --> 00:04:51,133
它的特点比较明确

168
00:04:51,133 --> 00:04:52,333
就是不用训练

169
00:04:52,333 --> 00:04:55,000
而且有周期性的一个外推功能

170
00:04:55,000 --> 00:04:57,266
但是没有办法表达相对的位置

171
00:04:57,300 --> 00:04:58,900
因为它是绝对位置编码吗

172
00:04:58,900 --> 00:05:00,966
那当开始的时候

173
00:05:00,966 --> 00:05:03,033
第一版的 transformer 的架构

174
00:05:03,033 --> 00:05:04,033
就是用的这个

175
00:05:04,033 --> 00:05:05,866
三角式的绝对位置编码

176
00:05:05,966 --> 00:05:07,933
然后我们往下看看

177
00:05:07,933 --> 00:05:09,200
这个图案蛮有意思

178
00:05:09,200 --> 00:05:10,400
你可以看到

179
00:05:10,400 --> 00:05:11,833
那我们再详细的展开

180
00:05:11,833 --> 00:05:13,100
就是有两种

181
00:05:13,133 --> 00:05:15,033
也就是这个图

182
00:05:15,033 --> 00:05:16,233
如果我们越短

183
00:05:16,233 --> 00:05:18,500
也就是一个 embedding 的一个维度

184
00:05:18,533 --> 00:05:20,400
越短你会发现

185
00:05:20,400 --> 00:05:22,600
表达的一个 position 的一个能力

186
00:05:22,600 --> 00:05:23,666
或者他的频谱

187
00:05:23,700 --> 00:05:26,533
这格子的大小的颜色有深有浅

188
00:05:26,533 --> 00:05:27,800
但是越往后

189
00:05:27,800 --> 00:05:28,500
你会发现

190
00:05:28,533 --> 00:05:31,166
好像频率趋近于相同了

191
00:05:31,166 --> 00:05:32,200
那同样

192
00:05:32,200 --> 00:05:34,233
当一个设置不同的参数

193
00:05:34,233 --> 00:05:36,433
我们可以把一个 embedding dimension

194
00:05:36,433 --> 00:05:38,900
做到相对比较长

195
00:05:38,933 --> 00:05:41,166
但不能说非常的非常的长了

196
00:05:41,166 --> 00:05:43,200
那 position 的相关

197
00:05:43,200 --> 00:05:44,300
一个位置的信息

198
00:05:44,333 --> 00:05:45,566
也会非常的丰富

199
00:05:45,566 --> 00:05:46,666
但是越往后

200
00:05:46,666 --> 00:05:48,700
频谱或者频率

201
00:05:48,733 --> 00:05:50,133
趋近于相同

202
00:05:50,200 --> 00:05:52,366
这个也是三角式绝对位置编码

203
00:05:52,366 --> 00:05:54,166
最大的问题

204
00:05:54,300 --> 00:05:56,633
我们当序列长度越长

205
00:05:56,633 --> 00:05:58,300
它的效果就越不好

206
00:05:58,633 --> 00:06:01,366
大部分只能表示前面的绝对的位置

207
00:06:01,366 --> 00:06:03,033
能够很好的区分出来

208
00:06:03,100 --> 00:06:04,300
那因此

209
00:06:04,300 --> 00:06:07,000
我们就有了一个相对位置编码了

210
00:06:07,000 --> 00:06:09,666
RPE 我们现在来到了第二个内容

211
00:06:09,700 --> 00:06:12,366
所谓的 RPE 相对位置编码

212
00:06:12,766 --> 00:06:14,566
来到了相对位置编码之后

213
00:06:14,566 --> 00:06:16,033
我们可以看一下

214
00:06:16,100 --> 00:06:16,966
回顾一下了

215
00:06:16,966 --> 00:06:19,366
应该是绝对位置编码的一个缺点

216
00:06:19,366 --> 00:06:20,066
首先第一个

217
00:06:20,100 --> 00:06:21,800
就是难以反应序列当中

218
00:06:21,800 --> 00:06:24,100
字符之间的相对位置

219
00:06:24,133 --> 00:06:25,966
只能表示绝对位置

220
00:06:25,966 --> 00:06:26,666
第一个单词

221
00:06:26,700 --> 00:06:30,133
如果跟序列的第 512 个单词有关联关系

222
00:06:30,133 --> 00:06:31,900
很难去表示

223
00:06:31,900 --> 00:06:34,100
那另外的话就缺乏外推性

224
00:06:34,100 --> 00:06:35,800
就是我们不能够表示

225
00:06:35,800 --> 00:06:36,866
就推理的时候

226
00:06:36,900 --> 00:06:37,500
不能表示

227
00:06:37,500 --> 00:06:40,500
比预训练文本更长的相对位置了

228
00:06:40,533 --> 00:06:41,900
那相对位置编码

229
00:06:41,900 --> 00:06:43,166
比较好理解

230
00:06:43,166 --> 00:06:44,466
就是建模序列当中

231
00:06:44,500 --> 00:06:47,800
任意两个位置的相对距离

232
00:06:47,833 --> 00:06:51,100
然后增强我们模型的一个感知能力

233
00:06:52,166 --> 00:06:53,500
我们举一个具体的例子

234
00:06:53,500 --> 00:06:54,500
例如男朋友的爸爸

235
00:06:54,500 --> 00:06:55,833
倾情打造了一个惊喜的婚房

236
00:06:55,833 --> 00:06:57,300
打开来不开门的瞬间就问了

237
00:06:57,333 --> 00:06:57,933
这是婚房吗

238
00:06:57,933 --> 00:06:59,566
是老年活动中心的联名

239
00:06:59,566 --> 00:07:01,000
然后发现了电视墙里面

240
00:07:01,000 --> 00:07:01,966
史诗级的审美

241
00:07:01,966 --> 00:07:03,700
每一天都在看新闻联播

242
00:07:03,733 --> 00:07:05,600
就好像在登机

243
00:07:06,500 --> 00:07:07,200
那说白了

244
00:07:07,200 --> 00:07:08,200
他说了这么多

245
00:07:08,200 --> 00:07:10,166
有可能中间都是废话

246
00:07:10,166 --> 00:07:13,566
只是前面的一段在装修婚房

247
00:07:13,566 --> 00:07:16,666
跟最后一段婚房很恶心

248
00:07:16,800 --> 00:07:18,800
婚房不好看有关系

249
00:07:18,800 --> 00:07:20,566
中间我们刚才念的很长的一段

250
00:07:20,566 --> 00:07:22,100
可能是真的没什么关系

251
00:07:22,133 --> 00:07:25,233
所以说相对位置编码这也很重要了

252
00:07:25,533 --> 00:07:26,600
 那相对位置编码

253
00:07:26,600 --> 00:07:28,666
实际上提出来的时候蛮有意思

254
00:07:28,700 --> 00:07:30,100
还是谷歌

255
00:07:30,100 --> 00:07:32,500
发明 transformer 架构的那个原班人马

256
00:07:32,500 --> 00:07:33,766
来提出来

257
00:07:33,766 --> 00:07:34,833
因为他们发现

258
00:07:34,833 --> 00:07:36,800
绝对位置编码其实并没那么的好用

259
00:07:36,800 --> 00:07:39,233
于是就提出了相对位置编码

260
00:07:39,233 --> 00:07:41,200
然后又出了另外一篇文章

261
00:07:41,200 --> 00:07:42,200
虽然这篇文章

262
00:07:42,200 --> 00:07:43,166
没有太多人用

263
00:07:43,166 --> 00:07:45,566
因为后来有了 voltage embedding

264
00:07:45,566 --> 00:07:46,400
就用的最多了

265
00:07:46,400 --> 00:07:47,866
那我们简单的回顾一下

266
00:07:47,900 --> 00:07:48,833
相对位置编码

267
00:07:48,833 --> 00:07:50,500
在 self attention 计算的时候

268
00:07:50,533 --> 00:07:52,966
你会发现相对的信息丢失了

269
00:07:52,966 --> 00:07:54,033
所以最直接的方式

270
00:07:54,033 --> 00:07:55,433
就是在 self attention 的时候

271
00:07:55,433 --> 00:07:57,233
再把它加进来

272
00:07:57,233 --> 00:07:59,266
这个就是一个具体的原理

273
00:07:59,300 --> 00:08:00,766
后面还会有个图

274
00:08:01,333 --> 00:08:04,500
那我们举一个比较明确的一个公式

275
00:08:04,500 --> 00:08:06,600
就是在计算 attention score 的时候

276
00:08:06,600 --> 00:08:08,500
跟一个 weight value 的时候

277
00:08:08,533 --> 00:08:10,533
就加了两个参数

278
00:08:10,600 --> 00:08:14,266
两个可学习训练的相对的位置参数

279
00:08:14,400 --> 00:08:15,733
然后 Multihead 之间

280
00:08:15,733 --> 00:08:17,800
就可以共享这个位置参数

281
00:08:17,800 --> 00:08:19,466
从而表达我们语义之间

282
00:08:19,500 --> 00:08:21,366
相对位置的信息

283
00:08:21,366 --> 00:08:22,466
那说白了就是这里面

284
00:08:22,500 --> 00:08:25,966
有一个 relative position encoder 的一个参数

285
00:08:25,966 --> 00:08:27,833
然后在我们计算的时候

286
00:08:27,833 --> 00:08:29,100
就把它加回去

287
00:08:29,133 --> 00:08:30,166
attention score

288
00:08:30,166 --> 00:08:32,766
还有 value core 就有两个位置

289
00:08:32,766 --> 00:08:35,233
把一些语义的相关的信息

290
00:08:35,233 --> 00:08:36,000
重新加回来

291
00:08:36,000 --> 00:08:36,366
不然的话

292
00:08:36,366 --> 00:08:39,166
我们在计算的时候算 QKV 要不管

293
00:08:39,166 --> 00:08:40,833
你不用去管 transformer 是什么

294
00:08:40,833 --> 00:08:42,166
我们将后面去讲

295
00:08:42,166 --> 00:08:43,566
就把一个位置的信息

296
00:08:43,566 --> 00:08:45,900
把每一步的加回来

297
00:08:45,933 --> 00:08:46,433
那这种

298
00:08:46,433 --> 00:08:49,966
就是相对位置编码的一个原理了

299
00:08:51,133 --> 00:08:52,500
那接下来我们看一下

300
00:08:52,500 --> 00:08:53,200
第三个内容

301
00:08:53,200 --> 00:08:54,900
就是旋转位置编码

302
00:08:54,933 --> 00:08:56,600
现在用的最多

303
00:08:56,600 --> 00:08:58,100
百分 99 的一个大模型

304
00:08:58,133 --> 00:09:00,733
都是用一个 rotating in position Embedding

305
00:09:00,800 --> 00:09:01,766
那我们看一下

306
00:09:01,766 --> 00:09:04,000
旋转位置编码的一个具体的原理

307
00:09:04,000 --> 00:09:04,766
比较有意思

308
00:09:04,766 --> 00:09:07,866
就来自于这篇文章 Rofarmer

309
00:09:07,966 --> 00:09:10,266
那最重要的就是将位置的信息

310
00:09:10,300 --> 00:09:12,500
编码成旋转的矩阵

311
00:09:12,500 --> 00:09:14,366
一一开始 ZOMI 是看不懂

312
00:09:14,366 --> 00:09:16,266
后来我自己去梳理的时候就发现

313
00:09:16,300 --> 00:09:18,266
哎呀其实也没有那么的难

314
00:09:18,266 --> 00:09:18,633
最主要

315
00:09:18,633 --> 00:09:19,833
就作用于查询

316
00:09:19,833 --> 00:09:22,866
就 q 跟 k 的注意之间

317
00:09:22,900 --> 00:09:25,500
隐式的把一些绝对位置的信息

318
00:09:25,500 --> 00:09:27,100
跟相对位置的信息

319
00:09:27,133 --> 00:09:29,933
编码成一个向量

320
00:09:30,233 --> 00:09:32,033
那它的优势比较明显

321
00:09:32,033 --> 00:09:33,300
就外推性比较强

322
00:09:33,333 --> 00:09:35,833
支持超长的一个序列

323
00:09:35,833 --> 00:09:36,500
另外的话

324
00:09:36,533 --> 00:09:39,000
一个远程的一个衰减性

325
00:09:39,000 --> 00:09:39,900
比较好

326
00:09:39,933 --> 00:09:41,566
我们刚才讲到了绝对位置编码

327
00:09:41,566 --> 00:09:45,000
它的一个远程的衰减是非常的夸张

328
00:09:45,000 --> 00:09:45,966
序列越长

329
00:09:45,966 --> 00:09:47,500
它的一个位置编码的信息

330
00:09:47,533 --> 00:09:48,833
衰减的非常的厉害

331
00:09:48,833 --> 00:09:49,500
就变成了

332
00:09:49,533 --> 00:09:52,133
没有办法去感知长序列的问题

333
00:09:52,133 --> 00:09:52,700
那另外的话

334
00:09:52,700 --> 00:09:54,366
它数学上是等价

335
00:09:54,366 --> 00:09:56,400
就通过旋转位置的一个信息

336
00:09:56,400 --> 00:09:58,633
保证绝对位置跟相对位置

337
00:09:58,633 --> 00:10:00,033
是保留下来

338
00:10:00,233 --> 00:10:00,800
那蛮有意思

339
00:10:00,800 --> 00:10:02,100
你可能听很多听不懂

340
00:10:02,133 --> 00:10:04,300
我们后面还是以图例为主

341
00:10:04,333 --> 00:10:05,900
那我们看一下它适用的情况

342
00:10:05,900 --> 00:10:07,100
就非常的多了

343
00:10:07,100 --> 00:10:07,966
现在千万的系列

344
00:10:07,966 --> 00:10:08,766
拉玛系列

345
00:10:08,766 --> 00:10:10,100
还有各种各样的长序列

346
00:10:10,133 --> 00:10:12,033
都是用这个旋转位置编码

347
00:10:12,033 --> 00:10:12,466
基本上

348
00:10:12,500 --> 00:10:14,766
已经固定下来了整个算法

349
00:10:14,766 --> 00:10:16,033
朋友们看一下这个图

350
00:10:16,033 --> 00:10:18,633
这个图非常非常的经典

351
00:10:18,633 --> 00:10:18,966
第一个

352
00:10:18,966 --> 00:10:20,900
就是首先我们会有

353
00:10:20,933 --> 00:10:23,566
左边有很多的一个 token

354
00:10:23,566 --> 00:10:24,666
那真正的 token

355
00:10:24,700 --> 00:10:26,500
在一个大模型之前

356
00:10:26,500 --> 00:10:28,000
输进去 embedding 的时候

357
00:10:28,000 --> 00:10:30,266
就会把它变成一个向量对吧

358
00:10:30,300 --> 00:10:30,633
但是

359
00:10:30,633 --> 00:10:33,766
这个向量我们可以分开很多段

360
00:10:33,800 --> 00:10:34,633
那第一段

361
00:10:34,633 --> 00:10:35,233
就θ一

362
00:10:35,233 --> 00:10:37,466
θ二还有θd 除以 2

363
00:10:37,500 --> 00:10:38,566
非常的多

364
00:10:38,566 --> 00:10:41,266
然后我们把θ一单独的拿出来

365
00:10:41,300 --> 00:10:43,133
就其中一个向量拿出来

366
00:10:43,133 --> 00:10:46,566
那这个词元的向量假设是只有两个 S1

367
00:10:46,566 --> 00:10:48,000
S2 这个时候

368
00:10:48,000 --> 00:10:49,400
我对这个向量

369
00:10:49,400 --> 00:10:51,466
你在一个笛卡尔坐标里面

370
00:10:51,500 --> 00:10:53,500
或者二维坐标里面进行表示

371
00:10:53,500 --> 00:10:54,633
它是有个方向

372
00:10:54,633 --> 00:10:56,600
因为每个向量都是有个方向

373
00:10:56,600 --> 00:11:01,566
我通过一个 m 乘以θ一做一个旋转

374
00:11:01,733 --> 00:11:03,933
那你会发现我只是旋转了

375
00:11:03,966 --> 00:11:04,766
那旋转完之后

376
00:11:04,766 --> 00:11:07,566
就变成这个第二个向量了

377
00:11:07,566 --> 00:11:09,800
那这个向量我们叫做θ2 了

378
00:11:09,800 --> 00:11:12,866
假设那这个时候我们会发现了

379
00:11:12,900 --> 00:11:17,633
这个θ1 跟θ2 的信息有了一个融合

380
00:11:17,633 --> 00:11:19,866
我只是通过简单的旋转

381
00:11:19,933 --> 00:11:22,433
那可能我这里面有那个

382
00:11:22,433 --> 00:11:24,966
1024 的一个维度

383
00:11:24,966 --> 00:11:26,166
现在只是两个

384
00:11:26,166 --> 00:11:29,200
我旋转完之后我其实变成 512 的维度

385
00:11:29,200 --> 00:11:31,000
位置压缩了

386
00:11:31,000 --> 00:11:33,166
但是信息保留下来了

387
00:11:33,166 --> 00:11:34,666
我只是经过一个旋转

388
00:11:34,700 --> 00:11:36,600
我没有丢失里面的信息

389
00:11:36,600 --> 00:11:39,400
我只是乘以了另外一个矩阵

390
00:11:39,433 --> 00:11:40,300
那这种方式

391
00:11:40,333 --> 00:11:43,300
就是旋转位置编码的最核心的原理了

392
00:11:43,300 --> 00:11:43,900
那当然了

393
00:11:43,900 --> 00:11:46,100
这里面还会有一个所谓

394
00:11:46,100 --> 00:11:46,766
刚才

395
00:11:46,766 --> 00:11:49,700
我说把绝对位置的信息都编码进去

396
00:11:49,733 --> 00:11:51,366
绝对位置的信息在这里面

397
00:11:51,366 --> 00:11:52,700
position 在这里面

398
00:11:52,733 --> 00:11:54,566
每个 token 的 position 都有

399
00:11:54,566 --> 00:11:57,633
positionm 乘以θ一

400
00:11:57,866 --> 00:11:59,700
那所以我们把绝对位置的信息

401
00:11:59,733 --> 00:12:00,900
也保留下来了

402
00:12:00,900 --> 00:12:01,433
然后

403
00:12:01,433 --> 00:12:03,866
相对位置的信息也记录下来了

404
00:12:03,900 --> 00:12:04,933
通过这种方式

405
00:12:04,933 --> 00:12:06,366
很好的去实现

406
00:12:06,366 --> 00:12:09,300
一个很长的 QKV 的序列

407
00:12:09,333 --> 00:12:12,233
变成我们编码后的相对比较短的序列

408
00:12:12,233 --> 00:12:15,066
然后信息之间也做了一个融合

409
00:12:15,300 --> 00:12:17,333
那简单的我们回顾一下数式的原理

410
00:12:17,333 --> 00:12:18,166
就是这个函数

411
00:12:18,166 --> 00:12:21,400
对磁向量 q 添加一个绝对的信息

412
00:12:21,400 --> 00:12:22,600
然后变成一个函数

413
00:12:22,600 --> 00:12:23,766
将成 q

414
00:12:23,766 --> 00:12:25,766
小 m 那 rotate Embedding

415
00:12:25,766 --> 00:12:27,666
就想 q m 跟 k n 之间

416
00:12:27,833 --> 00:12:29,966
进行一个点积的计算

417
00:12:29,966 --> 00:12:33,266
也就是希望 q m k 跟 F k n 之间

418
00:12:33,300 --> 00:12:35,533
能够有相对的位置信息

419
00:12:35,533 --> 00:12:36,933
就是 m 跟 n

420
00:12:37,133 --> 00:12:39,333
两个 token 位置之间的信息

421
00:12:39,333 --> 00:12:40,366
能够保留下来

422
00:12:40,366 --> 00:12:41,233
计算下来

423
00:12:41,233 --> 00:12:42,600
那这两个

424
00:12:42,600 --> 00:12:43,966
或者我们刚才那条函数

425
00:12:43,966 --> 00:12:46,366
怎么去计算才能够保存下来

426
00:12:46,366 --> 00:12:47,666
这里面只是数学的定义

427
00:12:47,700 --> 00:12:50,900
大家看我刚才的一个了解就行了

428
00:12:50,900 --> 00:12:52,200
比较简单

429
00:12:52,300 --> 00:12:54,166
那接下来为了让这个函数

430
00:12:54,166 --> 00:12:55,700
带有相对位置的信息

431
00:12:55,733 --> 00:12:57,300
所以我们需要把这个函数

432
00:12:57,300 --> 00:13:00,366
表象成一个跟 q k

433
00:13:00,366 --> 00:13:03,033
还有 m 跟 n 两个的一个函数

434
00:13:03,033 --> 00:13:05,566
我们另存为一个新的函数

435
00:13:05,566 --> 00:13:07,100
g q k m 乘以 n

436
00:13:07,133 --> 00:13:08,033
那 m 乘以 n

437
00:13:08,033 --> 00:13:08,766
m 减 n

438
00:13:08,766 --> 00:13:10,400
就代表两个向量之间

439
00:13:10,400 --> 00:13:12,233
相对位置的信息

440
00:13:12,233 --> 00:13:13,066
所以我们可以看到

441
00:13:13,100 --> 00:13:14,533
我们把绝对位置的信息

442
00:13:14,533 --> 00:13:17,133
跟相对位置的信息都保留下来了

443
00:13:17,133 --> 00:13:21,200
接下来就怎么去算或者这个函数嘛

444
00:13:21,900 --> 00:13:23,233
那有了这个信息之后

445
00:13:23,233 --> 00:13:25,633
我们看一下所谓的二维的位置编码

446
00:13:25,633 --> 00:13:27,233
举一个具体的例子

447
00:13:27,233 --> 00:13:29,000
非常的容易理解

448
00:13:29,000 --> 00:13:31,300
假设我们现在词向量是二维

449
00:13:31,333 --> 00:13:34,700
也就是只有那个 Q1 跟那个 Q0

450
00:13:34,800 --> 00:13:35,233
然后

451
00:13:35,233 --> 00:13:38,433
我们拿到一个位置的编码的函数

452
00:13:38,433 --> 00:13:39,000
那这里面

453
00:13:39,000 --> 00:13:39,600
可以看到

454
00:13:40,833 --> 00:13:41,433
蛮有意思

455
00:13:41,433 --> 00:13:43,866
就跟我们刚才的一个绝对位置编码

456
00:13:43,900 --> 00:13:44,900
其实蛮有意思

457
00:13:44,900 --> 00:13:47,100
蛮像的那 m

458
00:13:47,100 --> 00:13:48,700
就是一个下标

459
00:13:48,700 --> 00:13:51,566
把绝对位置的信息都加进来

460
00:13:51,566 --> 00:13:52,766
那我们这里面

461
00:13:52,766 --> 00:13:54,966
因为在一个矩阵里面

462
00:13:54,966 --> 00:13:57,600
我们想把它一个逆时针旋转 45 度

463
00:13:57,600 --> 00:13:59,066
弧为π除以 4

464
00:13:59,100 --> 00:14:01,933
所以我们就定义了 cos 等于 4 除以π

465
00:14:02,533 --> 00:14:03,166
π除以 4

466
00:14:03,166 --> 00:14:04,600
那我们就这么一乘

467
00:14:04,600 --> 00:14:07,066
就得到了一个新的二维向量了

468
00:14:07,066 --> 00:14:08,566
但是向量的模长没有改变

469
00:14:08,566 --> 00:14:09,300
依然是一

470
00:14:09,333 --> 00:14:10,733
所以整体的计算的过程

471
00:14:10,733 --> 00:14:13,333
从上面就变成下面的这种方式

472
00:14:13,333 --> 00:14:15,100
那我们数学化的来理解一下

473
00:14:15,100 --> 00:14:17,233
就是原来的这条向量蓝色

474
00:14:17,233 --> 00:14:18,400
经过一个旋转之后

475
00:14:18,400 --> 00:14:20,633
就变成新的这条向量了

476
00:14:20,633 --> 00:14:21,166
蛮有意思

477
00:14:21,166 --> 00:14:24,466
我们其实看的还是很明白

478
00:14:24,933 --> 00:14:27,166
那这意味着我们只需要将向量

479
00:14:27,166 --> 00:14:28,500
旋转某个角度

480
00:14:28,533 --> 00:14:29,300
就可以实现

481
00:14:29,300 --> 00:14:32,733
对该向量添加绝对的位置信息了

482
00:14:32,733 --> 00:14:35,033
把那个 m position 加进去了

483
00:14:35,033 --> 00:14:36,666
这个就是旋转位置

484
00:14:36,700 --> 00:14:38,800
一个旋转的理念

485
00:14:38,800 --> 00:14:41,833
所以说旋转 rotate 很重要

486
00:14:42,433 --> 00:14:44,500
只是一个数学矩阵的一个概念

487
00:14:44,533 --> 00:14:45,200
那我们现在

488
00:14:45,200 --> 00:14:47,200
把二维推广到高维

489
00:14:47,566 --> 00:14:48,366
高维的向量

490
00:14:48,366 --> 00:14:50,466
说实话其实非常的复杂

491
00:14:50,500 --> 00:14:53,100
所以我们这里面就变成两两一组

492
00:14:53,100 --> 00:14:55,533
然后分别进行一个旋转

493
00:14:55,533 --> 00:14:57,533
所以为什么看到一个示例

494
00:14:58,100 --> 00:14:59,400
就是刚才的这个图

495
00:14:59,400 --> 00:15:00,400
为什么是两个格子

496
00:15:00,400 --> 00:15:01,233
两个格子

497
00:15:01,233 --> 00:15:02,600
是因为我们还是

498
00:15:02,600 --> 00:15:04,566
为了方便计算和加速计算

499
00:15:04,566 --> 00:15:06,500
和并行的计算的时候

500
00:15:06,533 --> 00:15:07,566
分别用到

501
00:15:07,566 --> 00:15:08,200
那最终

502
00:15:08,200 --> 00:15:08,966
高维的向量

503
00:15:08,966 --> 00:15:10,633
可以表示成这边

504
00:15:10,633 --> 00:15:11,466
我们左边

505
00:15:11,500 --> 00:15:13,600
认为是高维向量的一个旋转矩阵

506
00:15:13,600 --> 00:15:14,000
右边

507
00:15:14,000 --> 00:15:17,433
就是一个具体的 token 了

508
00:15:17,433 --> 00:15:18,100
那蛮有意思

509
00:15:18,133 --> 00:15:18,700
我们看一下

510
00:15:18,700 --> 00:15:19,200
实际上

511
00:15:19,200 --> 00:15:20,700
刚才的那个计算公式

512
00:15:20,733 --> 00:15:22,333
是缺乏远程的衰减性

513
00:15:22,333 --> 00:15:24,833
也就是不管我的一个相对距离

514
00:15:24,833 --> 00:15:28,000
越长基本上你会发现一个波频

515
00:15:28,000 --> 00:15:28,866
其实是

516
00:15:28,900 --> 00:15:30,433
就是一个刚才讲到

517
00:15:30,433 --> 00:15:31,800
两个函数的累积

518
00:15:31,800 --> 00:15:32,666
你会发现

519
00:15:32,900 --> 00:15:34,733
基本上在不同的频率里面

520
00:15:34,733 --> 00:15:35,366
基本上一致

521
00:15:35,366 --> 00:15:36,033
所以这个时候

522
00:15:36,033 --> 00:15:37,500
就缺乏远程的衰减性了

523
00:15:37,533 --> 00:15:38,500
也就是我第一个单词

524
00:15:38,500 --> 00:15:39,766
跟最后一个单词的关联关系

525
00:15:39,766 --> 00:15:41,100
是很接近

526
00:15:41,133 --> 00:15:42,033
没有区别

527
00:15:42,700 --> 00:15:45,000
那这里面就借鉴了绝对位置编码

528
00:15:45,000 --> 00:15:46,600
把每个分组的θ

529
00:15:46,600 --> 00:15:49,466
就设置了一个不同的常量

530
00:15:49,500 --> 00:15:51,500
从而引入了远程衰减

531
00:15:51,500 --> 00:15:53,333
因为我们希望当前的单词

532
00:15:53,333 --> 00:15:55,766
跟它附近的单词是比较强相关

533
00:15:55,766 --> 00:15:56,566
而不是说

534
00:15:56,566 --> 00:15:58,000
当前的单词跟最后一个单词

535
00:15:58,000 --> 00:15:59,600
其实非常强关系

536
00:15:59,600 --> 00:16:00,500
那这个没有必然

537
00:16:00,533 --> 00:16:02,600
因为我们说话肯定是有上下文

538
00:16:02,600 --> 00:16:03,700
语境嘛所以

539
00:16:03,733 --> 00:16:05,833
我们就沿用了一个绝对位置

540
00:16:05,833 --> 00:16:07,433
三角形的编码了

541
00:16:07,700 --> 00:16:09,366
这就设置了一个参数

542
00:16:09,366 --> 00:16:10,966
可以将高位的向量

543
00:16:10,966 --> 00:16:13,233
就更新成这一条

544
00:16:13,400 --> 00:16:14,866
使得我们两个函数的累积

545
00:16:14,900 --> 00:16:16,133
也叫 q 乘以 k

546
00:16:16,133 --> 00:16:19,533
就能够有一个衰减的系数

547
00:16:19,533 --> 00:16:21,433
那这个我们叫做远程衰减性

548
00:16:21,433 --> 00:16:23,400
远程衰减性可以调

549
00:16:23,400 --> 00:16:25,300
就是根据一个 rotate bending

550
00:16:25,333 --> 00:16:26,900
里面的一个入参

551
00:16:26,900 --> 00:16:28,800
那我们可以看到还是有的调

552
00:16:28,800 --> 00:16:30,866
所以说我们现在大模型的调参侠

553
00:16:30,966 --> 00:16:33,200
是偶尔去看一下一个单词

554
00:16:33,200 --> 00:16:34,366
或者关联关系

555
00:16:34,366 --> 00:16:35,666
这个大模型的好不好

556
00:16:35,700 --> 00:16:38,433
不过现在默认像 Code LLaMA

557
00:16:38,433 --> 00:16:40,266
就是将这个远程缩减系数

558
00:16:40,300 --> 00:16:42,000
就设的非常的大

559
00:16:42,000 --> 00:16:44,133
LLama2 也设的非常的大

560
00:16:44,133 --> 00:16:45,200
但是更大的 base

561
00:16:45,200 --> 00:16:48,833
也会使得注意机制就变弱了

562
00:16:48,833 --> 00:16:50,266
就是当前的关系

563
00:16:50,300 --> 00:16:52,833
可能只跟之前有关联关系

564
00:16:53,166 --> 00:16:55,633
可能跟更长的就关联关系比较少了

565
00:16:55,633 --> 00:16:56,900
所以这个函数的设置

566
00:16:56,933 --> 00:16:57,933
还是蛮有意思

567
00:16:57,933 --> 00:17:01,133
就是可能代码代码的长度非常长

568
00:17:01,133 --> 00:17:02,166
所以参数

569
00:17:02,166 --> 00:17:03,266
设的相对比较大

570
00:17:03,300 --> 00:17:04,300
那普通的对话

571
00:17:04,300 --> 00:17:05,133
就不需要那么大

572
00:17:05,133 --> 00:17:06,600
但是已经很大了

573
00:17:07,233 --> 00:17:07,800
基本上

574
00:17:07,800 --> 00:17:09,666
我们已经做完所有的总结了

575
00:17:09,700 --> 00:17:12,233
讲完整个 Embedding 的内容

576
00:17:12,233 --> 00:17:13,700
特别是 postion Embedding

577
00:17:13,733 --> 00:17:15,966
那我们简单的做个小结

578
00:17:15,966 --> 00:17:17,800
现在一个外推性

579
00:17:17,800 --> 00:17:19,966
还是需要进行创新

580
00:17:19,966 --> 00:17:21,433
为什么整个大模型

581
00:17:21,433 --> 00:17:22,800
都会选择 rotating Embedding

582
00:17:22,800 --> 00:17:24,200
是因为它的长度

583
00:17:24,733 --> 00:17:25,566
可以外推

584
00:17:25,566 --> 00:17:27,433
而且可以支持非常长

585
00:17:27,433 --> 00:17:28,900
因为我们刚才讲讲过了

586
00:17:28,933 --> 00:17:31,800
就是支持了一个远程衰减之后

587
00:17:31,800 --> 00:17:33,866
整体能够支持非常长的长序列

588
00:17:33,900 --> 00:17:36,166
所以说现在成为了整个大模型的主流

589
00:17:36,333 --> 00:17:36,800
另外的话

590
00:17:36,800 --> 00:17:37,866
我们看一下第二个

591
00:17:37,900 --> 00:17:39,900
就是任务导向的设计

592
00:17:39,900 --> 00:17:41,600
现在短文本

593
00:17:41,733 --> 00:17:43,533
真的短文本可以用绝对位置编码

594
00:17:43,533 --> 00:17:46,033
因为计算起来的复杂度没那么的高

595
00:17:46,033 --> 00:17:47,300
然后只是学习完之后

596
00:17:47,333 --> 00:17:47,966
就计算了

597
00:17:47,966 --> 00:17:48,866
然后但是

598
00:17:48,900 --> 00:17:49,700
长文本

599
00:17:49,700 --> 00:17:51,700
基本上都会用到 rotate embedding

600
00:17:51,700 --> 00:17:53,233
包括一些

601
00:17:53,300 --> 00:17:53,966
多模态

602
00:17:53,966 --> 00:17:55,666
哎呀讲到多模态就来了

603
00:17:55,700 --> 00:17:57,700
那多模态的位置编码

604
00:17:57,833 --> 00:17:59,166
到底用什么位置编码

605
00:17:59,166 --> 00:18:00,000
有什么区别

606
00:18:00,000 --> 00:18:02,466
说实话还是用 rotate embedding

607
00:18:02,533 --> 00:18:04,400
但是大家也可以去思考一下

608
00:18:04,500 --> 00:18:07,500
多模态 rotate embedding 怎么去用

609
00:18:07,500 --> 00:18:08,333
那今天的内容

610
00:18:08,333 --> 00:18:09,100
就到这里为止了

611
00:18:09,100 --> 00:18:09,600
谢谢各位

612
00:18:09,600 --> 00:18:11,200
拜了个拜

