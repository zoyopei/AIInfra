1
00:00:00,000 --> 00:00:01,766
内容/录制:Z0MI 酱，视频后期/字幕:梁嘉铭

2
00:00:02,800 --> 00:00:03,366
hello 大家好

3
00:00:03,366 --> 00:00:05,266
我是那个逼自己一把

4
00:00:05,300 --> 00:00:07,833
还不如放自己一马的 ZOMI

5
00:00:12,733 --> 00:00:14,366
今天还是在变形金刚

6
00:00:14,366 --> 00:00:17,366
整个 Transformer 系列里面的长序列支持

7
00:00:17,933 --> 00:00:19,900
回顾一下整个 Transformer attention 架构

8
00:00:19,900 --> 00:00:21,566
其实已经分开了三节内容

9
00:00:21,566 --> 00:00:23,200
跟大家去分享了

10
00:00:23,200 --> 00:00:23,433
第一节

11
00:00:23,433 --> 00:00:26,500
就是看一下整个 attention 的核心的机制

12
00:00:26,533 --> 00:00:28,033
然后看一下 attention 的变种

13
00:00:28,033 --> 00:00:31,100
MQA GQA MLA 现在来到了

14
00:00:31,133 --> 00:00:32,633
Transformer 的一个长序列了

15
00:00:32,633 --> 00:00:34,166
看一下长序列有哪些挑战

16
00:00:34,166 --> 00:00:36,600
和怎么去解决长序列的问题

17
00:00:36,600 --> 00:00:37,366
当然这里面

18
00:00:37,366 --> 00:00:38,633
不是指通过分布式

19
00:00:38,633 --> 00:00:40,666
并行去解决长序列的问题

20
00:00:40,700 --> 00:00:43,833
而是通过模型架构的创新

21
00:00:43,966 --> 00:00:46,200
去解决长序列的事情

22
00:00:46,300 --> 00:00:47,133
那蛮有意思

23
00:00:47,133 --> 00:00:48,700
就是整个长序列

24
00:00:48,700 --> 00:00:49,500
现在

25
00:00:49,500 --> 00:00:51,933
分开两个内容跟大家一起去分享

26
00:00:52,033 --> 00:00:52,433
第一种

27
00:00:52,433 --> 00:00:55,966
就是可能在 2024 年之前

28
00:00:56,033 --> 00:00:58,900
两个解决长序列的 Transformer 的架构

29
00:00:59,400 --> 00:01:01,666
那第一种就是 longformer

30
00:01:01,766 --> 00:01:04,833
第二种就是 Transformer XL

31
00:01:04,900 --> 00:01:05,433
那第二种

32
00:01:05,433 --> 00:01:09,166
就是专门针对 attention 架构进行改进

33
00:01:09,166 --> 00:01:10,166
就更细化了

34
00:01:10,166 --> 00:01:13,600
变成 NSA 还有另外的话就是 MOBA 了

35
00:01:13,600 --> 00:01:15,233
NSA 就是 deepseek

36
00:01:15,233 --> 00:01:17,000
幻方自己去发明

37
00:01:17,000 --> 00:01:17,600
那 MOBA

38
00:01:17,600 --> 00:01:20,266
反正对应的是 Kimi 自己去发明

39
00:01:20,300 --> 00:01:20,900
蛮有意思

40
00:01:20,900 --> 00:01:22,900
都是国产的内容

41
00:01:22,900 --> 00:01:23,566
那现在

42
00:01:23,566 --> 00:01:25,633
马上就来到了第一个内容

43
00:01:25,633 --> 00:01:29,566
看一下 longformer 到底是个什么东西

44
00:01:29,733 --> 00:01:30,300
那首先

45
00:01:30,300 --> 00:01:32,333
longformer 最重要的提出点

46
00:01:32,333 --> 00:01:34,800
就是 sliding window attention

47
00:01:34,800 --> 00:01:37,233
那看一下 longformer 的这篇文章

48
00:01:37,533 --> 00:01:38,000
好了

49
00:01:38,000 --> 00:01:40,566
现在打开了 longformer 这篇文章

50
00:01:40,566 --> 00:01:41,066
蛮有意思

51
00:01:41,100 --> 00:01:43,500
longformer 主要是 long document Transformer

52
00:01:43,500 --> 00:01:46,000
也就是长文本的 Transformer

53
00:01:46,000 --> 00:01:47,833
当时还不叫上下文

54
00:01:47,933 --> 00:01:48,700
因为这篇文章

55
00:01:48,700 --> 00:01:51,200
相对发表的比较早一点

56
00:01:51,200 --> 00:01:53,366
那现在一个个的轮下来

57
00:01:53,366 --> 00:01:54,166
看一下 longformer

58
00:01:54,166 --> 00:01:57,000
最重要的其实还是这一个图了

59
00:01:57,000 --> 00:01:57,566
那第一种

60
00:01:57,566 --> 00:01:59,300
就是 Transformer 的架构

61
00:01:59,333 --> 00:02:00,966
或者 attention 的时候

62
00:02:00,966 --> 00:02:03,000
我的一个输入跟输出

63
00:02:03,000 --> 00:02:05,666
其实是对应已成为一个矩阵

64
00:02:05,700 --> 00:02:07,533
那这里面的 sliding window

65
00:02:07,533 --> 00:02:09,233
有点有点像

66
00:02:09,233 --> 00:02:10,566
就是 CNN 里面

67
00:02:10,566 --> 00:02:12,066
空洞卷积其中一部分

68
00:02:12,100 --> 00:02:13,900
一个卷积

69
00:02:14,000 --> 00:02:16,366
那这个 dilated sliding Windows

70
00:02:16,366 --> 00:02:19,266
有点像 dilated 那个卷积核

71
00:02:19,300 --> 00:02:21,133
错开来那最后

72
00:02:21,133 --> 00:02:23,533
它还有一个 global 加 sliding window

73
00:02:23,533 --> 00:02:25,133
也就是把那个

74
00:02:25,133 --> 00:02:25,966
第二种

75
00:02:25,966 --> 00:02:27,066
就是 sliding 这种

76
00:02:27,100 --> 00:02:28,166
加上 global

77
00:02:28,166 --> 00:02:30,466
把全局也就是有些能注意到

78
00:02:30,500 --> 00:02:32,300
有些注意不到的这种方式

79
00:02:32,300 --> 00:02:33,433
融合起来

80
00:02:33,433 --> 00:02:34,700
那下面的内容

81
00:02:34,733 --> 00:02:38,000
基本上就是讲刚才的这 3 种模式

82
00:02:38,000 --> 00:02:39,366
到底是怎么去训

83
00:02:39,366 --> 00:02:40,600
怎么去实现

84
00:02:40,766 --> 00:02:43,200
当然了这篇文章的一个实现

85
00:02:43,200 --> 00:02:44,600
是基于 PyTorch 的一个

86
00:02:44,600 --> 00:02:46,666
还有 Openai 的 Triton 来去实现

87
00:02:46,700 --> 00:02:48,233
写的一些算子

88
00:02:48,233 --> 00:02:50,400
那最后还有一些实验部分

89
00:02:50,400 --> 00:02:51,266
那不管怎么样

90
00:02:51,300 --> 00:02:53,300
这篇文章还是比较好懂

91
00:02:53,300 --> 00:02:55,900
最核心的就刚才的这一个图

92
00:02:56,166 --> 00:02:57,700
同样的回到 PPT 里面

93
00:02:57,700 --> 00:02:59,233
看一下 sliding Windows

94
00:02:59,333 --> 00:03:01,566
其实最重要的就是设置了一个窗口

95
00:03:01,566 --> 00:03:04,400
w 那规定了这个每个 token

96
00:03:04,400 --> 00:03:06,600
只能看到 w 个 token

97
00:03:06,600 --> 00:03:08,600
你要看到自己左边和右边

98
00:03:08,600 --> 00:03:09,600
一个 token

99
00:03:09,633 --> 00:03:10,566
那两侧

100
00:03:10,566 --> 00:03:11,166
分别是

101
00:03:11,166 --> 00:03:12,800
像这里面的左侧是

102
00:03:12,800 --> 00:03:15,400
w 个右侧是 1/2 w 个

103
00:03:15,400 --> 00:03:18,433
所以说这里面是这么一个过程

104
00:03:18,433 --> 00:03:19,900
整体的时间复杂度

105
00:03:19,933 --> 00:03:21,566
是 o n 乘以 w

106
00:03:21,666 --> 00:03:22,300
不过没关系

107
00:03:22,333 --> 00:03:24,633
可以先不管时间复杂度

108
00:03:24,633 --> 00:03:25,066
这里面

109
00:03:25,100 --> 00:03:26,133
就不用担心了

110
00:03:26,133 --> 00:03:27,700
因为整个 longformer

111
00:03:27,700 --> 00:03:30,600
没有办法建立整个序列的信息

112
00:03:30,600 --> 00:03:32,400
因为整个 Transformer 的结构

113
00:03:32,400 --> 00:03:33,833
是一层一层堆叠

114
00:03:33,833 --> 00:03:36,700
当前层也就当前的 sequence

115
00:03:36,733 --> 00:03:38,833
当前对应的一个序列

116
00:03:38,833 --> 00:03:40,566
或者当前对应的 token
x
117
00:03:40,566 --> 00:03:41,966
这个是 token_1 了

118
00:03:41,966 --> 00:03:43,166
然后对应的上面

119
00:03:43,166 --> 00:03:44,500
也是 token_1

120
00:03:44,533 --> 00:03:45,800
那可能注意力机制

121
00:03:45,800 --> 00:03:49,300
只留在 token_1 的左右两侧的上下文

122
00:03:49,333 --> 00:03:50,000
不过没关系

123
00:03:50,000 --> 00:03:50,966
因为整个 Transformer 结构

124
00:03:50,966 --> 00:03:52,666
刚才讲到了

125
00:03:52,700 --> 00:03:54,900
它是一个层层堆叠的结构

126
00:03:54,900 --> 00:03:55,566
那这里面

127
00:03:55,566 --> 00:03:56,866
是第 L1 层

128
00:03:56,900 --> 00:03:58,966
有可能 L2 层它错开

129
00:03:59,000 --> 00:04:01,300
所以说高层的 Transformer 架构

130
00:04:01,333 --> 00:04:03,633
会比低层的 Transformer 架构

131
00:04:03,633 --> 00:04:05,466
有更好的感受野

132
00:04:05,500 --> 00:04:06,900
那感受野这个翻译

133
00:04:06,900 --> 00:04:10,533
其实也是来自于 CNN 的一个翻译

134
00:04:10,533 --> 00:04:12,100
因此整个 Transformer

135
00:04:12,100 --> 00:04:14,300
能够很好地去融合局部的信息

136
00:04:14,300 --> 00:04:15,633
到全局的信息里面

137
00:04:15,633 --> 00:04:16,500
sliding Windows

138
00:04:16,533 --> 00:04:17,133
而且

139
00:04:17,133 --> 00:04:19,433
很好地扩充模型的长度

140
00:04:19,433 --> 00:04:21,233
因为不需要全部

141
00:04:21,233 --> 00:04:22,066
一些

142
00:04:22,100 --> 00:04:23,733
所有的序列的上下文的信息

143
00:04:23,733 --> 00:04:25,033
都感知到

144
00:04:25,033 --> 00:04:26,566
那其实这篇文章

145
00:04:26,566 --> 00:04:29,766
为了更好地去感知哎

146
00:04:29,766 --> 00:04:31,566
更多的信息就来了

147
00:04:31,566 --> 00:04:33,066
提出了另外一个 Dilate

148
00:04:33,100 --> 00:04:34,800
那个 sliding Windows 了

149
00:04:34,800 --> 00:04:35,366
这里面

150
00:04:35,366 --> 00:04:37,166
刚才讲到的左边的 b

151
00:04:37,166 --> 00:04:40,000
sliding Windows 只考虑到了长度为 w

152
00:04:40,233 --> 00:04:41,200
上下文

153
00:04:41,200 --> 00:04:42,766
这里面 1/2 w

154
00:04:42,766 --> 00:04:44,600
这里面也是 1/2 w

155
00:04:44,633 --> 00:04:45,666
那这种方式

156
00:04:45,666 --> 00:04:48,066
在不增加模型的计算量之下

157
00:04:48,133 --> 00:04:50,233
在整个

158
00:04:50,233 --> 00:04:52,566
 sliding Windows attention 里面

159
00:04:52,566 --> 00:04:53,766
就加了一个空隙

160
00:04:53,766 --> 00:04:55,233
所以可以看到中间

161
00:04:55,233 --> 00:04:57,166
有很多小小的白点

162
00:04:57,166 --> 00:04:58,200
这些空隙

163
00:04:58,200 --> 00:05:01,566
那这里面整个空隙的幅度就为 d

164
00:05:01,566 --> 00:05:03,400
这样的话可以在序列当中

165
00:05:03,400 --> 00:05:06,766
让每个 token 的感受野或者感知野

166
00:05:06,766 --> 00:05:10,166
sense 整体扩散的比较大

167
00:05:10,166 --> 00:05:12,466
要变成了 d 乘以 w 了

168
00:05:12,500 --> 00:05:13,733
那在第 n 层

169
00:05:13,733 --> 00:05:16,600
感受野就是 m 乘以 d 乘以 w

170
00:05:16,600 --> 00:05:17,566
这种方式

171
00:05:17,566 --> 00:05:19,233
可以让模型

172
00:05:19,233 --> 00:05:21,200
能够会让序列

173
00:05:21,200 --> 00:05:24,200
学习到更多上下文序列的东西

174
00:05:24,200 --> 00:05:25,900
但作者觉得这样还不够

175
00:05:25,933 --> 00:05:28,100
于是又提出了第三种

176
00:05:28,100 --> 00:05:31,000
就是 global 加 sliding Windows

177
00:05:31,000 --> 00:05:32,466
虽然有很多种

178
00:05:32,500 --> 00:05:33,433
但说实话

179
00:05:33,433 --> 00:05:34,866
现在业界用的更多

180
00:05:34,900 --> 00:05:36,833
是单纯的 sliding Windows

181
00:05:36,833 --> 00:05:39,066
因为随着模型的层数的增加

182
00:05:39,100 --> 00:05:40,400
可以感受到

183
00:05:40,400 --> 00:05:42,800
不同的一个序列的内容

184
00:05:42,833 --> 00:05:43,500
那不管怎么样

185
00:05:43,533 --> 00:05:44,533
现在回到了

186
00:05:44,533 --> 00:05:46,633
global 加 sliding Windows 里面

187
00:05:46,633 --> 00:05:48,266
这里面就分别

188
00:05:48,300 --> 00:05:52,766
将原始的输入输出映射到 q kv 三个矩阵空间里面

189
00:05:52,933 --> 00:05:54,400
很有意思就是 longformer

190
00:05:54,400 --> 00:05:55,300
将这两种

191
00:05:55,333 --> 00:05:57,733
单独映射到两个独立的空间

192
00:05:57,766 --> 00:05:58,100
第一种

193
00:05:58,133 --> 00:06:00,400
就是其实把 b 跟 c

194
00:06:00,400 --> 00:06:02,200
其实是融合到一起

195
00:06:02,333 --> 00:06:02,800
主要

196
00:06:02,800 --> 00:06:05,966
使用两种 attention 来做进行映射

197
00:06:05,966 --> 00:06:07,200
这两个 attention

198
00:06:07,200 --> 00:06:08,300
这里面值得注意了

199
00:06:08,333 --> 00:06:11,000
一种叫做假设是 s

200
00:06:11,000 --> 00:06:12,066
就 sliding Windows

201
00:06:12,100 --> 00:06:13,966
就是 KV 对应的 s

202
00:06:13,966 --> 00:06:16,366
来计算整个 sliding Windows attention

203
00:06:16,366 --> 00:06:18,166
另外的话使用那个 q

204
00:06:18,166 --> 00:06:21,266
k v 的 G 来计算 global 的一个 attention

205
00:06:21,300 --> 00:06:23,566
通过这种方式来去计算

206
00:06:23,566 --> 00:06:27,100
左右两边还有中间的这一块

207
00:06:27,133 --> 00:06:29,233
分别的来进行一个处理

208
00:06:29,233 --> 00:06:30,800
所以说第三种

209
00:06:30,800 --> 00:06:32,400
现在用的比较少

210
00:06:32,400 --> 00:06:34,033
整体的效果怎么说

211
00:06:34,033 --> 00:06:35,000
它不是说效果不好

212
00:06:35,000 --> 00:06:36,066
而是计算量了

213
00:06:36,100 --> 00:06:38,100
或者计算的复杂度了偏高了

214
00:06:38,100 --> 00:06:40,033
所以说现在用的更多的是 b

215
00:06:40,033 --> 00:06:41,633
sliding window attention

216
00:06:42,933 --> 00:06:43,900
到目前为止

217
00:06:43,900 --> 00:06:46,366
现在来到了 Transformer 的一个 XL

218
00:06:46,366 --> 00:06:48,566
这一篇文章是 Kimi 杨植麟

219
00:06:48,566 --> 00:06:52,766
创业之前在清华发表的一篇论文了

220
00:06:52,766 --> 00:06:55,000
让打开看一下这篇论文

221
00:06:55,433 --> 00:06:55,966
实际上

222
00:06:55,966 --> 00:07:00,100
这篇论文还是主要解决固定长度

223
00:07:00,133 --> 00:07:01,600
一些长上下文

224
00:07:01,600 --> 00:07:03,766
哎说实话效果不好

225
00:07:03,800 --> 00:07:06,400
所以作者就提出了一个 Transformer XL

226
00:07:06,400 --> 00:07:08,600
那主要是分开训练阶段

227
00:07:08,600 --> 00:07:09,700
跟推理阶段

228
00:07:09,733 --> 00:07:11,566
或者叫做验证阶段不一样

229
00:07:11,566 --> 00:07:14,633
里面最重要的就是来到了一个

230
00:07:14,633 --> 00:07:15,866
很重要的就是 segmentation

231
00:07:15,900 --> 00:07:17,733
做一个分片的概念

232
00:07:17,733 --> 00:07:18,366
通过分片

233
00:07:18,366 --> 00:07:19,900
使得整个模型

234
00:07:19,933 --> 00:07:21,800
能够处理更好的一些

235
00:07:21,800 --> 00:07:23,300
上下文更长的上下文

236
00:07:23,366 --> 00:07:23,900
另外的话

237
00:07:23,933 --> 00:07:25,100
在推理阶段

238
00:07:25,100 --> 00:07:27,433
能也能够做一个大规模的并行

239
00:07:27,433 --> 00:07:29,466
可以看到这里面有非常多的连线

240
00:07:29,566 --> 00:07:30,000
后面

241
00:07:30,000 --> 00:07:32,800
会简单的用示意图来演示一下

242
00:07:32,800 --> 00:07:33,300
那当然了

243
00:07:33,333 --> 00:07:35,566
这里面就讲了很多数学的概念

244
00:07:35,566 --> 00:07:36,366
怎么去实现了

245
00:07:36,366 --> 00:07:38,900
这篇文章理解起来相对比较难

246
00:07:39,400 --> 00:07:40,100
在这里面

247
00:07:40,133 --> 00:07:41,366
终于用两个动图

248
00:07:41,366 --> 00:07:43,266
来跟大家简单的分享一下

249
00:07:43,300 --> 00:07:44,633
首先 Transformer XL

250
00:07:44,633 --> 00:07:45,266
这篇文章

251
00:07:45,300 --> 00:07:47,766
主要是解决长上下文的碎片化

252
00:07:47,766 --> 00:07:49,400
还有推理速度慢的问题

253
00:07:49,400 --> 00:07:51,466
这里面主要注意了两个

254
00:07:51,500 --> 00:07:53,233
第一个上下文碎片化

255
00:07:53,233 --> 00:07:54,866
第二个就推理速度慢

256
00:07:54,900 --> 00:07:55,300
所以

257
00:07:55,300 --> 00:07:58,100
针对第一个问题上下文的碎片化

258
00:07:58,100 --> 00:08:01,533
就提出了一个片段递归的概念

259
00:08:01,733 --> 00:08:04,900
为了解决长期的一个上下文的依赖

260
00:08:04,900 --> 00:08:07,433
而作者还对绝对位置编码

261
00:08:07,433 --> 00:08:08,766
进行了一个改进

262
00:08:08,766 --> 00:08:10,200
因为序列越长

263
00:08:10,200 --> 00:08:11,266
你压缩了

264
00:08:11,300 --> 00:08:13,100
那位置编码也需要改变

265
00:08:13,100 --> 00:08:15,600
就是每个 token 的位置

266
00:08:15,666 --> 00:08:15,900
并且

267
00:08:15,933 --> 00:08:19,333
推出了相对位置编码的相关的概念

268
00:08:19,333 --> 00:08:20,766
当然了这个 Transformer XL

269
00:08:20,766 --> 00:08:23,066
是当时候为了解决长下乡文

270
00:08:23,100 --> 00:08:24,566
非常好的一篇文章

271
00:08:24,566 --> 00:08:25,800
但实际上用起来

272
00:08:25,800 --> 00:08:28,966
并没有太多的实用性价值

273
00:08:28,966 --> 00:08:31,766
那简单的看一下整个片段递归

274
00:08:31,766 --> 00:08:33,600
在整个 Transformer 架构里面

275
00:08:33,600 --> 00:08:35,100
很重要的就是

276
00:08:35,133 --> 00:08:38,166
引入了这个 fixed 或者 new segmentation

277
00:08:38,166 --> 00:08:40,766
segmentation 分片的这么一个概念

278
00:08:40,766 --> 00:08:44,766
在整个 Transformer XL 的一个机制里面

279
00:08:44,766 --> 00:08:46,200
上一个片段的状态

280
00:08:46,200 --> 00:08:48,200
例如这是片段一

281
00:08:48,300 --> 00:08:51,933
是 s1 segmentation 1 会被缓存下来

282
00:08:51,933 --> 00:08:54,000
然后在计算当前的时候

283
00:08:54,000 --> 00:08:55,200
下一个的时候

284
00:08:55,200 --> 00:08:57,800
重复使用上一个时间片段的内容

285
00:08:57,800 --> 00:08:59,266
也就是算 S2 的时候

286
00:08:59,300 --> 00:09:02,433
会把 s1 的一个上下文的信息

287
00:09:02,433 --> 00:09:06,200
隐藏状态的信息加进来进行计算

288
00:09:06,200 --> 00:09:08,566
这种叫做片段递归

289
00:09:08,566 --> 00:09:09,300
那后面

290
00:09:09,333 --> 00:09:12,000
可以看一个动图来去实现

291
00:09:12,466 --> 00:09:14,433
那动图马上就来了

292
00:09:14,433 --> 00:09:16,233
在整个原始的 Transformer 架构

293
00:09:16,233 --> 00:09:17,766
可能是一个片段一个片段

294
00:09:17,766 --> 00:09:18,700
一个片段的算

295
00:09:18,733 --> 00:09:20,566
因为模型太长了

296
00:09:20,566 --> 00:09:22,200
基本上都会固定片段的去算

297
00:09:22,200 --> 00:09:23,766
一个 Transformer

298
00:09:24,000 --> 00:09:24,766
那另外的话

299
00:09:24,766 --> 00:09:26,600
整个 Transformer XL 就把

300
00:09:26,600 --> 00:09:28,233
这里面的信息

301
00:09:28,233 --> 00:09:30,500
s1 去记录下来

302
00:09:30,533 --> 00:09:33,133
S2 的时候会把 S1 的信息给到 S2

303
00:09:33,133 --> 00:09:34,333
那算 S3 的时候

304
00:09:34,333 --> 00:09:36,600
会把 S2 的信息给到 S3

305
00:09:36,600 --> 00:09:36,833
但

306
00:09:36,833 --> 00:09:39,833
S2 的信息已经融合了 S1 之前的信息

307
00:09:39,833 --> 00:09:42,400
通过这种方式做一个片段递归

308
00:09:42,400 --> 00:09:46,266
说白了把 RNN 的思路引进来了

309
00:09:46,300 --> 00:09:50,566
去解决 Transformer 架构的长序列的问题

310
00:09:50,833 --> 00:09:51,700
那同样

311
00:09:51,733 --> 00:09:52,166
这里面

312
00:09:52,166 --> 00:09:54,366
除了长序列的问题以外

313
00:09:54,400 --> 00:09:55,900
实际上很重要的一点就是

314
00:09:55,933 --> 00:09:57,800
你既然解决了序列问题

315
00:09:57,800 --> 00:09:58,600
序列越长了

316
00:09:58,600 --> 00:10:01,500
但是位置编码肯定也需要改进

317
00:10:01,533 --> 00:10:02,400
所以这里面

318
00:10:02,400 --> 00:10:05,200
就参考了一个相对位置编码的信息

319
00:10:05,200 --> 00:10:07,100
加进去 Transformer XL 里面

320
00:10:07,133 --> 00:10:07,500
同样

321
00:10:07,500 --> 00:10:09,200
可以看到这条公式非常的复杂

322
00:10:09,200 --> 00:10:11,900
但是重点提取有用的信息

323
00:10:11,933 --> 00:10:13,900
就是 q 乘以 k

324
00:10:13,966 --> 00:10:15,800
q 乘以 k q 乘以 k

325
00:10:15,900 --> 00:10:19,700
前后面多了很多相关的一些记录

326
00:10:19,700 --> 00:10:20,500
那这些记录

327
00:10:20,500 --> 00:10:23,566
就是对应一个 position in bending

328
00:10:23,566 --> 00:10:25,066
位置编码的信息

329
00:10:25,100 --> 00:10:26,366
当然有兴趣的小伙伴

330
00:10:26,366 --> 00:10:28,700
也可以深入的去看一下这篇文章

331
00:10:28,733 --> 00:10:29,933
讲的内容但 ZOMI

332
00:10:29,933 --> 00:10:32,133
可能觉得并不是那么的关心

333
00:10:32,133 --> 00:10:34,400
因为这篇文章其实已经过时了

334
00:10:34,400 --> 00:10:35,566
那简单吸收一下

335
00:10:35,566 --> 00:10:39,033
他的知识和概念和精华就可以了

336
00:10:39,100 --> 00:10:40,600
同样的在推理阶段

337
00:10:40,600 --> 00:10:41,166
可以看到

338
00:10:41,166 --> 00:10:44,066
传统的推理的是当前的序列

339
00:10:44,100 --> 00:10:45,833
依赖于下一个时间段的序列

340
00:10:45,833 --> 00:10:48,000
这么并行去做一个推理

341
00:10:48,000 --> 00:10:49,666
那在整个 Transformer XL 里面

342
00:10:49,700 --> 00:10:52,833
就进行了一个跨域叠加的一个推

343
00:10:52,833 --> 00:10:54,366
推理可以看到基本上

344
00:10:54,366 --> 00:10:56,666
推理的时候可以做的更快

345
00:10:56,700 --> 00:10:57,433
然后效率更高

346
00:10:57,433 --> 00:11:01,033
而且可以感知更长的上下文的内容

347
00:11:01,533 --> 00:11:02,300
如果大家觉得

348
00:11:02,300 --> 00:11:04,300
如果大家觉得 ZOMI 讲的比较水

349
00:11:04,300 --> 00:11:05,233
没有关系

350
00:11:05,233 --> 00:11:08,466
因为将会在后面推出文字版

351
00:11:08,500 --> 00:11:11,400
然后详细的把这些课程的内容

352
00:11:11,400 --> 00:11:12,500
通过文字去记载

353
00:11:12,533 --> 00:11:14,166
因为文字能够表达大量

354
00:11:14,166 --> 00:11:15,466
复杂的一个计算

355
00:11:15,500 --> 00:11:16,600
还有数学公式

356
00:11:22,000 --> 00:11:24,466
那么现在来到了 deepseek NSA

357
00:11:25,133 --> 00:11:26,300
NSA 的 s

358
00:11:26,300 --> 00:11:27,766
主要是做 Sparse

359
00:11:27,766 --> 00:11:28,766
主要提出了的话

360
00:11:28,766 --> 00:11:29,866
解决了两个问题

361
00:11:29,900 --> 00:11:32,300
一个就是长上下文的压缩

362
00:11:32,300 --> 00:11:34,300
第二个就是 self attention

363
00:11:34,533 --> 00:11:36,733
对 self attention 进行一个修改

364
00:11:36,733 --> 00:11:40,366
第三个就引入了一个稀疏的 attention

365
00:11:40,366 --> 00:11:41,100
那基本上

366
00:11:41,133 --> 00:11:43,633
整一篇文章就解决了这三个问题

367
00:11:43,633 --> 00:11:44,566
现在

368
00:11:44,566 --> 00:11:47,200
同样的去打开这篇文章

369
00:11:47,200 --> 00:11:50,500
NSA 就是 native spaces attention

370
00:11:50,533 --> 00:11:51,433
那这里面很有意思

371
00:11:51,433 --> 00:11:52,666
就是看一下它重点

372
00:11:52,700 --> 00:11:53,533
讲了两个点

373
00:11:53,533 --> 00:11:55,300
一个是硬件感知

374
00:11:55,400 --> 00:11:55,866
第二个

375
00:11:55,900 --> 00:11:58,733
就是训练的时候的一个稀疏性

376
00:11:58,766 --> 00:11:59,966
那训练的时候稀疏性

377
00:11:59,966 --> 00:12:01,433
更多写一个稀疏算子

378
00:12:01,433 --> 00:12:02,433
硬件感知

379
00:12:02,433 --> 00:12:02,966
更多是指

380
00:12:02,966 --> 00:12:06,100
一个具体的实现的时候

381
00:12:06,133 --> 00:12:09,166
专门是针对英伟达的 H800 deepseek

382
00:12:09,166 --> 00:12:10,500
来做优化

383
00:12:10,533 --> 00:12:12,400
那现在看一下它里面的一些图

384
00:12:12,400 --> 00:12:14,700
同样一开始就介绍了它效果很好

385
00:12:14,733 --> 00:12:15,500
那最核心

386
00:12:15,500 --> 00:12:19,033
就是 NSA 的这一个图片的概念

387
00:12:19,033 --> 00:12:20,400
刚才讲到的三个概念

388
00:12:20,400 --> 00:12:22,666
第一个就是 compressed 压缩

389
00:12:22,766 --> 00:12:25,466
把一堆的向量的特征

390
00:12:25,500 --> 00:12:28,800
压缩到一个具体的编码里面

391
00:12:28,800 --> 00:12:31,600
第二个就是 top n 的一个选择

392
00:12:31,600 --> 00:12:34,366
说实话不是所有的特征都有效

393
00:12:34,366 --> 00:12:35,966
所以在整个 Transformer 架构里

394
00:12:35,966 --> 00:12:38,900
面做了一个选择性的计算

395
00:12:38,933 --> 00:12:40,700
把最核心的一些计算

396
00:12:40,700 --> 00:12:43,300
或者最核心的一些内容特征

397
00:12:43,300 --> 00:12:45,566
就抽取出来进行一个计算

398
00:12:45,633 --> 00:12:47,366
最后就 sliding Windows 了

399
00:12:47,366 --> 00:12:48,166
就 sliding window

400
00:12:48,166 --> 00:12:50,300
就大家都知道了滑窗

401
00:12:50,333 --> 00:12:52,133
那滑窗这种方式还蛮有效

402
00:12:52,133 --> 00:12:54,366
就是当前的注意力

403
00:12:54,366 --> 00:12:57,866
只集中在当前的一些小框框里面

404
00:12:57,900 --> 00:13:00,733
最后把这三个 Compass 跟 selection

405
00:13:00,800 --> 00:13:02,800
跟 sliding window 相关的 attention

406
00:13:02,800 --> 00:13:04,500
抽取之后的特征

407
00:13:04,633 --> 00:13:06,633
进一步的给到 Transformer

408
00:13:06,633 --> 00:13:08,566
来进行一个计算

409
00:13:08,566 --> 00:13:09,433
那通过这种方式

410
00:13:09,433 --> 00:13:11,166
把三种特征 concat 到一起

411
00:13:11,166 --> 00:13:12,466
最后输出

412
00:13:12,500 --> 00:13:15,200
那这个就是这篇文章的核心思路

413
00:13:15,200 --> 00:13:16,000
后面的内容

414
00:13:16,000 --> 00:13:18,266
更多是指它的一个数学的原理

415
00:13:18,300 --> 00:13:19,700
介绍完数学原理之后

416
00:13:19,700 --> 00:13:22,333
就看一下硬件到底是怎么去实现

417
00:13:22,333 --> 00:13:24,900
特别是在 HBM 里面做分块

418
00:13:24,900 --> 00:13:28,533
在 L2Cache SRAM 里面怎么做切片

419
00:13:28,533 --> 00:13:30,033
怎么做循环

420
00:13:30,033 --> 00:13:30,566
那这个

421
00:13:30,566 --> 00:13:33,233
就有兴趣可以看回 ZOMI 之前分享

422
00:13:33,233 --> 00:13:35,966
矩阵层里面的编译器里面的原理

423
00:13:36,000 --> 00:13:38,233
基本上在做自己手写算子

424
00:13:38,233 --> 00:13:39,600
都会把它分块

425
00:13:39,600 --> 00:13:41,100
因为整个矩阵太大了

426
00:13:41,133 --> 00:13:43,033
但是计算核是很小

427
00:13:43,033 --> 00:13:45,066
所以需要把矩阵分片

428
00:13:45,200 --> 00:13:47,300
从 HBM 搬到 L2 的 case

429
00:13:47,333 --> 00:13:50,100
再搬到 L1 的 case 这种方式进行处理

430
00:13:50,100 --> 00:13:51,900
最后就是一些实验了

431
00:13:51,933 --> 00:13:53,633
那不管怎么样

432
00:13:54,133 --> 00:13:56,900
还是回到整个文章里面

433
00:13:56,900 --> 00:13:59,533
同样的其实这节视频非常的水

434
00:13:59,533 --> 00:14:00,600
我觉得没有必要看

435
00:14:00,600 --> 00:14:03,266
ZOMI 只是做一个简单的引子

436
00:14:03,266 --> 00:14:04,266
那简单的可以看一下

437
00:14:04,300 --> 00:14:07,333
核心思路就是设计一种原生可训练

438
00:14:07,333 --> 00:14:09,100
稀疏的架构 NSA

439
00:14:09,100 --> 00:14:10,633
通过层级化

440
00:14:10,633 --> 00:14:13,066
所谓的层级化就这是其中第一层了

441
00:14:13,100 --> 00:14:13,633
第二层了

442
00:14:13,633 --> 00:14:14,566
第三层了

443
00:14:14,566 --> 00:14:16,666
层级化的 token 的建模

444
00:14:16,700 --> 00:14:18,366
和对硬件对齐的优化

445
00:14:18,366 --> 00:14:21,333
实现长上下文本的依赖

446
00:14:21,366 --> 00:14:24,300
那简单的看一下里面的这个内容

447
00:14:24,300 --> 00:14:25,766
整体的灵感来源

448
00:14:25,766 --> 00:14:26,666
主要是 attention score

449
00:14:26,700 --> 00:14:28,700
存在内在的稀疏性

450
00:14:28,700 --> 00:14:31,300
其实真正的为什么 GQA 或者 MLA

451
00:14:31,300 --> 00:14:33,833
上一期视频分享的那些有效

452
00:14:33,833 --> 00:14:35,833
是因为只有少量的 KV 对

453
00:14:35,933 --> 00:14:37,433
是真正的有用

454
00:14:37,433 --> 00:14:40,166
不是所有的 KV 对都是有用

455
00:14:40,166 --> 00:14:41,100
所以 attention 

456
00:14:41,133 --> 00:14:44,100
实际上内部是充满了稀疏性

457
00:14:44,100 --> 00:14:44,566
那于是

458
00:14:44,566 --> 00:14:46,700
就可以利用现代 GPU 的特点

459
00:14:46,733 --> 00:14:48,566
跟 flash attention 的设计原理

460
00:14:48,566 --> 00:14:49,833
实现一个高效

461
00:14:49,833 --> 00:14:52,666
跟计算的一个仿存的实现

462
00:14:52,700 --> 00:14:54,133
那刚才讲到了两个创新点

463
00:14:54,133 --> 00:14:55,533
一个是 Hardware 对齐

464
00:14:55,533 --> 00:14:57,300
一个是训练的时候的感知

465
00:14:57,533 --> 00:14:58,966
两个点对点

466
00:14:58,966 --> 00:15:00,433
那这两个内容

467
00:15:00,433 --> 00:15:04,100
都是通过强工程实现去做

468
00:15:05,166 --> 00:15:06,700
这篇文章最核心的思路

469
00:15:06,733 --> 00:15:08,166
还是来于这个图案

470
00:15:08,333 --> 00:15:10,833
把三路的注意力了的特征

471
00:15:10,833 --> 00:15:11,833
进行了一个融合

472
00:15:11,833 --> 00:15:14,000
这是第一路的注意力特征

473
00:15:14,000 --> 00:15:15,966
这是第二路的注意力特征

474
00:15:15,966 --> 00:15:18,600
这是第三路的注意力特征

475
00:15:18,833 --> 00:15:20,033
三路径的融合

476
00:15:20,033 --> 00:15:23,466
再通过 gate 网络的动态加权

477
00:15:23,500 --> 00:15:26,100
避免整个模型因为局部的信息

478
00:15:26,100 --> 00:15:28,200
而忽略掉全局的信息

479
00:15:28,366 --> 00:15:30,433
那简单的看一下 Compass

480
00:15:30,433 --> 00:15:33,466
主要是将长序列划分成为块

481
00:15:34,000 --> 00:15:37,300
按块进行一个优化和压缩

482
00:15:37,333 --> 00:15:38,366
所以这里面可以看到

483
00:15:38,366 --> 00:15:39,900
它有很多个 block

484
00:15:40,000 --> 00:15:41,766
把整个长序列分成很多个 block

485
00:15:41,766 --> 00:15:43,166
进行一个压缩

486
00:15:43,366 --> 00:15:45,066
第二个就是选择

487
00:15:45,100 --> 00:15:47,833
对于发划分完之后的一些 block

488
00:15:48,000 --> 00:15:49,700
选择它们的重要性

489
00:15:49,733 --> 00:15:51,566
可能中间只有两个 block

490
00:15:51,566 --> 00:15:54,000
也就对应的这一个 block 和这一个 block

491
00:15:54,000 --> 00:15:55,366
重要性是比较高

492
00:15:55,366 --> 00:15:57,433
还原你整个细粒度了

493
00:15:57,433 --> 00:15:59,566
第三个就是 sliding Windows 了

494
00:15:59,566 --> 00:16:02,000
sliding Windows 就是把滑窗

495
00:16:02,000 --> 00:16:03,600
固定在附近的 token

496
00:16:03,600 --> 00:16:06,100
保证上下文的一个连贯性

497
00:16:07,133 --> 00:16:08,900
那逐个的去打开

498
00:16:08,900 --> 00:16:10,300
基本上就会发现

499
00:16:10,300 --> 00:16:12,800
第一步就是一个 compression

500
00:16:12,800 --> 00:16:16,166
压缩将文本划分成为多个语义

501
00:16:16,166 --> 00:16:19,033
那这里面假设是 512 乘 512 作为一个块

502
00:16:19,100 --> 00:16:22,833
那每个块就可以学习整个新的特征

503
00:16:22,833 --> 00:16:24,233
那真正的 512 的特征

504
00:16:24,233 --> 00:16:26,033
会压缩成 64 的特征

505
00:16:26,033 --> 00:16:27,800
512 换成 64

506
00:16:27,800 --> 00:16:31,566
当下一个 512 同样的压缩成 64 的特征

507
00:16:31,566 --> 00:16:32,466
通过这种方式

508
00:16:32,500 --> 00:16:34,300
对特征进行压缩

509
00:16:34,300 --> 00:16:35,000
那另外的话

510
00:16:35,000 --> 00:16:38,800
第二路就是对应的 selection 选择

511
00:16:38,800 --> 00:16:40,566
那基于压缩的特征

512
00:16:40,566 --> 00:16:44,000
选择前 10 个最重要的信息块

513
00:16:44,000 --> 00:16:45,633
当然这里面只是例子

514
00:16:45,633 --> 00:16:48,866
有 4 个选择了两块信息最重要

515
00:16:48,900 --> 00:16:51,333
然后进行了一个压缩出来

516
00:16:51,733 --> 00:16:53,733
那现在来到了第三路信息

517
00:16:53,733 --> 00:16:55,300
就是对当前的 token

518
00:16:55,300 --> 00:16:57,700
前后的一个局部性

519
00:16:57,800 --> 00:17:00,233
进行注意力的一个计算

520
00:17:00,233 --> 00:17:02,300
保证上下文的连贯

521
00:17:02,400 --> 00:17:04,500
那处理第 n 个 token 的时候

522
00:17:04,500 --> 00:17:07,933
滑窗就会滑动到 n 减那个 512

523
00:17:07,933 --> 00:17:10,400
和 n 加 512 的一个内容

524
00:17:10,400 --> 00:17:14,200
那这个就是按照 512 进行分块切分

525
00:17:14,333 --> 00:17:14,966
这种方式

526
00:17:14,966 --> 00:17:18,100
就是整个 3 路注意力机制的来源

527
00:17:18,133 --> 00:17:18,966
其实你会发现

528
00:17:18,966 --> 00:17:20,966
这一没有什么太多的创新

529
00:17:20,966 --> 00:17:22,300
基本上把很多的相

530
00:17:22,333 --> 00:17:23,966
关的知识融合进来做了

531
00:17:23,966 --> 00:17:26,800
更重要的就是工程上的实现

532
00:17:26,800 --> 00:17:28,466
发现效果还是很好

533
00:17:28,500 --> 00:17:29,833
对于长上下文

534
00:17:29,833 --> 00:17:31,766
因为序列越来越长

535
00:17:31,766 --> 00:17:34,266
真正有用的意义或者语句

536
00:17:34,300 --> 00:17:36,233
并没有那么的多

537
00:17:37,033 --> 00:17:37,866
那第四个内容

538
00:17:37,900 --> 00:17:39,433
就是 Kimi 的 MoBA 了

539
00:17:39,433 --> 00:17:40,700
那同样的 MoBA

540
00:17:40,733 --> 00:17:41,700
这篇文章

541
00:17:41,700 --> 00:17:43,400
可以重点的去参考一下

542
00:17:43,400 --> 00:17:46,833
ZOMI 之前分享的一个 MoBA 的内容

543
00:17:46,833 --> 00:17:48,633
也是 Kimi 跟 deepseek

544
00:17:48,633 --> 00:17:51,766
同样同时间推出了一个 MoBA 和 NSA

545
00:17:51,766 --> 00:17:54,033
这个事情就证明长序列

546
00:17:54,033 --> 00:17:56,233
对于现在的大模型来说

547
00:17:56,233 --> 00:17:59,366
是一个很重要的问题和很重要的点

548
00:18:00,833 --> 00:18:02,266
那简单做个小结

549
00:18:02,300 --> 00:18:03,500
从思路上来看

550
00:18:03,500 --> 00:18:04,833
MoBA 跟 NSA

551
00:18:04,833 --> 00:18:06,966
其实有一个比较相似的点

552
00:18:06,966 --> 00:18:09,633
就是专门对于稀疏注意力

553
00:18:09,633 --> 00:18:10,633
进行实现

554
00:18:10,633 --> 00:18:13,000
都是通过筛选来进行操作

555
00:18:13,000 --> 00:18:13,833
top n

556
00:18:13,833 --> 00:18:17,566
分块 Compress 都是筛选的过程

557
00:18:17,566 --> 00:18:18,966
但是注意力机制

558
00:18:18,966 --> 00:18:21,766
肯定会用到 q 乘以 k 呀

559
00:18:21,766 --> 00:18:23,033
那两者之间

560
00:18:23,033 --> 00:18:25,000
都会用一个比较小

561
00:18:25,000 --> 00:18:26,300
就比较小的计算

562
00:18:26,333 --> 00:18:28,100
去替换成一个大的计算

563
00:18:28,100 --> 00:18:29,900
所以说先分块

564
00:18:29,900 --> 00:18:30,933
再进行压缩

565
00:18:30,933 --> 00:18:33,833
这是这两篇文章相同之处

566
00:18:34,766 --> 00:18:35,733
那今天的内容

567
00:18:35,733 --> 00:18:37,233
基本上就到这里为止了

568
00:18:37,233 --> 00:18:38,566
简单的总结一下

569
00:18:38,566 --> 00:18:39,966
对于大模型来说

570
00:18:39,966 --> 00:18:42,833
高效可扩展和长上下文

571
00:18:42,833 --> 00:18:45,400
可能是大模型发展很重要的一个趋势

572
00:18:45,400 --> 00:18:48,400
从 2024 年到 2025 年

573
00:18:48,400 --> 00:18:49,200
可以感觉到

574
00:18:49,200 --> 00:18:52,066
大模型的长上下文的作用

575
00:18:52,233 --> 00:18:53,666
变得非常的大

576
00:18:53,700 --> 00:18:55,200
那长上下文来了

577
00:18:55,200 --> 00:18:56,766
第一个需要减少

578
00:18:56,766 --> 00:18:58,966
计算的复杂度

579
00:18:59,033 --> 00:19:00,600
因为整个 Transformer

580
00:19:00,600 --> 00:19:03,100
更重要的就是一个矩阵

581
00:19:03,133 --> 00:19:06,766
乘也是用 QKV 的相乘的计算

582
00:19:06,766 --> 00:19:08,866
那这个计算量还是非常的大

583
00:19:08,900 --> 00:19:12,166
所以说除了刚才的分块压缩以后

584
00:19:12,166 --> 00:19:14,466
业界就提出了 Linear Transformer 和

585
00:19:14,500 --> 00:19:15,900
Linear attention

586
00:19:15,900 --> 00:19:19,300
Mmaba RWKV 这种相关的概念

587
00:19:19,366 --> 00:19:21,033
那减少复杂度以外

588
00:19:21,033 --> 00:19:23,900
第二个就对于长序列的建模

589
00:19:23,966 --> 00:19:26,600
长序列很容易忘记信息

590
00:19:26,900 --> 00:19:27,966
而且长序列

591
00:19:27,966 --> 00:19:29,233
有可能序列

592
00:19:29,233 --> 00:19:32,066
会真正使得 Transformer 架构

593
00:19:32,100 --> 00:19:33,500
信息稀疏

594
00:19:33,500 --> 00:19:36,100
所以需要进一步压缩 KV Cache

595
00:19:36,233 --> 00:19:38,200
压缩 KV Cache 有两种好处

596
00:19:38,200 --> 00:19:39,833
第一种是节省内存

597
00:19:39,833 --> 00:19:40,466
那第二种

598
00:19:40,500 --> 00:19:43,166
就是使得一个模型的信息

599
00:19:43,166 --> 00:19:44,566
更加的集中

600
00:19:44,566 --> 00:19:47,666
更好的对长序列进行处理

601
00:19:47,800 --> 00:19:49,900
第三种就是多模态的扩展了

602
00:19:49,933 --> 00:19:52,400
现在的大语言模型开始慢慢融合了

603
00:19:52,433 --> 00:19:53,900
这种视觉跟语言

604
00:19:53,933 --> 00:19:57,133
还有音频相关的模型的表征的能力

605
00:19:57,133 --> 00:19:59,366
这也是大模型要探索的内容

606
00:19:59,366 --> 00:20:01,600
今年主要是集中在长上下

607
00:20:01,600 --> 00:20:02,766
文本的 longformer 了

608
00:20:02,766 --> 00:20:03,866
Transformer XL 了

609
00:20:03,966 --> 00:20:06,766
MoBA 和 NSA 进行讲解

610
00:20:06,766 --> 00:20:09,300
后面希望大家提供更多的思路

611
00:20:09,333 --> 00:20:10,800
在科研上面走得更远

612
00:20:10,800 --> 00:20:13,266
让大模型越来越智慧

613
00:20:13,300 --> 00:20:14,766
那今天的内容先到这里为止

614
00:20:14,766 --> 00:20:15,366
谢谢各位

615
00:20:15,366 --> 00:20:16,766
拜了个拜

