1
00:00:00,000 --> 00:00:02,800
内容/录制:Z0MI 酱，视频后期/字幕:梁嘉铭

2
00:00:02,800 --> 00:00:03,800
hello 大家好

3
00:00:03,800 --> 00:00:05,033
水太深风太大

4
00:00:05,033 --> 00:00:07,833
我是那个没有实力就少说话的 ZOMI

5
00:00:14,300 --> 00:00:15,733
今天跟大家来分享一下

6
00:00:15,733 --> 00:00:17,800
attention 的一个变种的全解析

7
00:00:17,800 --> 00:00:18,833
也是 ZOMI 简单

8
00:00:18,833 --> 00:00:20,200
给自己做一个回顾

9
00:00:20,300 --> 00:00:22,733
现在已经来到了整个变形金刚

10
00:00:22,733 --> 00:00:25,366
Transformer 架构的第五个小内容

11
00:00:25,366 --> 00:00:27,866
看一下整个 Transformer 核心机制

12
00:00:27,900 --> 00:00:30,433
attention 的相关的变种

13
00:00:30,833 --> 00:00:33,033
那还是简单的看一下

14
00:00:33,033 --> 00:00:33,566
其实

15
00:00:33,566 --> 00:00:36,033
attention 或者整个 transformer 的来源

16
00:00:36,033 --> 00:00:39,200
是因为传统的序列模型有局限性

17
00:00:39,300 --> 00:00:42,033
不能对很好的长序列的进行表征

18
00:00:42,033 --> 00:00:44,066
而且并行计算比较困难

19
00:00:44,166 --> 00:00:45,433
因为这两个问题

20
00:00:45,433 --> 00:00:47,633
于是就提出了一个新的解决方案

21
00:00:47,633 --> 00:00:50,233
就是 attention 的一个核心的思想

22
00:00:50,300 --> 00:00:52,133
通过权重分配

23
00:00:52,166 --> 00:00:53,733
通过概率分布

24
00:00:53,733 --> 00:00:55,433
让一个模型

25
00:00:55,433 --> 00:00:57,966
更加聚焦于关键的信息

26
00:00:57,966 --> 00:00:58,466
那这一节

27
00:00:58,500 --> 00:01:00,400
在上面的一个视频

28
00:01:00,400 --> 00:01:03,200
跟大家简单的一个已经陈述过

29
00:01:03,200 --> 00:01:05,866
现在来到了这期视频的一个核心

30
00:01:05,900 --> 00:01:08,200
看一下 attention 的挑战和优化的方案

31
00:01:08,200 --> 00:01:10,533
的挑战其实刚才已经讲

32
00:01:10,533 --> 00:01:13,233
传统的 RNN 的挑战

33
00:01:13,233 --> 00:01:14,633
就是 RNN

34
00:01:14,633 --> 00:01:16,666
然后有了 attention

35
00:01:16,700 --> 00:01:17,200
那现在

36
00:01:17,200 --> 00:01:19,900
看一下 attention 的一个挑战

37
00:01:19,933 --> 00:01:20,333
然后

38
00:01:20,333 --> 00:01:22,966
看一下 attention 的最新的一些架构

39
00:01:23,000 --> 00:01:23,733
那现在

40
00:01:23,733 --> 00:01:25,533
很多的大模型

41
00:01:25,533 --> 00:01:27,633
已经开始不用 MHA

42
00:01:27,633 --> 00:01:29,466
也就是 Multi head attention

43
00:01:29,500 --> 00:01:33,433
反倒转过来用那个 MQA 或者 GQA

44
00:01:33,433 --> 00:01:36,266
那用的更多的可能是 GQA 这个方案

45
00:01:36,300 --> 00:01:37,166
当然了幻方

46
00:01:37,166 --> 00:01:38,033
为什么这么火

47
00:01:38,033 --> 00:01:40,000
其实幻方用了一个 MLA

48
00:01:40,000 --> 00:01:41,800
把价格性能都打下来

49
00:01:41,800 --> 00:01:44,300
所以说现在最新的 transformer 架构

50
00:01:44,533 --> 00:01:45,533
或者 attention 的架构

51
00:01:45,533 --> 00:01:49,366
已经开始慢慢的摒弃传统的 MHA

52
00:01:49,366 --> 00:01:50,200
所以今天

53
00:01:50,200 --> 00:01:51,166
来看一下

54
00:01:51,333 --> 00:01:53,766
attention 的其他的变种模型

55
00:01:53,766 --> 00:01:54,833
到底是怎么样

56
00:01:57,433 --> 00:01:58,066
话说回来

57
00:01:58,100 --> 00:02:00,233
肯定要了解为什么会有变种

58
00:02:00,233 --> 00:02:02,600
之前要去看一下 attention 的一个挑战

59
00:02:02,600 --> 00:02:04,400
和相关的优化的方案

60
00:02:04,800 --> 00:02:06,666
那比较明确的一个痛点就是

61
00:02:06,700 --> 00:02:09,000
整个 transformer 的一个计算复杂度

62
00:02:09,166 --> 00:02:11,200
是非常的高

63
00:02:11,300 --> 00:02:12,700
整体的计算复杂度

64
00:02:12,700 --> 00:02:15,533
会约束一个序列的长度

65
00:02:15,533 --> 00:02:17,566
因为计算复杂度越来越高

66
00:02:17,566 --> 00:02:19,233
内存的复杂度越来越高

67
00:02:19,233 --> 00:02:20,066
的时候

68
00:02:20,100 --> 00:02:21,433
导致在单机单卡里面

69
00:02:21,433 --> 00:02:23,800
没有办法很好的放进去

70
00:02:24,133 --> 00:02:26,600
甚至在部分的一个并行计算里面

71
00:02:26,600 --> 00:02:28,266
或者用很大的机器里面

72
00:02:28,300 --> 00:02:29,600
都没有办法很好的处理

73
00:02:29,600 --> 00:02:31,766
一个长序列的问题

74
00:02:31,766 --> 00:02:33,766
那这里面比较明显的约束

75
00:02:33,766 --> 00:02:35,800
是内存的瓶颈

76
00:02:35,866 --> 00:02:37,066
那一般情况

77
00:02:37,100 --> 00:02:38,200
大语言模型的推理

78
00:02:38,200 --> 00:02:40,366
都会在 NPU 上面去推理嘛

79
00:02:40,366 --> 00:02:42,433
单张 NPU 的显存是有限

80
00:02:42,433 --> 00:02:42,866
一部分

81
00:02:42,900 --> 00:02:46,900
要放模型的一个权重参数

82
00:02:46,900 --> 00:02:48,166
这是比较明确

83
00:02:48,166 --> 00:02:50,700
包括前后向的一些激活值

84
00:02:50,733 --> 00:02:52,033
那这部分

85
00:02:52,033 --> 00:02:53,366
到底放多少

86
00:02:53,366 --> 00:02:56,200
主要是依赖于模型的大小

87
00:02:56,433 --> 00:02:58,833
那另外一部分就是

88
00:02:58,833 --> 00:03:02,000
最重要的就是用来放模型的 KV Cache

89
00:03:02,000 --> 00:03:03,000
所谓的 KV Cache

90
00:03:03,000 --> 00:03:06,000
在上一节里面给大家讲过

91
00:03:06,133 --> 00:03:08,133
attention 主要分开三个

92
00:03:08,133 --> 00:03:09,900
第一个输入的 query

93
00:03:10,200 --> 00:03:12,166
然后第二个是 key 跟 Value

94
00:03:12,166 --> 00:03:12,900
的值

95
00:03:12,933 --> 00:03:17,233
主要是来源于模型的权重参数而且

96
00:03:17,233 --> 00:03:19,400
可能长度或者它的大小

97
00:03:19,400 --> 00:03:21,400
取决于模型输入的长度

98
00:03:21,400 --> 00:03:23,166
模型输入的长度越长

99
00:03:23,300 --> 00:03:25,733
那 k 跟 v 就会越来越多嘛

100
00:03:25,733 --> 00:03:26,533
所以这一部分

101
00:03:26,533 --> 00:03:29,533
不仅依赖于模型自己的一个大小以外

102
00:03:29,533 --> 00:03:32,000
还依赖于模型的序列的长度

103
00:03:32,166 --> 00:03:35,100
具体可以看回之前面的公式

104
00:03:35,133 --> 00:03:37,033
也就是在整个推理的过程当中

105
00:03:37,033 --> 00:03:38,233
KV Cache

106
00:03:38,233 --> 00:03:40,600
是随着一个数的序列长度

107
00:03:40,600 --> 00:03:43,000
和问答问题的越来越长

108
00:03:43,033 --> 00:03:47,200
那它可能会占了整个显存的主导地位

109
00:03:47,200 --> 00:03:49,066
所以说内存的瓶颈

110
00:03:49,100 --> 00:03:51,400
可能会由于长序列

111
00:03:51,400 --> 00:03:53,366
慢慢的衍生出来

112
00:03:53,366 --> 00:03:54,166
所以这个 n

113
00:03:54,166 --> 00:03:56,500
就是序列的长度

114
00:03:56,900 --> 00:03:58,766
那有了这个痛点之后

115
00:03:58,766 --> 00:04:00,500
看一下最重要现在的目标

116
00:04:00,533 --> 00:04:04,000
就减少 KV Cache 的一个缓存

117
00:04:04,000 --> 00:04:06,166
使得在更少的设备里面

118
00:04:06,166 --> 00:04:08,966
能够去推理更长的上下文

119
00:04:09,100 --> 00:04:11,000
就更长的序列

120
00:04:11,000 --> 00:04:12,966
或者在相同的序列下面

121
00:04:12,966 --> 00:04:14,600
让 batch size 更大

122
00:04:14,600 --> 00:04:15,900
也就是吞吐更大

123
00:04:15,933 --> 00:04:18,433
我大家都是 512 的序列

124
00:04:18,433 --> 00:04:19,833
但是我这台机器

125
00:04:19,833 --> 00:04:22,433
我可以容纳 10 个用户上来

126
00:04:22,433 --> 00:04:24,166
这个吞吐会更大

127
00:04:24,200 --> 00:04:24,800
于是

128
00:04:24,800 --> 00:04:27,300
这一节就围绕着这个思路

129
00:04:27,333 --> 00:04:29,033
怎么去减少 KV Cache

130
00:04:29,166 --> 00:04:30,600
业界提出了 MQA

131
00:04:30,600 --> 00:04:32,566
GQA 还有 MLA

132
00:04:32,633 --> 00:04:33,233
反正

133
00:04:33,233 --> 00:04:36,033
都是围绕着怎么去减少 KV Cache 的同时

134
00:04:36,100 --> 00:04:36,800
尽可能

135
00:04:36,800 --> 00:04:38,066
保证模型

136
00:04:38,100 --> 00:04:40,300
训练的时候的一个效果

137
00:04:40,300 --> 00:04:43,233
所以说正式的来到了第二个内容

138
00:04:43,233 --> 00:04:46,666
看一下最新的 attention 的架构的演进

139
00:04:47,066 --> 00:04:47,666
那这里面

140
00:04:47,700 --> 00:04:49,966
还是说了一堆废话

141
00:04:49,966 --> 00:04:50,700
说白了我

142
00:04:50,733 --> 00:04:53,000
们现在用的更多的是一个

143
00:04:53,000 --> 00:04:54,866
一开始就提出了 MQA

144
00:04:54,900 --> 00:04:56,400
也就 Multi query attention

145
00:04:56,433 --> 00:04:58,900
那这里面就 v 跟 k

146 
00:04:58,933 --> 00:05:02,966
就我有很多个输入查询的一个语句

147
00:05:02,966 --> 00:05:04,233
那 k 跟 v

148
00:05:04,233 --> 00:05:07,400
主要是由一个把它极度的压缩

149
00:05:07,400 --> 00:05:08,033
那另外的话

150
00:05:08,033 --> 00:05:09,466
现在用的最多呀

151
00:05:09,500 --> 00:05:11,400
包括 llama 系列

152
00:05:11,400 --> 00:05:13,033
基本上都会用 GQA

153
00:05:13,100 --> 00:05:16,033
那 GQA 就跟那个 MQA 其实比较类似

154
00:05:16,033 --> 00:05:18,633
把 query 分开好几组

155
00:05:18,766 --> 00:05:21,800
每两组共享一对 k 跟 v

156
00:05:21,800 --> 00:05:24,800
那这种方式那传统的是 MHA

157
00:05:24,800 --> 00:05:27,166
MHA 就是一个 key 跟一个 value

158
00:05:27,166 --> 00:05:29,666
跟一个 quiva 是一一对应

159
00:05:29,700 --> 00:05:30,633
我输一个 s

160
00:05:30,633 --> 00:05:33,633
就有对应的 QKV 三个值

161
00:05:33,633 --> 00:05:37,000
当然了刚才讲到的 MQA 跟 GQA

162
00:05:37,000 --> 00:05:39,366
都是在横向的维度

163
00:05:39,366 --> 00:05:41,866
去把 key 跟 value 值尽可能的共享

164
00:05:42,000 --> 00:05:43,433
那幻方

165
00:05:43,433 --> 00:05:45,200
Deepseek 的 MLA

166
00:05:45,200 --> 00:05:47,566
最主要的就是在纵向

167
00:05:47,566 --> 00:05:50,433
把这两个 key 跟 value 进行一个压缩

168
00:05:50,433 --> 00:05:52,400
压缩到另外一个 later 的 project

169
00:05:52,400 --> 00:05:55,033
也就是一个隐空间变量里面

170
00:05:55,233 --> 00:05:58,033
通过这种方式去减少 KV Cache

171
00:05:58,200 --> 00:05:58,766
那接下来

172
00:05:58,766 --> 00:06:00,500
将会逐个的去跟大家打开

173
00:06:00,533 --> 00:06:01,966
看一下里面有什么秘密

174
00:06:01,966 --> 00:06:02,433
那当然

175
00:06:02,433 --> 00:06:04,500
这节视频的内容可能有点水

176
00:06:04,533 --> 00:06:06,133
你了解到这里之后

177
00:06:06,200 --> 00:06:08,066
我觉得基本上已经结束

178
00:06:08,100 --> 00:06:09,033
这一次视频

179
00:06:09,033 --> 00:06:10,200
没什么太多的内容

180
00:06:10,200 --> 00:06:11,400
后面更多的是展开

181
00:06:11,400 --> 00:06:13,866
里面的一个细节和公式

182
00:06:15,000 --> 00:06:17,133
现在来到了真正的第一个内容

183
00:06:17,133 --> 00:06:18,433
就是 MQA

184
00:06:18,500 --> 00:06:20,733
Multi query attention 这篇论文

185
00:06:20,733 --> 00:06:22,933
来自于 2019 年的谷歌发表

186
00:06:22,933 --> 00:06:25,100
现在来打开这篇论文

187
00:06:25,633 --> 00:06:27,866
那打开了这篇论文蛮有意思

188
00:06:27,900 --> 00:06:30,233
就是这篇论文只有一个作者

189
00:06:30,300 --> 00:06:32,400
然后非常的牛逼

190
00:06:32,433 --> 00:06:34,900
那看到这篇论文的文章的名字

191
00:06:34,933 --> 00:06:37,400
叫做 Fast Transformer Decoding

192
00:06:37,400 --> 00:06:37,766
也就是

193
00:06:37,766 --> 00:06:41,566
它能够很快地进行 transformer 的 decode 部分

194
00:06:41,966 --> 00:06:44,500
那蛮有意思的就是冒号后面是有问

195
00:06:44,633 --> 00:06:46,866
write-Head is All You Need

196
00:06:46,900 --> 00:06:49,700
只需要一个 Head 就可以

197
00:06:49,700 --> 00:06:50,533
写入的 Head

198
00:06:50,566 --> 00:06:52,433
那看一下里面的一个具体的内容

199
00:06:52,433 --> 00:06:52,866
蛮有意思

200
00:06:52,900 --> 00:06:54,733
就是一开始二点一

201
00:06:54,733 --> 00:06:55,700
就证明

202
00:06:55,700 --> 00:06:58,600
DotProductAttention 的一个具体的问题

203
00:06:58,633 --> 00:06:58,966
然后

204
00:06:58,966 --> 00:07:02,233
看一下 multi-head attention 具体的一个计算

205
00:07:02,233 --> 00:07:04,033
那有了相关的计算之后

206
00:07:04,033 --> 00:07:07,166
其实你会发现比较耗时的是在 k 跟 v

207
00:07:07,200 --> 00:07:07,900
k v 里面

208
00:07:07,933 --> 00:07:10,000
会大量的去做累积

209
00:07:10,300 --> 00:07:12,000
于是这里面就提出

210
00:07:12,000 --> 00:07:13,966
这篇文章的最核心的思想

211
00:07:13,966 --> 00:07:17,500
就是 multi-head attention 这个内容

212
00:07:17,533 --> 00:07:18,800
那通过 multi-head attention

213
00:07:18,800 --> 00:07:21,200
最核心的就是共享 query

214
00:07:21,200 --> 00:07:21,966
那这里面 

215
00:07:21,966 --> 00:07:23,866
下面就是一堆的相关

216
00:07:23,900 --> 00:07:27,100
证明了 multi-head attention 的相关的内容

217
00:07:27,200 --> 00:07:28,366
这里面这篇文章

218
00:07:28,366 --> 00:07:30,666
ZOMI 觉得还是非常的有意思

219
00:07:30,700 --> 00:07:32,900
因为都是大量的通过这种

220
00:07:32,900 --> 00:07:34,166
伪代码的计算

221
00:07:34,166 --> 00:07:35,966
去证明了我这个公式

222
00:07:35,966 --> 00:07:37,966
或者我这个思想是非常的 OK

223
00:07:37,966 --> 00:07:39,900
然后效率也是非常的好

224
00:07:39,933 --> 00:07:42,966
而且还不怎么影响模型的精度

225
00:07:42,966 --> 00:07:44,466
那引用的文章也是比较少

226
00:07:44,500 --> 00:07:45,233
所以这篇文章

227
00:07:45,233 --> 00:07:46,633
还是非常的有意思

228
00:07:46,633 --> 00:07:49,000
相关的同学们也可以重点去看看

229
00:07:49,066 --> 00:07:49,833
那不管怎么样

230
00:07:49,833 --> 00:07:50,966
简单的回到

231
00:07:50,966 --> 00:07:53,466
整一篇文章的一个最重要的思想

232
00:07:53,500 --> 00:07:57,500
就是每个 head 去计算 attention 的时候

233
00:07:57,800 --> 00:08:00,700
每个 head 的 KV 都是共享

234
00:08:00,733 --> 00:08:02,700
所以看一下右边的这个图

235
00:08:02,700 --> 00:08:04,800
有非常多的 query 输进去

236
00:08:04,800 --> 00:08:07,466
模型里面有很多的查询语句

237
00:08:07,500 --> 00:08:11,200
但是里面的 k 跟 v 是非常的少

238
00:08:11,200 --> 00:08:15,200
所以看一下右边的这条公式 QTS

239
00:08:15,333 --> 00:08:18,400
其实 s 是关于上面的坐标

240
00:08:18,400 --> 00:08:20,633
关于每一个输出

241
00:08:20,633 --> 00:08:22,800
q 都是不一样

242
00:08:22,966 --> 00:08:24,700
o 输出也是不一样

243
00:08:24,733 --> 00:08:27,700
但是 k 跟 v 是相同共享

244
00:08:27,700 --> 00:08:29,833
因此这个 multi-query attention

245
00:08:29,833 --> 00:08:32,000
虽然论文证明了很多

246
00:08:32,000 --> 00:08:32,900
但主要的思想

247
00:08:32,933 --> 00:08:35,100
就看这条公式就可以

248
00:08:35,100 --> 00:08:36,233
那整体看一下

249
00:08:36,233 --> 00:08:37,700
整个核心思想

250
00:08:37,733 --> 00:08:40,100
就是 KV 开始缓存的时候

251
00:08:40,100 --> 00:08:41,933
只缓存一个 head 的 KV

252
00:08:41,933 --> 00:08:43,333
虽然有 Multi head

253
00:08:43,333 --> 00:08:45,000
然后降低了整个

254
00:08:45,000 --> 00:08:47,266
MHA 的 KV Cache 的一个大小

255
00:08:47,300 --> 00:08:49,533
就变成 h 分之一

256
00:08:49,533 --> 00:08:50,800
那优点比较明显

257
00:08:50,800 --> 00:08:52,466
主要是节省显存嘛

258
00:08:52,500 --> 00:08:54,633
然后 KV Cache 降得非常的低

259
00:08:54,633 --> 00:08:57,000
能够减少计算跟通讯的开销

260
00:08:57,000 --> 00:08:59,266
提升长序列的推理的问题

261
00:08:59,333 --> 00:09:01,766
那缺点也会比较明显

262
00:09:01,766 --> 00:09:03,200
这里面所谓的性能下降

263
00:09:03,200 --> 00:09:04,366
不是指效率

264
00:09:04,366 --> 00:09:06,800
而是指模型的效果下降

265
00:09:06,800 --> 00:09:09,800
因为 KV Cache 经过非常大的压缩

266
00:09:09,800 --> 00:09:12,300
变成一个缓存一个 KV

267
00:09:12,333 --> 00:09:13,033
那整体

268
00:09:13,033 --> 00:09:15,833
是影响模型的稳定性和效果

269
00:09:15,833 --> 00:09:17,300
所以说这个提出来之后

270
00:09:17,333 --> 00:09:19,600
用的人并没有那么的多

271
00:09:20,400 --> 00:09:23,033
但是 MQA 是一个非常好的尝试

272
00:09:23,033 --> 00:09:25,033
于是就有了一个 GQA

273
00:09:25,033 --> 00:09:28,066
既然 MQA 压缩的太高

274
00:09:28,100 --> 00:09:29,000
那我是不是可以

275
00:09:29,000 --> 00:09:30,200
放缓一点

276
00:09:30,200 --> 00:09:31,900
于是就出现了 GQA

277
00:09:31,933 --> 00:09:35,933
GQA 是到了 2023 年的时候去引入

278
00:09:35,966 --> 00:09:38,000
也就是 training

279
00:09:38,000 --> 00:09:38,900
题目非常的长

280
00:09:38,933 --> 00:09:40,200
ZOMI 就不再读

281
00:09:40,200 --> 00:09:40,666
那同样

282
00:09:40,700 --> 00:09:42,700
现在打开一下这篇文章

283
00:09:43,233 --> 00:09:43,866
可以看到

284
00:09:43,900 --> 00:09:46,500
这篇文章貌似也是来自于谷歌 research

285
00:09:46,500 --> 00:09:47,433
也就是谷歌

286
00:09:47,433 --> 00:09:49,633
实际上在 AI 人工智能里面

287
00:09:49,633 --> 00:09:54,033
做了很多基础性的研究和基础的工作

288
00:09:54,033 --> 00:09:55,766
整体的 GQA

289
00:09:55,766 --> 00:09:58,866
这个 g 是来自于 Generalized

290
00:09:59,166 --> 00:10:02,800
Multi-Query Transformer from Multi-Head Checkpoints

291
00:10:02,966 --> 00:10:03,633
那蛮有意思

292
00:10:03,633 --> 00:10:04,833
这里面从训练的时候

293
00:10:04,833 --> 00:10:07,633
就进行了一个提出通用

294
00:10:07,633 --> 00:10:09,233
Multi-Query 的一个 transformer

295
00:10:09,366 --> 00:10:10,900
然后相关的内容

296
00:10:10,933 --> 00:10:13,800
让整体的看一看这个图

297
00:10:13,866 --> 00:10:14,700
那这篇文章

298
00:10:14,733 --> 00:10:15,166
这个图

299
00:10:15,166 --> 00:10:16,000
就最有意思

300
00:10:16,000 --> 00:10:17,100
就简单看完这个图

301
00:10:17,133 --> 00:10:18,733
你基本上就搞清楚

302
00:10:18,766 --> 00:10:21,500
MQA MHA GQ 有什么意思

303
00:10:21,533 --> 00:10:22,300
那 MQA

304
00:10:22,300 --> 00:10:23,433
就是 q key

305
00:10:23,433 --> 00:10:25,500
value 都是一一对应

306
00:10:25,533 --> 00:10:27,833
那刚才讲到的 MQA

307
00:10:27,833 --> 00:10:29,366
就是一个 query

308
00:10:29,433 --> 00:10:30,166
所有的 query

309
00:10:30,166 --> 00:10:31,700
整个 head 里面

310
00:10:31,733 --> 00:10:33,233
就共享了 k 跟 v

311
00:10:33,366 --> 00:10:33,833
这里面

312
00:10:33,833 --> 00:10:35,166
就是多个 query

313
00:10:35,166 --> 00:10:36,600
我才共享

314
00:10:36,600 --> 00:10:38,066
某几个 k 跟 v

315
00:10:38,100 --> 00:10:39,366
那包括现在的千问

316
00:10:39,366 --> 00:10:40,866
还是 Llama 系列

317
00:10:40,900 --> 00:10:42,533
都主要是用 GQA

318
00:10:42,533 --> 00:10:44,600
这种方式减少了缓存的时候

319
00:10:44,600 --> 00:10:47,233
保证模型的效果更好

320
00:10:47,233 --> 00:10:47,700
那同样

321
00:10:47,733 --> 00:10:49,633
简单的翻完这篇文章

322
00:10:49,633 --> 00:10:52,500
就认证了我的一个训练的时间

323
00:10:52,533 --> 00:10:54,700
模型的效率其实可以保证

324
00:10:54,700 --> 00:10:56,100
然后跟 MHA 一样

325
00:10:56,100 --> 00:10:58,600
比较类似 GQK 的时间更短

326
00:10:58,600 --> 00:10:59,100
那同样

327
00:10:59,133 --> 00:11:00,900
下面有很多相关

328
00:11:00,900 --> 00:11:02,500
一些简单的效用实验

329
00:11:02,500 --> 00:11:03,100
reference 也比

330
00:11:03,100 --> 00:11:05,800
较多是因为是 2023 年的文章

331
00:11:06,033 --> 00:11:08,233
同样的回到整个 PPT 里面

332
00:11:08,233 --> 00:11:09,233
这个 GQA

333
00:11:09,233 --> 00:11:12,200
主要是解决了 k v Cache 过度压缩

334
00:11:12,200 --> 00:11:14,466
对应 MQA 的过度压缩

335
00:11:14,500 --> 00:11:18,933
将 k v 两个 head 分为 g 组

336
00:11:18,933 --> 00:11:21,933
每组就共享 KV 值

337
00:11:21,933 --> 00:11:24,800
那这里面就一组两组 3 组 4 组

338
00:11:24,800 --> 00:11:26,800
这里面一共有 4 组值

339
00:11:26,800 --> 00:11:28,766
所以 g 就等于 4

340
00:11:28,766 --> 00:11:31,166
那通过这种方式来去实现

341
00:11:31,166 --> 00:11:33,633
一个整体的压缩

342
00:11:33,633 --> 00:11:34,266
那同样

343
00:11:34,300 --> 00:11:35,000
比较有意思的就

344
00:11:35,000 --> 00:11:37,166
看一下这一个公式

345
00:11:37,166 --> 00:11:39,300
公式还是 attention 的公式

346
00:11:39,333 --> 00:11:42,800
但是区别就在于 SG 除以 h

347
00:11:42,800 --> 00:11:44,666
h 就是 head 数量

348
00:11:44,700 --> 00:11:45,133
s

349
00:11:45,133 --> 00:11:48,133
对应的就是一个 sequence 的数

350
00:11:48,133 --> 00:11:48,766
然后 g

351
00:11:48,766 --> 00:11:51,366
就是对应一个多少组

352
00:11:51,366 --> 00:11:52,666
这个 KV head

353
00:11:52,666 --> 00:11:54,500
所以说这里面可以看到 q

354
00:11:54,533 --> 00:11:57,133
一共有 s 序列长度

355
00:11:57,133 --> 00:11:58,566
然后 sg 除以 h

356
00:11:58,566 --> 00:12:01,500
就是 k 跟 v 共享的一个内容

357
00:12:01,533 --> 00:12:02,000
最后

358
00:12:02,000 --> 00:12:04,633
输出还是有 s 个序列的长度

359
00:12:04,633 --> 00:12:05,300
所以说

360
00:12:05,333 --> 00:12:06,233
实际上

361
00:12:06,233 --> 00:12:08,200
我听说有些朋友在研究时序

362
00:12:08,200 --> 00:12:10,633
问题是其实时序里面

363
00:12:10,633 --> 00:12:12,066
也是非常的好找工作

364
00:12:12,100 --> 00:12:13,600
或者非常的吃香

365
00:12:13,600 --> 00:12:14,866
整个 transformer 的架构

366
00:12:14,900 --> 00:12:17,700
主要还是处理时序的内容

367
00:12:17,800 --> 00:12:18,566
那不管怎么样

368
00:12:18,566 --> 00:12:19,666
简单看一下

369
00:12:19,700 --> 00:12:21,833
其实主要是将每一份 KV 值

370
00:12:21,833 --> 00:12:23,000
均分为 G 组

371
00:12:23,000 --> 00:12:25,400
就刚才看到的这个图

372
00:12:25,666 --> 00:12:28,000
每一个 k v 值均分为 g 组

373
00:12:28,000 --> 00:12:30,200
所以对应的就是变成 query

374
00:12:30,200 --> 00:12:31,566
我乘以一个 g 然

375
00:12:31,566 --> 00:12:32,666
后再除以 haed

376
00:12:33,300 --> 00:12:36,333
那对应的每组的 KV 值

377
00:12:36,333 --> 00:12:38,633
就重复了 h 除以 g 次

378
00:12:38,633 --> 00:12:40,100
正好有 h 个 head 嘛

379
00:12:40,133 --> 00:12:43,100
所以一共需要就是 s 乘以 g

380
00:12:43,100 --> 00:12:44,433
然后除以 h 组

381
00:12:44,533 --> 00:12:45,500
那看不懂没关系

382
00:12:45,500 --> 00:12:47,100
大家可以自己去算一算

383
00:12:47,100 --> 00:12:48,800
当 g 等于 h 的时候

384
00:12:48,800 --> 00:12:50,466
那就是 MHA 嘛

385
00:12:50,500 --> 00:12:52,300
Llama2 和 3-70B

386
00:12:52,300 --> 00:12:54,300
使用 GQA 就设置了 g 等于 8

387
00:12:54,300 --> 00:12:56,100
刚好每张卡可以负责计算

388
00:12:56,100 --> 00:12:58,233
一组 KV attention 的值

389
00:12:58,533 --> 00:12:59,200
这种方式

390
00:12:59,200 --> 00:13:01,066
可以保证 KV Cache 的多样性

391
00:13:01,100 --> 00:13:03,766
同时减少卡间的一个通讯效率

392
00:13:03,766 --> 00:13:04,833
所以说后面

393
00:13:04,833 --> 00:13:06,900
将会在最后一个视频

394
00:13:06,933 --> 00:13:08,133
跟大家去了解一下

395
00:13:08,133 --> 00:13:10,166
整个大模型的这些参数

396
00:13:10,166 --> 00:13:11,800
到底是怎么去设置

397
00:13:11,800 --> 00:13:13,800
对大模型有什么影响

398
00:13:13,800 --> 00:13:16,900
那更怎么在 NPU 上面

399
00:13:16,966 --> 00:13:19,500
更好的进行一个配置

400
00:13:19,766 --> 00:13:20,400
那总体来说

401
00:13:20,400 --> 00:13:22,466
总结一下 GQA 的优点

402
00:13:22,500 --> 00:13:23,133
就是

403
00:13:23,133 --> 00:13:25,966
平衡了一个性能跟效率之间的关系

404
00:13:26,033 --> 00:13:28,166
保证 k 跟 v 有的多样性的同时

405
00:13:28,166 --> 00:13:30,366
减少 KV Cache 的一个大小

406
00:13:30,366 --> 00:13:33,066
而且稳定性比 MQA 更好

407
00:13:33,100 --> 00:13:34,533
训练过程比较稳定

408
00:13:34,533 --> 00:13:35,200
那缺点

409
00:13:35,200 --> 00:13:38,000
就是人为的去设置合理的组数

410
00:13:38,000 --> 00:13:40,266
这个 g 这个多了一个超参

411
00:13:40,300 --> 00:13:42,300
本来大模型的参数已经够多

412
00:13:42,300 --> 00:13:43,333
随便改个参数

413
00:13:43,333 --> 00:13:46,033
有可能导致模型的训练的不稳定性

414
00:13:46,033 --> 00:13:48,966
当然也可以参考 Llama 的方式

415
00:13:48,966 --> 00:13:54,033
就是把 GQA 设置成 8 减少一个卡间通讯 

416
00:13:55,533 --> 00:13:56,733
那 ZOMI 能预料到

417
00:13:56,733 --> 00:13:58,700
其实看这期视频的人比较少

418
00:13:58,700 --> 00:13:59,800
因为整个 transformer

419
00:13:59,800 --> 00:14:02,000
网上已经很多人去分享

420
00:14:02,000 --> 00:14:04,333
ZOMI 主要也是因为哎

421
00:14:04,366 --> 00:14:07,300
需要去分享整个 AI 系统相关的知识

422
00:14:07,300 --> 00:14:08,700
里面刚好

423
00:14:08,700 --> 00:14:11,433
这里面有了一个大模型的算法

424
00:14:11,433 --> 00:14:12,033
所以

425
00:14:12,033 --> 00:14:14,033
就去回顾了一下相关的内容

426
00:14:14,033 --> 00:14:16,433
其实大家可以关心整个 AI 系统里面

427
00:14:16,433 --> 00:14:18,066
ZOMI 大模型系统里面

428
00:14:18,100 --> 00:14:20,833
ZOMI 还有很多东西去跟大家去分享

429
00:14:20,833 --> 00:14:21,700
包括互联

430
00:14:21,733 --> 00:14:22,966
存储集群

431
00:14:22,966 --> 00:14:24,366
容器化部署

432
00:14:24,366 --> 00:14:25,600
包括分布式训练

433
00:14:25,600 --> 00:14:27,566
分布式推理相关的内容

434
00:14:27,566 --> 00:14:28,166
但这里面

435
00:14:28,166 --> 00:14:29,466
也会分享一些模型

436
00:14:29,500 --> 00:14:31,333
跟算法相关的事情

437
00:14:31,333 --> 00:14:34,233
所以说才会有了这一期

438
00:14:34,233 --> 00:14:35,500
虽然比较粗浅

439
00:14:35,533 --> 00:14:38,100
但是也做个简单回顾的视频

440
00:14:43,033 --> 00:14:44,800
那现在来到了第五个内容

441
00:14:44,800 --> 00:14:46,600
看一下 MLA

442
00:14:46,600 --> 00:14:48,300
那 MLA 的名字

443
00:14:48,333 --> 00:14:51,133
其实叫做 multi-head latent attention

444
00:14:51,200 --> 00:14:54,566
主要是来源于幻方 DeepSeek v2

445
00:14:54,566 --> 00:14:56,033
这篇文章

446
00:14:56,033 --> 00:14:56,833
那同样

447
00:14:56,833 --> 00:14:59,900
简单的打开这一篇文章去回顾一下

448
00:15:00,000 --> 00:15:01,833
那么现在打开来这篇文章

449
00:15:01,833 --> 00:15:02,233
蛮有意思

450
00:15:02,233 --> 00:15:05,066
就是 DeepSeek 自己的实验室

451
00:15:05,100 --> 00:15:06,400
就发表了这篇文章

452
00:15:06,400 --> 00:15:07,866
说它的一个更加 strong

453
00:15:07,900 --> 00:15:11,966
更加 economy efficient mixture of expert language model

454
00:15:11,966 --> 00:15:13,800
当然了这篇文章的重要性

455
00:15:13,800 --> 00:15:17,066
主要是介绍整个 deepseek V2 的模型结构

456
00:15:17,100 --> 00:15:18,933
然后整个 MLA

457
00:15:18,933 --> 00:15:21,900
是里面的其中一个思路之一

458
00:15:21,900 --> 00:15:23,333
MLA 的出现

459
00:15:23,333 --> 00:15:25,900
减少了一个训练的时间和代价

460
00:15:25,900 --> 00:15:29,333
还有 KV Cache 的缓存减少了 93.3%

461
00:15:29,333 --> 00:15:30,533
非常的夸张

462
00:15:30,533 --> 00:15:31,333
吞吐的时间

463
00:15:31,333 --> 00:15:32,533
也变得非常的大

464
00:15:32,533 --> 00:15:33,800
那有兴趣的小伙伴们

465
00:15:33,800 --> 00:15:36,466
也可以看回 ZOMI 之前分享

466
00:15:36,566 --> 00:15:40,233
MOE 关于 DeepSeek V2 相关的文章

467
00:15:40,233 --> 00:15:40,633
这里面

468
00:15:40,633 --> 00:15:43,666
因为在之前分享 DeepSeek MOE 的时候

469
00:15:43,700 --> 00:15:44,700
已经有了相关的内容

470
00:15:44,700 --> 00:15:46,766
所以简单的做一个

471
00:15:46,766 --> 00:15:47,833
走读一下论文

472
00:15:47,833 --> 00:15:48,633
那论文里面

473
00:15:48,633 --> 00:15:49,600
最核心的就是

474
00:15:49,600 --> 00:15:52,400
在整个 transformer 架构里面的 attention

475
00:15:53,166 --> 00:15:53,833
有两个

476
00:15:53,833 --> 00:15:57,433
deepseek v2 主要是提出了一个共享专家

477
00:15:57,433 --> 00:15:58,500
所以有上面这条内容

478
00:15:58,533 --> 00:15:59,533
但是今天

479
00:15:59,533 --> 00:16:02,566
主要是关心 attention 这块的一个改变

480
00:16:02,566 --> 00:16:05,033
提出了一个 multi-head latent attention

481
00:16:05,033 --> 00:16:06,566
所以叫做 MLA

482
00:16:06,800 --> 00:16:07,400
那这里面

483
00:16:07,400 --> 00:16:09,800
ZOMI 就会详细的跟大家展开

484
00:16:09,800 --> 00:16:12,100
这一个部分到底是怎么去计算

485
00:16:12,133 --> 00:16:13,766
在后面的内容 ppt

486
00:16:13,833 --> 00:16:16,433
那简单的看完整个这篇文章

487
00:16:16,433 --> 00:16:17,400
这里面就讲到

488
00:16:17,400 --> 00:16:19,000
MHA 的一个具体的问题

489
00:16:19,000 --> 00:16:20,766
MHA 不好 GQA 也不好

490
00:16:20,766 --> 00:16:22,000
还有 MQA 也不好

491
00:16:22,000 --> 00:16:24,000
所以要提出 MLA

492
00:16:24,000 --> 00:16:24,566
这部分

493
00:16:24,566 --> 00:16:27,366
MLA 就主要是来自于这个内容

494
00:16:27,366 --> 00:16:29,066
后面会详细的去展开

495
00:16:29,100 --> 00:16:31,500
主要是对一个 latent

496
00:16:31,500 --> 00:16:34,133
进行一个低秩的压缩

497
00:16:34,133 --> 00:16:36,033
相关的后面都是举证

498
00:16:36,033 --> 00:16:37,633
然后模型的效果

499
00:16:37,633 --> 00:16:40,433
证明了幻方 Deepseek V2 这个模型非常的好

500
00:16:40,433 --> 00:16:41,366
还有很多引用

501
00:16:41,366 --> 00:16:42,466
当然了这里面

502
00:16:42,533 --> 00:16:44,800
是因为是整个幻方

503
00:16:44,800 --> 00:16:45,766
所有的同事

504
00:16:45,766 --> 00:16:46,866
进行一个努力

505
00:16:46,900 --> 00:16:47,566
所以这里面

506
00:16:47,566 --> 00:16:48,766
就出现了一个 appendix

507
00:16:48,766 --> 00:16:50,400
然后感谢很多的人

508
00:16:50,400 --> 00:16:51,166
如果想挖人的话

509
00:16:51,166 --> 00:16:53,266
也可以重点针对这文章的内容

510
00:16:53,566 --> 00:16:55,233
人物去挖一挖

511
00:16:55,233 --> 00:16:56,000
那不管怎么样

512
00:16:56,000 --> 00:16:58,233
还是处于学习的心态

513
00:16:58,233 --> 00:16:59,800
去看这篇文章

514
00:16:59,800 --> 00:17:00,300
蛮有意思

515
00:17:00,333 --> 00:17:02,500
就是后面的实验大部分都是中文

516
00:17:02,500 --> 00:17:03,033
所以证明

517
00:17:03,033 --> 00:17:03,900
他训练的时候

518
00:17:03,933 --> 00:17:06,400
用了很多中文的语料

519
00:17:06,700 --> 00:17:07,533
那不管怎么样

520
00:17:07,533 --> 00:17:09,233
这里面分享的内容

521
00:17:09,233 --> 00:17:11,866
在整篇文章里面只是其中的一部分

522
00:17:12,000 --> 00:17:14,400
回到整个 MLA 的内容

523
00:17:14,400 --> 00:17:16,100
MLA 将潜在的特征

524
00:17:16,133 --> 00:17:18,300
纳入了注意力机制范围内

525
00:17:18,300 --> 00:17:20,400
去降低计算复杂度

526
00:17:20,500 --> 00:17:21,600
那核心的思想

527
00:17:21,600 --> 00:17:25,033
是对 k 跟 v 哦进行一个压缩后

528
00:17:25,033 --> 00:17:28,666
送给 MHA 进行一个计算

529
00:17:28,700 --> 00:17:32,300
所以说实质的本质它的计算还是 MHA

530
00:17:32,366 --> 00:17:34,666
更重要的是它在计算之前

531
00:17:34,800 --> 00:17:38,100
用更短的 KV 向量来进行表征

532
00:17:38,133 --> 00:17:41,300
从而减少 KV Cache 的一个大小

533
00:17:41,633 --> 00:17:42,200
那下面

534
00:17:42,200 --> 00:17:45,700
重点的每一个内容去展开

535
00:17:45,733 --> 00:17:48,433
最主要是通过低秩的一个矩阵

536
00:17:48,433 --> 00:17:51,833
将输入 X i 映射成为 CI

537
00:17:51,833 --> 00:17:52,900
那这里面

538
00:17:52,933 --> 00:17:53,700
c

539
00:17:53,700 --> 00:17:57,133
的维度会比 X 一开始输入的维度更少

540
00:17:57,133 --> 00:17:58,933
于是原始的 KV Cache 的缓存

541
00:17:58,933 --> 00:18:03,766
每个 head 的 KI 跟 VI 变成了 CI

542
00:18:03,966 --> 00:18:04,566
那这里面

543
00:18:04,566 --> 00:18:06,600
简单的看一下这个图

544
00:18:06,600 --> 00:18:09,000
图还是非常的明显

545
00:18:09,000 --> 00:18:10,366
就是我一开始输进去

546
00:18:10,366 --> 00:18:11,966
这个 input hidden

547
00:18:11,966 --> 00:18:14,966
就是 x 的一个维度非常的长

548
00:18:15,166 --> 00:18:15,766
那最后

549
00:18:15,766 --> 00:18:18,033
输给一个 Multi attention 的时候

550
00:18:18,033 --> 00:18:20,100
就变成它的维度是很少

551
00:18:20,133 --> 00:18:23,200
不管是 q 跟 k 的维度也是很少

552
00:18:23,200 --> 00:18:25,066
就是只有 3 个圈圈

553
00:18:25,100 --> 00:18:26,333
那有 3 个圈圈

554
00:18:26,333 --> 00:18:27,333
不管 k 跟 v

555
00:18:27,333 --> 00:18:29,833
那整体的 input 的一个维度

556
00:18:29,833 --> 00:18:31,900
一共有非常多个圈圈

557
00:18:31,933 --> 00:18:32,900
所以通过圈圈

558
00:18:32,900 --> 00:18:33,533
就知道

559
00:18:33,533 --> 00:18:37,033
它的一个特征的维度是极度的压缩

560
00:18:38,133 --> 00:18:38,633
接下来

561
00:18:38,633 --> 00:18:41,666
将每一步的打开它的一个计算哦

562
00:18:41,700 --> 00:18:42,500
那首先

563
00:18:42,500 --> 00:18:43,566
还是要看一下

564
00:18:43,566 --> 00:18:45,633
它的一个具体的表达

565
00:18:45,633 --> 00:18:47,433
看不懂没关系因为这期内容

566
00:18:47,433 --> 00:18:50,100
主要是理解刚才它的一个维度的压缩

567
00:18:50,133 --> 00:18:50,900
就好

568
00:18:50,900 --> 00:18:53,900
CTKV 主要是在时间步 t 里面

569
00:18:53,900 --> 00:18:54,500
去计算

570
00:18:54,500 --> 00:18:57,533
key 跟 value 的一个缓存的向量

571
00:18:57,600 --> 00:19:01,233
那 wDKV 是一个权重矩阵

572
00:19:01,233 --> 00:19:04,100
用于将隐藏的空间 ht

573
00:19:04,200 --> 00:19:06,600
映射到一个这个空间里面

574
00:19:06,600 --> 00:19:07,200
那这一步

575
00:19:07,200 --> 00:19:08,900
主要是通过神经网络的矩阵

576
00:19:08,933 --> 00:19:11,600
点乘来去得到

577
00:19:11,600 --> 00:19:13,200
那看一下右边的这个图

578
00:19:13,200 --> 00:19:16,300
说白了就是我一开始输进去的一个 x

579
00:19:16,333 --> 00:19:18,233
那这里面是用 ht 来代表

580
00:19:18,233 --> 00:19:19,666
那输进去的时候

581
00:19:19,700 --> 00:19:20,800
通过简单的矩阵乘

582
00:19:20,800 --> 00:19:23,400
或者一个特征向量 WDKV

583
00:19:23,433 --> 00:19:25,300
然后压缩成为 CTKV

584
00:19:25,333 --> 00:19:26,433
也就变少

585
00:19:26,433 --> 00:19:28,833
变成只有 4 个圈圈

586
00:19:28,833 --> 00:19:31,633
通过一开始的数变成很少的圈圈

587
00:19:31,766 --> 00:19:33,400
接着第二步

588
00:19:33,400 --> 00:19:36,766
去算 k 跟 v 的值

589
00:19:37,100 --> 00:19:39,766
同样的是输入的变成了 CTKV

590
00:19:39,766 --> 00:19:42,500
而不是输入变成 ST 了或者 ht

591
00:19:42,566 --> 00:19:43,433
那这里面

592
00:19:43,433 --> 00:19:46,666
又引入了一个权重矩阵的 WUK

593
00:19:46,700 --> 00:19:48,966
同样的 WUK 只是做一个映射

594
00:19:48,966 --> 00:19:51,833
把 CTKV 映射成为 KCT

595
00:19:52,000 --> 00:19:57,166
然后把 CT 的 KV 又同样的映射成为 VTC

596
00:19:57,200 --> 00:19:59,266
那通过这种方式进行压缩

597
00:19:59,300 --> 00:20:01,033
同样的看不懂左边的公式

598
00:20:01,033 --> 00:20:01,466
没关系

599
00:20:01,500 --> 00:20:03,133
看一下右边的这个图

600
00:20:03,133 --> 00:20:05,766
本来这里面有 4 个圈圈

601
00:20:05,766 --> 00:20:08,833
现在我同样的有两个矩阵

602
00:20:08,833 --> 00:20:12,200
就 WUK 通过这个矩阵

603
00:20:12,200 --> 00:20:16,033
把 4 个圈圈压缩成为两个圈圈

604
00:20:16,033 --> 00:20:17,833
那 k 跟 v 都是两个圈圈

605
00:20:17,833 --> 00:20:20,466
就意味着模型的维度哦

606
00:20:20,500 --> 00:20:21,366
就变小

607
00:20:21,366 --> 00:20:24,466
从 4 个圈圈变成两个圈圈那

608
00:20:24,500 --> 00:20:26,966
接着看一下第三

609
00:20:26,966 --> 00:20:29,800
因为刚才在之前的两个视频里面

610
00:20:29,800 --> 00:20:30,633
也会讲到

611
00:20:30,633 --> 00:20:34,200
为了保证一个模型的位置编码

612
00:20:34,200 --> 00:20:35,233
和位置的信息

613
00:20:35,233 --> 00:20:37,366
特别是 attention 长序列信息

614
00:20:37,366 --> 00:20:41,200
需要把一个就是位置编码的信息

615
00:20:41,200 --> 00:20:43,500
同样的给到模型

616
00:20:44,400 --> 00:20:46,766
这个是绝对位置编码去提出来

617
00:20:46,766 --> 00:20:48,600
同样这里面的位置编码

618
00:20:48,600 --> 00:20:50,433
用了 ROPE 的位置编码

619
00:20:50,433 --> 00:20:52,633
把一开始的输入的 ht

620
00:20:52,633 --> 00:20:55,100
也是对应 ST 很长的序列

621
00:20:55,133 --> 00:20:56,933
压缩成为一个圈圈

622
00:20:56,933 --> 00:20:59,000
但这里面可以看到 KTR

623
00:20:59,033 --> 00:21:02,233
同样的通过 ROPE 进行压缩

624
00:21:02,233 --> 00:21:04,500
那这个压缩的投影的矩阵

625
00:21:04,533 --> 00:21:06,800
就压缩的矩阵就变成 WKV

626
00:21:06,800 --> 00:21:09,033
所以这里面一开始输的是 ht

627
00:21:09,100 --> 00:21:12,566
通过 WKR 这么一个矩阵

628
00:21:12,566 --> 00:21:14,066
进行维度的压缩

629
00:21:14,100 --> 00:21:15,933
然后给到 ROPE 进行计算

630
00:21:15,933 --> 00:21:19,300
得到只有一个圈圈的 kTR

631
00:21:19,300 --> 00:21:21,800
就是变成一个圈圈的一个压缩的维度

632
00:21:22,133 --> 00:21:24,733
那有了 KV 值

633
00:21:24,733 --> 00:21:27,200
跟一个所谓的位置编码

634
00:21:27,200 --> 00:21:28,500
那来到了 STEP4

635
00:21:28,533 --> 00:21:29,400
第四部分

636
00:21:29,433 --> 00:21:31,833
组合潜在向量 KV 跟位置编码

637
00:21:31,833 --> 00:21:34,300
得到最终的向量的值

638
00:21:34,400 --> 00:21:35,633
那所谓的组合

639
00:21:35,633 --> 00:21:37,666
说实话比较容易

640
00:21:37,700 --> 00:21:39,766
直接是 concat 的操作

641
00:21:39,766 --> 00:21:44,000
就在大模型里面

642
00:21:44,000 --> 00:21:49,500
就把这一个 k 跟这个 k 里面合并起来

643
00:21:49,533 --> 00:21:50,133
所以这里面

644
00:21:50,133 --> 00:21:50,900
一个圈圈

645
00:21:50,900 --> 00:21:54,166
两个圈圈合并起来就变成三个圈圈

646
00:21:54,166 --> 00:21:56,366
通过这种方式直接 concat

647
00:21:56,566 --> 00:21:59,000
把一个权重组合起来

648
00:21:59,000 --> 00:22:00,600
变成一个更大的权重

649
00:22:00,600 --> 00:22:02,233
所以这里面一个圈圈加两个圈圈

650
00:22:02,233 --> 00:22:03,500
一共有三个圈圈

651
00:22:03,533 --> 00:22:06,733
把维度相对再扩展一点点

652
00:22:06,766 --> 00:22:08,966
就得到了 MHA

653
00:22:08,966 --> 00:22:11,700
其中的一个输入 key

654
00:22:12,800 --> 00:22:15,466
那接下来右边的这一坨

655
00:22:15,500 --> 00:22:16,200
也是一样

656
00:22:16,200 --> 00:22:17,200
跟刚才一样

657
00:22:17,200 --> 00:22:20,300
就反正有好几个矩阵的方式

658
00:22:20,333 --> 00:22:21,833
然后再把它 concat 起来

659
00:22:21,933 --> 00:22:24,633
那同样的以图更好的去了解

660
00:22:24,633 --> 00:22:26,300
首先一开始是 ht

661
00:22:26,333 --> 00:22:28,000
有很多个维度的输入

662
00:22:28,000 --> 00:22:29,833
经过一个映射投影

663
00:22:29,833 --> 00:22:32,600
变成只有 4 个圈圈

664
00:22:32,600 --> 00:22:34,600
潜在的空间 CTQ

665
00:22:34,733 --> 00:22:37,300
那通过 CTQ 再进行一个投影

666
00:22:37,300 --> 00:22:38,500
分成两部分

667
00:22:38,500 --> 00:22:40,800
一部分就变成 query c

668
00:22:40,800 --> 00:22:43,166
一部分变成 query 的 r

669
00:22:43,200 --> 00:22:45,366
那 c 就是潜在的空间

670
00:22:45,366 --> 00:22:46,966
r 就是位置编码

671
00:22:46,966 --> 00:22:49,766
再把这两个这三个圈圈合并起来

672
00:22:49,766 --> 00:22:50,666
concat 起来

673
00:22:50,700 --> 00:22:53,433
就变成最终的三个圈圈 q

674
00:22:53,533 --> 00:22:56,366
那再把 q 输给 MHA

675
00:22:56,433 --> 00:22:59,100
那 q 跟 k 是一一对应

676
00:22:59,133 --> 00:23:01,600
然后同样的还有一个 v

677
00:23:01,600 --> 00:23:02,633
那通过这种方式

678
00:23:02,633 --> 00:23:04,400
q 跟 v 一一对应

679
00:23:04,400 --> 00:23:07,066
给 MHA 进行计算

680
00:23:07,100 --> 00:23:09,966
所以说最核心的还是 MHA

681
00:23:10,166 --> 00:23:11,866
那同样的后面的公式

682
00:23:11,900 --> 00:23:13,933
就是跟传统的 MHA 一样

683
00:23:13,933 --> 00:23:14,900
算一个 Softmax

684
00:23:14,900 --> 00:23:19,200
然后再算一个整体的概率分布

685
00:23:19,233 --> 00:23:20,000
那基本上

686
00:23:20,000 --> 00:23:22,633
也学习完了 MLA

687
00:23:22,633 --> 00:23:25,100
那这里面还是提出几个问题哦

688
00:23:25,133 --> 00:23:28,933
那第一个就是 MLA 相比于 MHA

689
00:23:29,000 --> 00:23:32,000
最主要的思想就是在 MHA 之前

690
00:23:32,000 --> 00:23:34,400
多了一些 QKV 的处理

691
00:23:34,400 --> 00:23:35,666
矩阵的压缩

692
00:23:35,700 --> 00:23:37,700
为什么会变得更加的高效

693
00:23:37,700 --> 00:23:39,366
大家要思考一下这个问题

694
00:23:39,366 --> 00:23:40,766
矩阵

695
00:23:40,766 --> 00:23:42,633
秩的压缩

696
00:23:42,766 --> 00:23:44,400
维度的压缩

697
00:23:44,400 --> 00:23:46,633
为什么使得计算更加高效

698
00:23:46,900 --> 00:23:48,233
这个问题是很简单

699
00:23:48,233 --> 00:23:51,166
反正就计算计算量渐变少了嘛

700
00:23:51,166 --> 00:23:52,233
那第二个问题

701
00:23:52,233 --> 00:23:53,500
就是位置编码

702
00:23:53,533 --> 00:23:55,733
ROP 为什么不直接加

703
00:23:55,733 --> 00:23:58,200
加在 KTIC 里面

704
00:23:58,200 --> 00:24:02,066
我需要单独的创建一些新的 KTR

705
00:24:02,200 --> 00:24:04,233
位置编码为什么这么去加

706
00:24:04,233 --> 00:24:05,600
其实这个问题也比较简单

707
00:24:05,600 --> 00:24:07,866
就是经过多轮的秩

708
00:24:07,900 --> 00:24:09,366
或者维度的压缩之后

709
00:24:09,366 --> 00:24:11,400
有可能位置信息就变少

710
00:24:11,400 --> 00:24:13,266
或者位置信息就压没

711
00:24:13,300 --> 00:24:16,033
所以说把 ROPE 再加回去

712
00:24:16,033 --> 00:24:18,166
把位置信息编码加回去

713
00:24:18,166 --> 00:24:20,966
很好的处理记忆力的问题

714
00:24:20,966 --> 00:24:21,400
那这个

715
00:24:21,400 --> 00:24:22,766
是保证记忆

716
00:24:22,800 --> 00:24:25,366
能够记住更多的位置的信息

717
00:24:25,566 --> 00:24:26,233
那第三个

718
00:24:26,233 --> 00:24:28,766
就是为什么对于查询向量 q

719
00:24:28,766 --> 00:24:31,900
也需要进行一个潜在的计算

720
00:24:31,933 --> 00:24:34,400
也就是需要进行维度的压缩

721
00:24:34,400 --> 00:24:36,233
q 保持一样不就行了吗

722
00:24:36,233 --> 00:24:38,233
说实话其实最核心的就是

723
00:24:38,233 --> 00:24:41,500
保证维度的计算是相同

724
00:24:41,533 --> 00:24:43,966
保证都在同一个维度里面去计算

725
00:24:43,966 --> 00:24:45,433
因为我 q 太长

726
00:24:45,433 --> 00:24:47,266
那 k 跟 v 的值减少

727
00:24:47,300 --> 00:24:49,833
有可能导致模型的训练

728
00:24:49,833 --> 00:24:50,700
不稳定

729
00:24:50,700 --> 00:24:53,633
那在这一个视频的最后

730
00:24:53,633 --> 00:24:56,100
还是要做一个总结跟思考

731
00:24:57,200 --> 00:24:59,066
首先上面的这个总结

732
00:24:59,100 --> 00:25:01,333
是在上一节视频跟大家分享过

733
00:25:01,333 --> 00:25:02,433
下面看一下

734
00:25:02,500 --> 00:25:04,333
为了提升模型的效率

735
00:25:04,333 --> 00:25:05,166
attention 的架构

736
00:25:05,166 --> 00:25:07,033
还是在持续的演进

737
00:25:07,033 --> 00:25:08,000
至少 MQA

738
00:25:08,000 --> 00:25:10,000
是在 19 年的时候发布

739
00:25:10,166 --> 00:25:12,400
GQA 是在 23 年的时候发布

740
00:25:12,400 --> 00:25:14,466
MLA 是在 24 年的时候发布

741
00:25:14,500 --> 00:25:17,300
可以看到整个业绩还是演进的非常的快

742
00:25:17,300 --> 00:25:17,966
那 MQA

743
00:25:17,966 --> 00:25:20,000
就是共享多个 KV 的值

744
00:25:20,000 --> 00:25:23,200
就把多个 KV 的值变成一个 KV 的值

745
00:25:23,566 --> 00:25:24,566
那 GQA

746
00:25:24,566 --> 00:25:26,366
就是把兼顾

747
00:25:26,366 --> 00:25:29,500
MHA 跟 GQA 的一个连接方式

748
00:25:29,700 --> 00:25:30,533
那 MLA

749
00:25:30,533 --> 00:25:33,233
就探索更高效的注意力机制

750
00:25:33,233 --> 00:25:35,233
更好的处理长序列

751
00:25:35,333 --> 00:25:36,233
那今天的内容

752
00:25:36,233 --> 00:25:37,066
就到这里为止

753
00:25:37,100 --> 00:25:38,200
谢谢各位

754
00:25:38,233 --> 00:25:39,200
拜了个拜

