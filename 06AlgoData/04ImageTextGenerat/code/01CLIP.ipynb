{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141fcd55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "723d8639",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import timm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5365b8",
   "metadata": {},
   "source": [
    "选用图像编码器，采用 ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12481cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(ViT, self).__init__()\n",
    "        self.model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bfbee",
   "metadata": {},
   "source": [
    "选用文本编码器，采用 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1972b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        BERT_LOCAL_PATH = './bert-base-uncased'\n",
    "        self.model = BertModel.from_pretrained(BERT_LOCAL_PATH)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(BERT_LOCAL_PATH)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        return_tensors='pt'指定返回的是 PyTorch 张量。padding=True 和 truncation=True 表示如果输入的文本长度不一致 \n",
    "        将进行填充或截断 以确保所有文本具有相同的长度 这是 BERT 模型处理批量数据时的要求。\n",
    "        \"\"\"\n",
    "        encoded_input = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        outputs = self.model(**encoded_input)\n",
    "        # 取[CLS]标记的输出作为句子的表示\n",
    "        return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1534e3",
   "metadata": {},
   "source": [
    "构建 CLIP 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd011a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, image_output_dim, text_output_dim):\n",
    "        super(CLIP, self).__init__()\n",
    "        self.image_encoder = ViT(image_output_dim)\n",
    "        self.text_encoder = TextEncoder()\n",
    "\n",
    "        # 因为图像和文本 emb 可能维度不同(图像 512, 文本 768)所以需要对图像和文本的 emb 再经过一层以将维度持平\n",
    "        self.W_i = nn.Parameter(torch.randn(image_output_dim, text_output_dim))\n",
    "        self.W_t = nn.Parameter(torch.randn(768, text_output_dim)) # BERT-base 的最后隐藏层大小为 768\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_emb = self.image_encoder(image) # （b, 3, 224, 224） -> （b, 512）\n",
    "        text_emb = self.text_encoder(text)  # (b) -> (b, 768)\n",
    "\n",
    "        # 将图像和文本的 emb 映射到相同的维度\n",
    "        image_emb = torch.matmul(image_emb, self.W_i)   # (b, 512)\n",
    "        text_emb = torch.matmul(text_emb, self.W_t) # (b, 512)\n",
    "\n",
    "        # 计算余弦相似度\n",
    "        logits = torch.matmul(image_emb, text_emb.T)    # (b, b)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89af20c",
   "metadata": {},
   "source": [
    "加载 CIFAR10 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfff526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_dataset():\n",
    "    # 调整图像大小, 转换为 PyTorch 张量并将像素值归一化到 [0, 1] 的范围。\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    train_dataset = CIFAR10(root='./cifar10', train=True, download=True, transform=transform)\n",
    "    loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    classes = train_dataset.classes\n",
    "    return loader, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33580f17",
   "metadata": {},
   "source": [
    "主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ce5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 加载数据集\n",
    "    dataset, classes = load_cifar10_dataset()\n",
    "    print(f\"Classes: {classes}\")\n",
    "    # 初始化 CLIP 模型\n",
    "    clip_model = CLIP(image_output_dim=512, text_output_dim=512)\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        # 获得一个 batch 的图像和标签\n",
    "        texts = [classes[label] for label in labels]\n",
    "        logits = clip_model(images, texts)\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        # 对角线是真实的值, 故把位置当作真实标签\n",
    "        labels = torch.arange(logits.shape[0])  # (0, 1, 2, 3)\n",
    "\n",
    "        # 计算损失 loss_i 是每一张图像我都要把它判定为正确得文本，而 loss_t 是每一个文本我都要把它判定为正确得图像\n",
    "        # logits 是模型的输出, 假设形状为(b, c) b 是批次大小,c 是类别数量; 该函数, 表示的是每个批次对应各个类别的预测分数. \n",
    "        # 在这里即各个 Text/Image 对应各个 Image/Text 的相似度\n",
    "        # labels 通常是一个一维张量, 包含每个样本的真实标签\n",
    "        loss_i = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "        loss_t = torch.nn.CrossEntropyLoss()(logits.T, labels)\n",
    "\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "        print(f\"Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
