<!--Copyright © ZOMI 适用于[License](https://github.com/chenzomi12/AIInfra)版权许可-->

# NV Blackwell 产品演进分析

NVIDIA Blackwell 架构正引领 AI 计算进入新纪元。本节将深入解析其整体架构演进与多样化的产品形态，展示从单芯片到大规模系统部署的完整生态。

下图中概述了在BlackWell架构之前NVIDIA GPU的所有架构演进的过程，涵盖了从2010年的Fermi到2022年的Hopper各个时期。其中详细列举了各架构的发布时间、核心组成、主要特点与优势、制造工艺（纳米制程），以及代表型号。我们可以发现NVIDIA GPU技术在过去十余年间的显著发展，包括在核心设计、性能提升及制造工艺上的持续进步。

![NVIDIA GPU的架构演进](./images/Evolution01.png)


作为 NVIDIA 最新的计算平台，Blackwell 架构产品家族拥有从基础芯片到大规模系统部署的完整生态。其产品形态涵盖从单一芯片、到模组、再到计算托盘（Compute Tray），直至整个机柜，由此衍生出众多新型号和专业名词。接下来，我们将通过详细介绍 Blackwell 架构的产品家族，来剖析其真正的含义，以及这些产品背后所代表的最新 AI 方向与产品演进趋势。

![Blackwell架构产品家族](./images/Evolution02.png)

本节内容主要分为三部分：GPU产品介绍；HGX产品介绍；NVLink 与 SuperPOD 产品介绍

## GPU产品介绍

下图展示了NVIDIA主要GPU产品及其关键技术规格的对比，涵盖了从Ampere架构的A100到最新Blackwell架构的B系列产品。

首先，我们可以从架构演进的角度来看。表格清晰地展示了NVIDIA从Ampere（代表产品A100）发展到Hopper（代表产品H100、H200、GH200），再到最新的Blackwell（代表产品B100、B200、Full B200、GB200）的路线图。这种迭代更新体现了NVIDIA在高性能计算和AI领域的持续技术投入。

其次，存储性能是衡量GPU能力的重要指标。在HBM大小和HBM带宽两行中，我们可以看到显著的提升。从A100的80GB HBM和2TB/s带宽，逐步提升到H200的141GB和4.8TB/s，而Blackwell架构的产品更是实现了飞跃，B100和B200拥有180GB/192GB HBM和8TB/s带宽，GB200更是达到了惊人的384GB HBM和16TB/s带宽。这表明NVIDIA正不断扩大显存容量和带宽，以满足日益增长的AI模型对数据吞吐量的需求。

再者，计算能力是GPU的核心竞争力。表格详细列出了多种浮点运算（FLOPs）和整数运算（OPS）的性能指标。从FP16到FP4，我们可以看到：

* FP16 FLOPs 从A100的312T提升到H100的1P，Blackwell系列更是达到1.75P至5P。
* INT8 OPS 也从624T飙升至2P甚至Blackwell的10P。
* FP8、FP6和FP4 是Blackwell架构引入或大幅增强的计算特性，Hopper架构虽支持FP8但Blackwell有更高性能，而FP6和FP4仅在Blackwell系列中支持，尤其FP4在GB200上达到了20P。

NVLink带宽的提升同样关键，它衡量了GPU之间以及GPU与CPU之间的数据传输速度。从A100的600GB/s到Hopper的900GB/s，再到Blackwell的1.8TB/s，GB200甚至达到3.6TB/s，这使得构建大规模多GPU系统和实现CPU-GPU协同工作变得更加高效。

最后，功耗（Powers）是性能提升伴随的考量，从A100的400W到Blackwell系列的700W至2700W（GB200），反映了为实现更高性能所需的能源消耗。

值得注意的是，在芯片设计中，“1 Die”（单晶粒）表示单个封装内仅含一个独立的硅晶粒，而“2 Die”（双晶粒）则意味着一个封装内实际包含两个独立的硅晶粒。这两个晶粒通常通过高速互联技术（如NVIDIA的NVLink-C2C）实现无缝连接，在逻辑上协同工作，形成一个更强大的整体。
在Blackwell架构问世之前，包括Ampere和Hopper在内的NVIDIA GPU架构均采用单晶粒（1 Die）设计。然而，Blackwell架构则引入了革命性的双晶粒（2 Die）构造。这一设计上的根本性转变使得一张Blackwell架构的B100芯片，在计算能力和资源上，实际上等同于传统意义上的两张GPU卡。
正因如此，在Blackwell架构的系统配置中，例如在一个计算托盘（Trays板）内，您会观察到它可能包含8张Ampere或Hopper架构的GPU芯片，但若配置Blackwell架构的B系列芯片，则仅需4张B系列芯片即可达到相同的总卡概念（即4张B芯片等同于8张单Die卡的算力），这充分体现了双晶粒设计在提升单芯片集成度和性能密度方面的显著优势。

![GPU产品](./images/Evolution03.png)


下图则表示B100芯片的概念图。

![B100](./images/Evolution04.png)


接下来，我们将详细分析FP16浮点运算能力在这三代GPU架构中的显著变化。

首先，从Ampere架构的A100到Hopper架构的H100，FP16算力实现了超过三倍的惊人增长（从312 TFLOPs提升至1 PFLOP）。值得注意的是，尽管性能大幅提升，但功耗仅从400瓦增加到700瓦。随后，从Hopper架构的H200到Blackwell架构的B200，FP16算力再次实现了超过两倍的提升（从1 PFLOP提升至2.25 PFLOP），而同期功耗仅增加了300瓦（从700瓦增至1000瓦）。这些数据清晰地揭示了芯片制程工艺进步所带来的两大核心优势：一是能够实现几何级的性能飞跃，二是能够尽可能地抑制功耗的同步大幅增长。这种高效的性能提升模式，正是半导体制造工艺持续演进的强大驱动力。

下面则引出一个问题：DGX B200是否适用于大模型推理任务呢？

![DGX B200](./images/Evolution05.png)

如上图所示，在训练场景下，DGX B200 相较于 DGX H100 展现出约三倍的性能提升；而在推理场景下，其性能提升更是高达15倍以上。这一数据强烈暗示 DGX B200 在大模型推理任务中具备显著优势。

接下来，我们将对DGX B200实现如此性能提升的内在机制进行深入分析：

首先，从硬件规格来看，DGX B200 相较于 DGX H100 在FP16浮点运算能力上提升了2.25倍。与此同时，其高带宽内存（HBM）容量显著增大，显存带宽也实现了2.23倍的提升，而NVLink的互联带宽更是直接翻倍。在这些核心硬件性能指标的全面跃升下，若假设模型算力利用率（MFU）能够达到50%，那么整体训练性能实现三倍左右的提升是完全符合硬件增益逻辑的。

然而，对于推理场景，我们认为DGX B200的适用性并非全然乐观，甚至在某些情况下，其所谓的巨大性能提升可能需要更审慎的评估。图中所示的“15倍推理性能提升”，是通过比较B200的FP4算力与H100的FP8算力得出的。这种跨精度等级的比较并不完全对等，因为FP4相较于FP8会引入更高的量化误差，可能不适用于所有对精度要求严苛的推理任务。此外，在推理场景下，相较于训练，对性价比的考量往往更为关键。尽管B200在峰值性能上表现卓越，但其高昂的成本和在实际推理部署中是否能充分发挥FP4性能（例如，并非所有模型都支持或需要FP4推理，或者FP4推理需要额外的模型量化工作），这些因素都可能影响其在推理场景下的实际效益和普及程度。


## HGX产品介绍

在深入探讨 HGX 产品线之前，让我们先厘清两个关键术语：HGX 和 DGX。理解它们各自独特的角色，将有助于您更清晰地了解 NVIDIA 在高性能计算领域的产品布局。

* HGX（Hyperscale GPU eXchange） 是 NVIDIA 推出的一款强大且模块化的 GPU 平台，主要面向 OEM 厂商，例如浪潮（Inspur）、戴尔（Dell）和超微（Supermicro）。您可以将 HGX 想象成高性能服务器的“核心引擎”。它主要由一个 GPU 托盘或主板模块构成，集成了多颗 GPU 及其高速互联网络，例如 NVLink 和 NVSwitch。至关重要的一点是，HGX 模块本身不包含CPU、存储或其他服务器必备组件。相反，它提供核心的 GPU 计算能力和高速网络互联，由 OEM 厂商将其整合到他们完整的服务器系统中（例如超微的 SYS 系列）。简而言之，HGX 提供的是**硬件模块 + 高速互联**，旨在帮助构建高密度、模块化、可扩展的 GPU 集群。
* DGX（Deep GPU Xceleration）是 NVIDIA 直接提供的一整套开箱即用的高性能服务器系统。这类产品由 NVIDIA 亲自设计、制造和销售。DGX 系统是一个一体化的解决方案，集成了 GPU、CPU、内存、存储和网络组件，形成一个完整且优化的系统。DGX 产品专为大型 AI 模型的训练和推理而设计，提供无缝、即插即用的体验。

现在，我们已经区分了这两个关键平台，接下来将以 HGX 系列产品为例，介绍从 Ampere 架构到 Blackwell 架构的技术演进。

![HGX](./images/Evolution06.png)

图中详尽地展示了NVIDIA不同代次HGX平台产品的关键技术规格对比，涵盖了从Ampere架构的HGX A100到最新Blackwell架构的HGX B100和HGX B200。

核心指标方面，图中详细对比了以下几个关键性能参数：

**HBM大小和HBM带宽**：这些数据反映了系统总体的显存容量和数据吞吐能力。可以看到，从HGX A100的640GB HBM和2TB/s带宽，逐步提升到HGX H100的1.1TB HBM和3.35TB/s带宽，再到HGX B200惊人的1.5TB HBM和8TB/s带宽，显存容量和带宽都呈现出显著的增长趋势，以满足日益庞大的AI模型对显存和数据传输的需求。

**浮点和整数运算能力 (FLOPs/OPS)**：表格列出了不同精度（FP16、INT8、FP8、FP6、FP4）的理论峰值算力。

* FP16 算力从HGX A100的312T FLOPs提升到HGX H100的1 PFLOPs，再到HGX B200的2.25 PFLOPs，显示了AI训练能力的大幅提升。
* INT8 (8-bit integer) 算力同样从624T OPS跃升至4.5P OPS，表明在推理任务中的效率显著提高。
* FP8、FP6和FP4 是更低精度的浮点运算，其中FP8在Hopper架构中开始支持，而FP6和FP4则主要在Blackwell架构中出现并显示出极高的性能（例如HGX B200的FP4达到9P FLOPs），这对于大模型推理和进一步压缩计算量至关重要。

**互联带宽**：

* GPU-GPU带宽：衡量了HGX模块内部GPU之间的数据传输速度，从A100的600 GB/s提升到H100的900 GB/s，再到Blackwell的1.8 TB/s，确保了多GPU协同计算的高效性。
* NVLink带宽：这是GPU之间，乃至HGX模块之间的高速互联总带宽，从HGX A100的4.8 TB/s到HGX H100的7.2 TB/s，再到HGX B200的14.4 TB/s，其翻倍的增长对于构建更大规模的AI集群至关重要。
* 网络带宽：表格还包括了外部网络连接的能力，如Ethernet带宽和IB（InfiniBand）带宽。这些指标从HGX A100的200 Gb/s以太网和8 x 200 Gb/s IB，提升到HGX B200的2 x 400 Gb/s以太网和8 x 400 Gb/s IB，体现了系统对外数据传输能力的显著增强，这对于分布式训练和集群扩展至关重要。

**功耗**：分GPUs Power（GPU核心功耗）和总Power（整个HGX服务器的功耗）。可以看到，随着性能的提升，功耗也相应增加，从HGX A100的6.5kw总功耗，跃升到HGX B200的14.3kw。

**网络产品**：展示了各代HGX平台所搭载的网络适配器或DPU（数据处理器），如ConnectX系列网卡和BlueField-3 DPU，它们提供了高性能的网络连接和卸载能力。

通过深入分析NVIDIA HGX平台的产品规格演变，我们可以清晰地描绘出其技术迭代的轨迹。首先，从Ampere架构的HGX A100迈向Hopper架构的HGX H100/H200时，FP16浮点运算能力实现了约3.2倍的飞跃，与此同时，总功耗的增幅却控制在2倍以内，这充分彰显了新一代纳米制程工艺在提升计算效率和优化能耗方面的卓越贡献。紧接着，从Hopper架构的HGX H100/H200过渡到Blackwell架构的HGX B100/B200，FP16算力再次取得了约2倍的显著增长，而令人惊喜的是，系统总功耗却基本保持不变，这主要归功于Blackwell架构引入的创新性双晶粒（2 Die）设计，有效提升了封装内的计算密度。然而，在外部网络连接方面，尽管GPU核心性能实现了跨越式发展，但Blackwell架构的InfiniBand（IB）带宽仍维持在8x400Gb/s，其升级速度相较于GPU算力的爆发式增长显得相对滞缓，这可能预示着未来超大规模AI集群在节点间通信方面可能面临新的挑战。


## NVL & SuperPod 产品介绍

GB200 NVL72 是 NVIDIA Grace Blackwell 超级芯片的创新集成，采用先进的液冷机架式设计，集成了 36 个 GB200 Grace Blackwell 超级芯片。

作为 GB200 NVL72 的核心，每个 GB200 Grace Blackwell 超级芯片都通过 NVIDIA NVLink™-C2C 高速互连技术，将两颗高性能的 NVIDIA Blackwell Tensor Core GPU 与一个 NVIDIA Grace™ CPU 紧密连接起来，实现了 CPU 与 GPU 之间以及两颗 GPU 之间的极致带宽通信。

在此基础上，GB200 SuperPOD 则是一个由 8 个 GB200 NVL72 单元组成的超级计算集群，旨在提供前所未有的 AI 训练和推理性能。

![NVL&SuperPod](./images/Evolution07.png)


上图全面介绍了Hopper架构（GH200 NVL 32, GH200 SuperPod）和Blackwell架构（GB200 NVL72, GB200 SuperPod）的主要产品及其关键性能指标。

1.  **内存与带宽**：HBM总大小从GH200 SuperPod的24.5TB（LPDDR5X为123TB）大幅跃升至GB200 SuperPod的110TB，显存带宽也从Hopper的4.8TB/s提升到Blackwell的8TB/s。这表明新架构为超大规模AI模型提供了更广阔的内存空间和更快的数据存取速度。

2.  **计算能力 (FLOPs/OPS)**：
    * **FP16** 算力从GH200 SuperPod的256 PFLOPs，飙升至GB200 SuperPod的1440 PFLOPs（即1.44 EFLOPS），实现了超过5倍的巨大飞跃，展现了Blackwell架构在AI训练任务中的强大潜力。
    * **INT8和FP8** 算力也呈现相似的倍数增长，对于推理任务至关重要。
    * 值得注意的是，Blackwell架构引入并大幅提升了**FP6和FP4**的计算能力，其中GB200 SuperPod的FP4算力高达5760 PFLOPs（5.76 EFLOPS），这预示着在低精度推理和更高效的AI应用方面将有突破性的进展。

3.  **内部互联带宽**：
    * **GPU-GPU带宽** 从Hopper架构的0.9TB/s提升到Blackwell架构的1.8TB/s，翻了一番。
    * **NVLink带宽** 的提升尤为显著，从GH200 SuperPod的230TB/s，跃升至GB200 SuperPod的1PB/s（1000TB/s），这对于实现GPU集群内的高效数据同步和模型并行训练至关重要，确保了超大规模集群的线性扩展能力。NVLink Switch也从Gen3 64 Port升级到Gen4 72 Port，提升了交换能力。

4.  **外部网络带宽**：
    * **Ethernet带宽** 从256 x 200Gb/s提升到576 x 400Gb/s，总带宽大幅增加。
    * **IB带宽** 从256 x 400Gb/s提升到576 x 800Gb/s，这表明NVIDIA正在为构建更大、更复杂的分布式AI训练和推理集群提供更强大的外部通信能力，以减少网络瓶颈。

5.  **总功耗**：随着性能的急剧提升，功耗也相应增加。GH200 SuperPod的总功耗为256kW，而GB200 SuperPod的总功耗高达777.6kW，这反映了这些超级计算系统对电力和散热基础设施的极致要求。

6.  **网络产品**：Hopper架构的HGX使用ConnectX-7 NIC，而Blackwell架构则升级到ConnectX-8 NIC，提供了更高的网络性能和更先进的网络卸载功能。


## 总结与思考

NVIDIA Blackwell 架构的蓬勃发展，无疑为国内芯片与计算服务器产业的未来指明了极具价值的启发方向。

其中一个最为显著且不可逆转的趋势是，计算架构正从最初相对简单的**芯片堆叠模式，逐步演进为提供整套大规模集群解决方案**的复杂生态。NVIDIA Blackwell 架构（如 GB200 SuperPOD）的出现，不再仅仅是推出更强大的 GPU 芯片，而是着眼于如何将数以百计、乃至千计的高性能 GPU，通过超高带宽的 NVLink 互联、先进的网络架构（例如 ConnectX-8 InfiniBand），以及精密的软件协调层，无缝地整合为一个具备极致扩展能力和计算效率的统一体。这不仅仅是硬件性能的线性累加，更是通过系统级优化，将计算、存储、网络融为一体，以应对万亿参数级别大模型训练和推理的严峻挑战。

这种持续扩大规模并提供端到端解决方案的发展模式，对国内芯片设计企业和计算服务器制造商而言意义深远。未来的竞争不再局限于单一芯片性能，更在于构建高效协同、易于部署和管理的全栈式 AI 计算基础设施。这意味着，除了在芯片设计上追赶国际水平，国内厂商还需在高速互联技术（如类 NVLink）、大规模集群管理软件、高性能散热供电及定制化数据中心部署方案等多维度深度布局和创新，才能在未来的 AI 算力竞赛中占据一席之地。简而言之，竞争焦点已从“造好芯”拓展到“搭好台”，再到“唱好戏”，最终形成一个无缝连接、高度优化的完整计算生态。

