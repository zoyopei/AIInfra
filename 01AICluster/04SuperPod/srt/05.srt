1
00:00:00,000 --> 00:00:02,066
内容/录制:Z0MI 酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,066 --> 00:00:02,933
哈喽大家好

3
00:00:02,933 --> 00:00:04,733
刚看完精神小妹的视频

4
00:00:08,733 --> 00:00:11,133
现在我们要来深夜的录制一个课了

5
00:00:11,133 --> 00:00:13,933
就是世界上最大的 10 万集群了

6
00:00:13,933 --> 00:00:15,333
10 万 GPU 的集群

7
00:00:15,333 --> 00:00:19,000
由英伟达跟 xAI 合作的搞的 Colossus

8
00:00:19,166 --> 00:00:21,133
那下面我们看一下这一期视频

9
00:00:21,133 --> 00:00:22,566
主要分享哪些知识

10
00:00:22,650 --> 00:00:23,250
那首先

11
00:00:23,250 --> 00:00:23,766
这里面

12
00:00:23,766 --> 00:00:26,966
ZOMI 会分开 6 个内容跟大家去汇报和

13
00:00:27,050 --> 00:00:29,083
解读的好

14
00:00:29,083 --> 00:00:30,733
这一期视频还是蛮有意思
15
00:00:30,733 --> 00:00:32,283
我们从那个 YouTube 上面

16
00:00:32,283 --> 00:00:35,083
去重点去解读这 15 分钟

17
00:00:35,083 --> 00:00:36,733
这个我也不知道是谁了

18
00:00:36,733 --> 00:00:39,083
反正就采访了 XAI 里面

19
00:00:39,083 --> 00:00:40,883
Colossus 的一个具体的机房

20
00:00:41,083 --> 00:00:42,483
具体是在孟菲斯建

21
00:00:42,483 --> 00:00:43,000
一个

22
00:00:43,000 --> 00:00:46,483
全世界最大的 GPU 的集群的机房了

23
00:00:46,483 --> 00:00:47,050
那现在

24
00:00:47,050 --> 00:00:49,400
我们会重点来看一下这 15 分钟里面

25
00:00:49,400 --> 00:00:51,933
跟大家解读出哪些信息出来

26
00:00:52,250 --> 00:00:52,533
第一个

27
00:00:52,533 --> 00:00:53,450
我们会去看一下

28
00:00:53,450 --> 00:00:55,483
GPU 的 H100 的一个液冷的机架

29
00:00:55,483 --> 00:00:55,800
接着

30
00:00:55,800 --> 00:00:58,000
我们会看一下它的一个存储系统

31
00:00:58,000 --> 00:00:59,883
有液冷机架肯定会有存储

32
00:00:59,883 --> 00:01:01,400
当然这里面比较有意思的一点

33
00:01:01,400 --> 00:01:03,200
因为在整个 xAI 里面

34
00:01:03,200 --> 00:01:04,883
或者在特斯拉的自动驾驶

35
00:01:04,883 --> 00:01:06,083
和它的大模型场景

36
00:01:06,083 --> 00:01:08,850
会经常用到一些 CPU 作数据的预处理

37
00:01:08,850 --> 00:01:10,566
因此就会单独的出现了

38
00:01:10,566 --> 00:01:12,850
一个 GPU 的计算的集群

39
00:01:12,883 --> 00:01:14,966
有了 GPU 的集群和 CPU 的集群

40
00:01:14,966 --> 00:01:17,533
他们之间要进行一个 10 万卡的组网

41
00:01:17,566 --> 00:01:20,250
于是肯定很重要的就是用以太网

42
00:01:20,250 --> 00:01:22,566
去进行一个组网

43
00:01:22,683 --> 00:01:24,733
了解完所有的 AI 集群之后

44
00:01:24,733 --> 00:01:27,250
我们了解了其他的额外的东西

45
00:01:27,250 --> 00:01:30,166
就是基础建设的风火水电液冷

46
00:01:30,166 --> 00:01:30,650
各种各样

47
00:01:30,650 --> 00:01:32,083
的东西最后

48
00:01:32,083 --> 00:01:34,600
我们还是要做一个简单的总结和思考

49
00:01:34,600 --> 00:01:36,366
就是我们 10 万卡集群

50
00:01:36,366 --> 00:01:39,133
对于 AI 的一个相关的思考

51
00:01:39,133 --> 00:01:40,450
我们的模型该怎怎么样子

52
00:01:40,450 --> 00:01:42,283
我们的未来的产品该怎怎么样子

53
00:01:42,333 --> 00:01:45,366
未来可能会有多少个 10 万集群的出现

54
00:01:45,366 --> 00:01:48,133
我们都做一个简单的分享

55
00:01:48,333 --> 00:01:49,850
我们现在来到了第一个内容

56
00:01:49,850 --> 00:01:53,133
看一下整个 GPU 的 H100 的液冷的机架

57
00:01:53,133 --> 00:01:54,250
那在整个机架里面

58
00:01:54,250 --> 00:01:56,450
其实还是有蛮多东西了

59
00:01:56,450 --> 00:01:57,000
那这里面

60
00:01:57,000 --> 00:02:00,083
整个 Colossus 的 AI 的超节点

61
00:02:00,083 --> 00:02:01,883
或者一个超 10 万卡集群

62
00:02:01,883 --> 00:02:03,366
它使用的是一个超微

63
00:02:03,366 --> 00:02:04,766
整体的解决方案

64
00:02:04,766 --> 00:02:05,850
于是 ZOMI 在这里面

65
00:02:05,850 --> 00:02:07,800
已经列了很多相关的信息

66
00:02:07,800 --> 00:02:09,966
我们后面会在一边播视频

67
00:02:10,000 --> 00:02:12,600
一边去总结相关的内容

68
00:02:12,600 --> 00:02:13,166
那里面

69
00:02:13,166 --> 00:02:15,133
最重要的是一个 GPU 的相关的内容

70
00:02:15,133 --> 00:02:16,366
还有机架的配置

71
00:02:16,366 --> 00:02:18,250
另外他还会跟大家去分享一下

72
00:02:18,250 --> 00:02:19,450
整个 CPU 的 tray

73
00:02:19,450 --> 00:02:21,083
和对应的可维护性

74
00:02:21,083 --> 00:02:21,766
那我们现在

75
00:02:21,766 --> 00:02:22,850
一边来去欣赏

76
00:02:22,850 --> 00:02:25,400
我们对应的一些 GPU 的集群

77
00:02:25,400 --> 00:02:28,283
然后一边去跟大家分享相关的资讯

78
00:02:29,366 --> 00:02:31,450
在 Colossus 这个 10 万机区里面

79
00:02:31,450 --> 00:02:35,333
现在其实是部署了 10 万个 hopper 的 GPU

80
00:02:35,333 --> 00:02:37,450
那这里面的 hopper 不完全是 H100 哦

81
00:02:37,450 --> 00:02:39,650
它其实还有 5 万个 H100

82
00:02:39,650 --> 00:02:41,200
还有 5 万个 H200

83
00:02:41,200 --> 00:02:43,166
那整个 Colossus 的它基本的构成

84
00:02:43,166 --> 00:02:45,366
大家可以看到有非常多的这种导管

85
00:02:45,366 --> 00:02:46,566
那这里证明

86
00:02:46,566 --> 00:02:48,650
它是使用超微的一个液冷机架

87
00:02:48,650 --> 00:02:51,250
每个机架配备八台的 4U 的服务器

88
00:02:51,250 --> 00:02:52,250
所以我们可以看到

89
00:02:52,250 --> 00:02:54,400
每个服务器它是相对比较厚

90
00:02:54,400 --> 00:02:56,050
因为它采用的是 4U 而不

91
00:02:56,050 --> 00:02:58,850
是 1U 那我们之前在讲 DGX 的时候

92
00:02:58,850 --> 00:03:01,200
它可能做到 b 系列里面最薄

93
00:03:01,200 --> 00:03:02,133
是做到 1U

94
00:03:02,133 --> 00:03:04,283
所以说它后面就推出了

95
00:03:04,283 --> 00:03:06,283
Tray 和 rank 相关的概念

96
00:03:06,283 --> 00:03:07,333
是为了把它做薄

97
00:03:07,333 --> 00:03:09,133
单位的功耗面积做大

98
00:03:09,166 --> 00:03:11,333
那接着我们看一下整个机架的配置

99
00:03:11,333 --> 00:03:12,200
还蛮有意思

100
00:03:12,200 --> 00:03:12,933
甚至是里面

101
00:03:12,933 --> 00:03:14,683
在整个 Colossus 里面

102
00:03:14,966 --> 00:03:15,883
每台服务器

103
00:03:15,883 --> 00:03:18,650
就每个节点其实是有 8 块 H100

104
00:03:18,650 --> 00:03:21,166
这样的话每个机架一共有 64 块

105
00:03:21,166 --> 00:03:22,800
也就是每个柜

106
00:03:22,800 --> 00:03:24,883
每个 Rack 有 64 块 GPU

107
00:03:24,883 --> 00:03:27,050
每个机架间有 64 块 GPU

108
00:03:27,050 --> 00:03:28,450
那它一共一排

109
00:03:28,450 --> 00:03:30,283
横下来它一共有八个机架

110
00:03:30,483 --> 00:03:33,200
一共一排有 512 个 GPU

111
00:03:33,333 --> 00:03:34,283
整体 Colossus

112
00:03:34,283 --> 00:03:37,533
它整个大的机房有超过 1,500 个机架

113
00:03:37,533 --> 00:03:38,800
接近 200 多个

114
00:03:38,800 --> 00:03:40,600
正面非常的夸夸张

115
00:03:40,600 --> 00:03:44,400
所以才组成了我们接近 10 万卡的集群

116
00:03:44,400 --> 00:03:46,483
那这里面的每个服务器后面

117
00:03:46,483 --> 00:03:49,166
或者上面顶端都会配备四个电源

118
00:03:49,166 --> 00:03:49,766
这些电源

119
00:03:49,766 --> 00:03:53,450
会支持热插拔的一些相关的接口

120
00:03:53,450 --> 00:03:55,000
所以可以非常的方便

121
00:03:55,000 --> 00:03:56,683
做一些运维和维护

122
00:03:56,683 --> 00:03:58,050
那另外的话我们可以看到

123
00:03:58,050 --> 00:03:59,400
在整个超微里面

124
00:03:59,400 --> 00:04:00,366
它基本上

125
00:04:00,366 --> 00:04:02,533
每个服务器有那个四个接口嘛

126
00:04:02,683 --> 00:04:04,366
然后我们看到有非常多的歧管

127
00:04:04,366 --> 00:04:05,533
那这些歧管

128
00:04:05,533 --> 00:04:06,733
其实非常方便

129
00:04:06,733 --> 00:04:10,283
我们一些液冷的排进和排出部件

130
00:04:10,283 --> 00:04:11,050
非常方便

131
00:04:11,050 --> 00:04:13,733
我们把可能把这个歧管的导头拔掉

132
00:04:13,733 --> 00:04:16,133
然后我们的托盘整个就可以插拔

133
00:04:16,166 --> 00:04:17,733
试着进行一个维护

134
00:04:17,966 --> 00:04:19,883
这是对整个 10 万卡集群里面

135
00:04:19,883 --> 00:04:21,766
非常重要的一个点

136
00:04:21,766 --> 00:04:23,083
因为 10 万卡集群

137
00:04:23,133 --> 00:04:26,000
真的是可维护性是非常的强

138
00:04:26,200 --> 00:04:26,600
或者

139
00:04:26,600 --> 00:04:29,566
可以说可维护性是非常的核心

140
00:04:29,566 --> 00:04:29,966
另外的话

141
00:04:29,966 --> 00:04:31,333
我们看一下网络

142
00:04:31,333 --> 00:04:32,483
在机架的背面

143
00:04:32,483 --> 00:04:35,366
其实用的一个是 GPU 跟 CPU 的一个连接

144
00:04:35,366 --> 00:04:37,883
400GB 的一个以太网的光缆

145
00:04:37,883 --> 00:04:38,533
这些光缆

146
00:04:38,533 --> 00:04:39,883
其实是方便去管理

147
00:04:39,883 --> 00:04:41,850
我们整体的一个参数面

148
00:04:41,850 --> 00:04:42,366
和业务面

149
00:04:42,366 --> 00:04:43,850
网络的每个节点

150
00:04:43,850 --> 00:04:47,000
其实有 9 个 400GB 的一个网络连接

151
00:04:47,000 --> 00:04:47,733
整体的带宽

152
00:04:47,733 --> 00:04:50,450
到了 3.6TB/s

153
00:04:50,483 --> 00:04:51,200
虽然这个视频

154
00:04:51,200 --> 00:04:51,800
播的有点快

155
00:04:51,800 --> 00:04:53,366
但是我们还是能够 get 到

156
00:04:53,366 --> 00:04:55,283
非常多有用的信息

157
00:04:55,283 --> 00:04:56,133
里面的网卡

158
00:04:56,133 --> 00:04:58,800
是用 BlueField 的一个自带的网卡芯片

159
00:04:58,800 --> 00:05:00,166
然后整体的解决方法

160
00:05:00,166 --> 00:05:02,766
是用 Mellanox 的 ConnectX-7 的网卡

161
00:05:02,766 --> 00:05:05,050
提供我们的 CPU 的服务

162
00:05:05,600 --> 00:05:06,650
我们现在看第二个点

163
00:05:06,650 --> 00:05:08,966
就是 NVMe 的一个存储系统

164
00:05:08,966 --> 00:05:10,200
这里面的整个存储系统

165
00:05:10,200 --> 00:05:12,366
使用的整体是 SSD

166
00:05:12,366 --> 00:05:13,000
在这里面

167
00:05:13,000 --> 00:05:15,083
总理总结了一些简单的规格

168
00:05:15,083 --> 00:05:17,650
我们还是换回我们整体的一个视频

169
00:05:17,883 --> 00:05:19,933
在整个存储规模里面

170
00:05:19,933 --> 00:05:21,166
整个 SSD

171
00:05:21,166 --> 00:05:22,600
里面的存储的 pod

172
00:05:22,600 --> 00:05:24,483
已经去到了 EB 级别了

173
00:05:24,483 --> 00:05:25,250
然后介质

174
00:05:25,250 --> 00:05:26,333
就是我们刚才讲到

175
00:05:26,333 --> 00:05:28,483
它用 SSD 然后整体的服务器

176
00:05:28,483 --> 00:05:29,650
用的还是比较薄

177
00:05:29,650 --> 00:05:31,933
1U 的一个超微的服务器

178
00:05:31,933 --> 00:05:33,200
因为超微它本身

179
00:05:33,200 --> 00:05:35,450
也是做一个存储 OEM 厂商嘛

180
00:05:35,450 --> 00:05:36,850
所以它使用自家的服务

181
00:05:36,850 --> 00:05:37,883
很正常

182
00:05:38,200 --> 00:05:39,366
不可能不用自己家

183
00:05:39,366 --> 00:05:40,683
然后整体的特点就是

184
00:05:40,683 --> 00:05:43,250
说实话整个 AI 的集群的训练

185
00:05:43,250 --> 00:05:44,966
特别是像特斯拉这种业务

186
00:05:44,966 --> 00:05:47,366
他会除了可能大于模型以外

187
00:05:47,366 --> 00:05:48,850
他会训自己的车载

188
00:05:48,850 --> 00:05:51,666
端到端的视频和端到端的大模型

189
00:05:51,666 --> 00:05:52,283
那因此

190
00:05:52,283 --> 00:05:53,333
它的一个存储的 POD

191
00:05:53,333 --> 00:05:54,650
对他来说非常重要

192
00:05:54,650 --> 00:05:57,333
因为车载的视频是非常的夸张

193
00:05:57,366 --> 00:05:57,883
所以

194
00:05:57,883 --> 00:06:00,533
他就单独的做了一个 SSD 集群

195
00:06:00,766 --> 00:06:02,483
集群跟集群之间的一个交互

196
00:06:02,483 --> 00:06:04,283
整体是通过一个网络

197
00:06:04,283 --> 00:06:06,050
以太网来进行一个访问

198
00:06:06,533 --> 00:06:07,800
我们现在来到了第三个内容

199
00:06:07,800 --> 00:06:10,200
看一下整个 CPU 的一个计算的集群

200
00:06:10,200 --> 00:06:11,850
那 CPU 的计算机还蛮有意思

201
00:06:11,850 --> 00:06:12,966
因为我们之前其实

202
00:06:12,966 --> 00:06:15,083
看到很多的万卡集群的服务器了

203
00:06:15,083 --> 00:06:16,883
大部分都只有一个 GPU 的集群

204
00:06:16,883 --> 00:06:20,400
那整个特斯拉或者在 xAI 里面的 Colossus

205
00:06:20,400 --> 00:06:21,283
整个集群里面

206
00:06:21,283 --> 00:06:23,000
是包含了一个独立

207
00:06:23,000 --> 00:06:24,250
CPU 的计算的集群

208
00:06:24,250 --> 00:06:25,133
那相关的规格

209
00:06:25,133 --> 00:06:26,200
在这里面 ZOMI

210
00:06:26,200 --> 00:06:27,766
是打开视频

211
00:06:27,766 --> 00:06:29,600
也是跟大家一起去分享

212
00:06:30,600 --> 00:06:32,650
那在整个服务器形态里面

213
00:06:32,650 --> 00:06:33,600
整个 CPU 集群

214
00:06:33,600 --> 00:06:36,650
采用的是一个 1U 的超微的一个服务器

215
00:06:36,650 --> 00:06:38,850
也就是超微自己做的一个解决方案了

216
00:06:39,050 --> 00:06:41,683
整体每个机架有 42 个 CPU

217
00:06:41,683 --> 00:06:42,566
这些 CPU 里面

218
00:06:42,566 --> 00:06:44,566
采用的是 X86

219
00:06:44,600 --> 00:06:45,850
具体是哪个型号

220
00:06:45,850 --> 00:06:48,450
其实没有讲到底是 AMD 还是英特尔

221
00:06:48,450 --> 00:06:49,650
到底是英特尔的至强

222
00:06:49,650 --> 00:06:52,083
还是 AMD 的其他型号

223
00:06:52,366 --> 00:06:53,333
这里面没有公布

224
00:06:53,333 --> 00:06:54,133
也打不开

225
00:06:54,133 --> 00:06:56,683
不过 whatever 这些可能不是很重要

226
00:06:56,683 --> 00:06:58,483
这里面为什么会采用 CPU

227
00:06:58,483 --> 00:06:59,966
可能 ZOMI 猜测

228
00:07:00,000 --> 00:07:02,166
更多是用在一些数据的预处理

229
00:07:02,166 --> 00:07:04,966
或者我们的一些普通的管理节点里面

230
00:07:04,966 --> 00:07:05,966
去使用

231
00:07:05,966 --> 00:07:07,083
那整体的网卡

232
00:07:07,083 --> 00:07:08,800
还是配备一个 400GB

233
00:07:08,800 --> 00:07:09,650
一个仪态网

234
00:07:09,683 --> 00:07:11,283
那散热还是非常有意思

235
00:07:11,283 --> 00:07:12,533
整体散热哦

236
00:07:12,766 --> 00:07:14,050
它采用的不是液冷

237
00:07:14,050 --> 00:07:16,250
因为只有 GPU 才会采用液冷

238
00:07:16,283 --> 00:07:18,533
整体的散热的采用的是风冷

239
00:07:18,533 --> 00:07:19,650
在 CPU 机芯里面

240
00:07:19,650 --> 00:07:22,250
通过机架后部的一个热交换器

241
00:07:22,283 --> 00:07:23,533
将具体的热量

242
00:07:23,533 --> 00:07:26,200
传递回液冷的一个回路当中

243
00:07:26,450 --> 00:07:27,000
所以可以看到

244
00:07:27,000 --> 00:07:28,733
在整个 10 万卡集群里面

245
00:07:28,766 --> 00:07:29,933
大部分的采用的是液冷

246
00:07:29,933 --> 00:07:31,933
但是有部分的零部件

247
00:07:31,933 --> 00:07:33,450
和部分的一个集群

248
00:07:33,450 --> 00:07:34,883
采用的是一个风冷

249
00:07:34,883 --> 00:07:35,966
包括我们的 CPU 集群

250
00:07:35,966 --> 00:07:38,050
和刚才讲到的一个存储的 POD

251
00:07:39,283 --> 00:07:40,766
接下来来到了第四个内容

252
00:07:40,766 --> 00:07:42,800
就是以太网的互联了

253
00:07:42,800 --> 00:07:44,000
所谓的以太网的互联

254
00:07:44,000 --> 00:07:45,766
我们主要分开两个

255
00:07:45,766 --> 00:07:48,133
第一个是参数面的网络的互联

256
00:07:48,133 --> 00:07:50,850
第二个就是业务面的网络的互联了

257
00:07:50,850 --> 00:07:51,733
所谓的参数面

258
00:07:51,733 --> 00:07:53,966
就是我们的 GPU 里面训练

259
00:07:53,966 --> 00:07:56,933
或者 NPU 里面训练的一个权重的参数

260
00:07:56,933 --> 00:07:58,283
互相传输的一个网络

261
00:07:58,283 --> 00:07:58,933
另外的话

262
00:07:58,933 --> 00:07:59,883
针对业务面

263
00:07:59,883 --> 00:08:01,333
主要是指我们的控制面

264
00:08:01,333 --> 00:08:03,800
和对外的一些接口相关的一个内容

265
00:08:03,800 --> 00:08:04,600
包括我们的数据

266
00:08:04,600 --> 00:08:06,200
会从存储的 Pod 存到 CPU

267
00:08:06,200 --> 00:08:08,533
CPU 再给到我们的 GPU 进行计算

268
00:08:08,533 --> 00:08:10,283
所以我们分开两个面了

269
00:08:10,283 --> 00:08:10,966
一个是参数面

270
00:08:10,966 --> 00:08:12,366
一个是业务面

271
00:08:12,366 --> 00:08:13,000
那这里面

272
00:08:13,000 --> 00:08:15,133
我们还是回到具体的视频里面喽

273
00:08:15,683 --> 00:08:17,000
那在整个参数里面

274
00:08:17,000 --> 00:08:18,933
主要是以 400GB 的一个以太网

275
00:08:18,933 --> 00:08:19,966
然后英伟达

276
00:08:19,966 --> 00:08:21,483
采用的是他自己家

277
00:08:21,483 --> 00:08:23,766
一个 Spectrum-X 的一个网络解决方案

278
00:08:23,800 --> 00:08:25,483
主要是支持一个 RDMA

279
00:08:25,483 --> 00:08:26,483
一个以太网方式

280
00:08:26,483 --> 00:08:28,733
那具体的交换机的型号

281
00:08:28,733 --> 00:08:30,733
基本上每个交换机有 64 个端口

282
00:08:30,733 --> 00:08:33,133
支持 800 Gb/s 的速度

283
00:08:33,133 --> 00:08:36,566
可以分割成为 128 个 400GB 的一个

284
00:08:36,566 --> 00:08:37,683
具体的链路

285
00:08:37,683 --> 00:08:39,600
网卡也是我们刚才讲到

286
00:08:39,600 --> 00:08:41,683
一个 BlueField 的一个 super Nic

287
00:08:41,683 --> 00:08:43,200
也是对应的超网卡

288
00:08:43,200 --> 00:08:44,050
为每个 GPU

289
00:08:44,050 --> 00:08:45,283
提供一个比较快速

290
00:08:45,283 --> 00:08:47,250
RDMA 的以太网的一个连接

291
00:08:47,800 --> 00:08:50,000
另外我们现在来看看业务面

292
00:08:50,166 --> 00:08:50,850
在业务面

293
00:08:50,850 --> 00:08:54,133
采用的网卡或者我们的交换器

294
00:08:54,133 --> 00:08:54,800
也是一样

295
00:08:54,800 --> 00:08:57,933
使用了 400GB 的一个以太网交换器

296
00:08:57,933 --> 00:08:59,600
那特点就是以太网了

297
00:08:59,883 --> 00:09:03,333
整体它基本上整个 Colossus 里面

298
00:09:03,333 --> 00:09:05,366
就没有采用 IB 的相关技术了

299
00:09:05,366 --> 00:09:07,000
而是完全采用以太网

300
00:09:07,000 --> 00:09:08,366
因为以太网说实话

301
00:09:08,366 --> 00:09:09,600
它的一个造价更便宜

302
00:09:09,600 --> 00:09:11,533
而且有比较好的扩展性

303
00:09:11,533 --> 00:09:12,800
满足了一个万卡

304
00:09:12,800 --> 00:09:14,366
一个 scale out 的一个交付

305
00:09:14,366 --> 00:09:17,166
那具体的分开参数面和一个业务面

306
00:09:17,166 --> 00:09:19,650
就是为了把 GPU 的网络和 CPU 的网络

307
00:09:19,650 --> 00:09:20,450
进行分段

308
00:09:20,766 --> 00:09:21,850
保证我们的集群

309
00:09:21,850 --> 00:09:24,000
能够去到最佳的性能

310
00:09:24,000 --> 00:09:25,250
我们不同网络之间

311
00:09:25,250 --> 00:09:26,683
可以并行的进行一些操作

312
00:09:26,683 --> 00:09:27,966
或者数据的传输

313
00:09:28,133 --> 00:09:30,766
那我们的网络基本上还是中规中矩

314
00:09:30,766 --> 00:09:33,533
我们有望未来的可能 10 万卡集群

315
00:09:33,533 --> 00:09:34,283
基本上

316
00:09:34,283 --> 00:09:36,050
都会采用以太网这种解决方案

317
00:09:36,050 --> 00:09:38,083
因为 IB 的造价实在太贵了

318
00:09:38,083 --> 00:09:39,400
成本也非常的高

319
00:09:39,966 --> 00:09:41,333
快来到倒数第二个刻内容了

320
00:09:41,333 --> 00:09:43,883
我们现在来看看其他的基础设施

321
00:09:43,883 --> 00:09:45,533
就是风火水电了

322
00:09:45,533 --> 00:09:46,650
那风火水电里面

323
00:09:46,650 --> 00:09:49,166
其实最重要的还是液冷

324
00:09:49,166 --> 00:09:51,600
和对应的一个嗯

325
00:09:51,600 --> 00:09:54,450
机房的电力的提供的设施

326
00:09:54,450 --> 00:09:57,050
那这边我们还是回到对应的视频

327
00:09:57,050 --> 00:09:59,133
首先整个散热方式

328
00:09:59,133 --> 00:10:01,533
采用了液冷加风冷的一个形态

329
00:10:01,533 --> 00:10:02,283
那液冷里面

330
00:10:02,283 --> 00:10:04,083
我们可以看到每个机架底部

331
00:10:04,083 --> 00:10:06,883
它都有一个 CDU 的一个冷却液

332
00:10:06,883 --> 00:10:08,400
分配单元

333
00:10:08,400 --> 00:10:10,366
所以它英文叫做 CDU 了

334
00:10:10,533 --> 00:10:12,333
然后还有一个冗余的泵系统

335
00:10:12,333 --> 00:10:13,083
那这个系统

336
00:10:13,083 --> 00:10:14,450
就在每一个机架

337
00:10:14,450 --> 00:10:16,533
就每个柜下面都有一个

338
00:10:16,533 --> 00:10:18,566
是对我们的整个所谓的液冷系统

339
00:10:18,566 --> 00:10:20,050
来进行一个控制

340
00:10:20,050 --> 00:10:20,766
那整体来

341
00:10:20,766 --> 00:10:22,566
我们可以看到里面有大量的这种

342
00:10:22,566 --> 00:10:24,966
那个在经过整个 CDU 的控制

343
00:10:24,966 --> 00:10:27,533
这些气管就会不断的去交互相关

344
00:10:27,533 --> 00:10:30,133
一些流进流出的一些液冷

345
00:10:30,133 --> 00:10:31,133
或液体了

346
00:10:31,483 --> 00:10:33,000
另外一套我们知道这里面

347
00:10:33,000 --> 00:10:35,733
其实还是保留风扇的系统

348
00:10:35,733 --> 00:10:37,600
那这些风扇主要是用来干嘛

349
00:10:37,600 --> 00:10:38,566
主要用来冷却

350
00:10:38,566 --> 00:10:39,766
内存电源单元

351
00:10:39,766 --> 00:10:40,850
还有主板管理控制器

352
00:10:40,850 --> 00:10:41,683
还有网卡

353
00:10:41,683 --> 00:10:43,800
一些相对低功耗的组件

354
00:10:43,800 --> 00:10:45,366
而高功耗的 GPU

355
00:10:45,366 --> 00:10:48,200
采用的就是我们的液冷的方案了

356
00:10:48,483 --> 00:10:48,850
那这边

357
00:10:48,850 --> 00:10:51,566
我们还看一下其他的冷却系统

358
00:10:51,566 --> 00:10:53,050
特别是 CPU

359
00:10:53,050 --> 00:10:55,733
还有一些网络的适配和存储的适配

360
00:10:55,733 --> 00:10:58,133
都是通过风冷的方式

361
00:10:58,133 --> 00:10:59,766
那在整个机房里面

362
00:10:59,766 --> 00:11:01,933
就采用了一个冷水的循环系统

363
00:11:02,000 --> 00:11:05,083
CPU 将整体的热量传递到循环水中了

364
00:11:05,283 --> 00:11:07,333
热液在整体的设施外面

365
00:11:07,333 --> 00:11:09,483
是不断的循环利用

366
00:11:09,483 --> 00:11:11,683
所以说整个嗯

367
00:11:11,683 --> 00:11:13,483
机房基本上你就得重建了

368
00:11:13,483 --> 00:11:14,450
你用液冷的话

369
00:11:14,450 --> 00:11:15,283
而现在

370
00:11:15,283 --> 00:11:18,333
国内要建 10 万卡的 AI 集群的话

371
00:11:18,333 --> 00:11:20,766
基本上也需要进行一个重建

372
00:11:20,766 --> 00:11:23,283
因为它的一个功耗实在是太大了

373
00:11:23,283 --> 00:11:24,200
另外它我们看一下

374
00:11:24,200 --> 00:11:26,883
它外部的一个电力的一个装置

375
00:11:26,883 --> 00:11:28,683
那供电采用是三相的电源

376
00:11:28,683 --> 00:11:29,566
每个机架下面

377
00:11:29,566 --> 00:11:32,600
其实有非常非常多的一个电源线

378
00:11:32,600 --> 00:11:33,883
那对于储能来说

379
00:11:33,883 --> 00:11:35,250
我们可以看到外面

380
00:11:35,250 --> 00:11:36,966
还是有几个大的箱子不过

381
00:11:36,966 --> 00:11:38,250
这里面没有人去揭开它

382
00:11:38,250 --> 00:11:39,366
ZOMI 看不到

383
00:11:39,366 --> 00:11:41,366
哎真希望帅哥

384
00:11:41,366 --> 00:11:43,250
能够把这个东西揭开

385
00:11:43,250 --> 00:11:44,966
大家看一下里面长什么样子

386
00:11:44,966 --> 00:11:46,400
那这面用的是一个特斯拉

387
00:11:46,400 --> 00:11:48,366
自己的 Megapack 的电磁组

388
00:11:48,366 --> 00:11:49,683
作为一个超级计算机

389
00:11:49,683 --> 00:11:52,533
和电网之间的一个能源缓冲区

390
00:11:52,566 --> 00:11:55,166
为什么会用英伟达的自己的 Megapack

391
00:11:55,166 --> 00:11:56,533
是因为整个 Megapack

392
00:11:56,533 --> 00:11:59,283
它可以存储非常高的一个电能

393
00:11:59,283 --> 00:12:00,166
整体来说

394
00:12:00,166 --> 00:12:00,800
一个大柜

395
00:12:00,800 --> 00:12:02,450
可以存储 3.9M

396
00:12:02,766 --> 00:12:05,283
千瓦的一个电能

397
00:12:05,283 --> 00:12:06,083
那这里面

398
00:12:06,083 --> 00:12:07,050
引入了 Macpad

399
00:12:07,050 --> 00:12:08,566
主要是去解决 GPU

400
00:12:08,566 --> 00:12:10,650
服务器的一个功耗的一个波动

401
00:12:10,650 --> 00:12:12,400
对电网造成的压力

402
00:12:12,400 --> 00:12:14,083
因为我们在闲时的时候

403
00:12:14,083 --> 00:12:16,800
可能说我们吸收的电源是很少的话

404
00:12:16,800 --> 00:12:17,733
消耗的电源很少

405
00:12:17,733 --> 00:12:20,283
但是如果 10 万卡都同时跑起来

406
00:12:20,283 --> 00:12:22,366
它瞬时的一个电力的消耗

407
00:12:22,366 --> 00:12:23,566
是非常夸张

408
00:12:23,566 --> 00:12:24,000
所以

409
00:12:24,000 --> 00:12:26,850
使用了 Megapack 进行一个电力的缓冲

410
00:12:26,966 --> 00:12:27,883
那今天内容

411
00:12:27,883 --> 00:12:31,933
我们还是回到这个 10 万卡集群的思考

412
00:12:31,933 --> 00:12:32,766
还蛮有意思

413
00:12:32,766 --> 00:12:35,333
我们将会在深入的思考的一些内容了

414
00:12:35,333 --> 00:12:36,650
放在在下一期视频了

415
00:12:36,650 --> 00:12:38,400
这期视频已经录得太长了

416
00:12:38,400 --> 00:12:40,850
那我们重点是考虑 10 万卡集群

417
00:12:40,850 --> 00:12:42,366
以前说大模型训练

418
00:12:42,366 --> 00:12:44,050
是一个非常复杂的系统工程

419
00:12:44,166 --> 00:12:47,733
实际上我们抛开训练大模型

420
00:12:47,800 --> 00:12:49,650
就这么建一个 10 万卡集群

421
00:12:49,650 --> 00:12:51,166
已经非常的复杂了

422
00:12:51,166 --> 00:12:53,050
我们不仅仅算力

423
00:12:53,050 --> 00:12:56,050
GPU 包括 NPU 的数率的增长以外

424
00:12:56,050 --> 00:12:58,050
我们还涉及非常之复杂

425
00:12:58,050 --> 00:13:00,050
技术和运营的挑战

426
00:13:00,133 --> 00:13:02,400
特别是运营的挑战是非常多

427
00:13:02,400 --> 00:13:03,800
特别是后续的维护

428
00:13:03,800 --> 00:13:04,250
那这里面

429
00:13:04,250 --> 00:13:06,800
  

430
00:13:06,800 --> 00:13:08,250
就像高功耗的一个管理哦

431
00:13:08,250 --> 00:13:09,800
高密度的机房的设置

432
00:13:09,800 --> 00:13:13,400
还有高稳定性训练的一系列的难题

433
00:13:13,800 --> 00:13:16,483
反正所有东西都变得非常的难

434
00:13:16,483 --> 00:13:17,450
因为 10 万卡基金

435
00:13:17,450 --> 00:13:19,450
说实话特斯拉的 xAI

436
00:13:19,450 --> 00:13:21,000
或者那个马斯克的 xAI

437
00:13:21,000 --> 00:13:23,283
是全世界第一个建出来

438
00:13:23,283 --> 00:13:24,250
或者全世界第一个

439
00:13:24,250 --> 00:13:26,400
披露自己相关的技术的了

440
00:13:26,400 --> 00:13:29,083
最终能否将所有的 GPU 的算力

441
00:13:29,083 --> 00:13:30,333
澎湃的释放出来

442
00:13:30,333 --> 00:13:32,400
还取决于我们的算法

443
00:13:32,483 --> 00:13:35,800
软件的架构的并行的调度的优化

444
00:13:35,800 --> 00:13:36,966
那这边的软件架构

445
00:13:36,966 --> 00:13:38,966
不仅仅是指 AI 英法的软件架构

446
00:13:38,966 --> 00:13:41,733
还包括我们网络的软件架构

447
00:13:41,733 --> 00:13:42,683
所以这里面

448
00:13:42,683 --> 00:13:44,050
建一个 10 万集群

449
00:13:44,333 --> 00:13:45,000
实在是太难了

450
00:13:45,000 --> 00:13:46,566
ZOMI 将会在后面的内容

451
00:13:46,650 --> 00:13:48,166
详细的去打开

452
00:13:48,166 --> 00:13:51,133
我们关于 10 万集群的思考

453
00:13:51,166 --> 00:13:51,933
今天的内容

454
00:13:51,933 --> 00:13:52,683
就到这为止

455
00:13:52,683 --> 00:13:53,166
谢谢各位

456
00:13:53,166 --> 00:13:53,966
拜了个拜

