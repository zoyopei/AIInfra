1
00:00:00,000 --> 00:00:02,366
内容/录制/字幕:Z0MI 酱，视频剪辑:梁嘉铭

2
00:00:02,366 --> 00:00:03,250
hello 大家好

3
00:00:03,250 --> 00:00:06,083
这里面是 ZOMI 的深夜电台

4
00:00:06,083 --> 00:00:07,000
刚下班没多久

5
00:00:07,000 --> 00:00:09,333
现在已经块到凌晨 1 点钟了

6
00:00:09,333 --> 00:00:10,650
那现在来块速

7
00:00:10,650 --> 00:00:11,966
跟大家去解读一下

8
00:00:11,966 --> 00:00:14,600
Kimi 的最新这两天发布的文章

9
00:00:14,600 --> 00:00:17,050
叫做 MoBA 的一个深度解读

10
00:00:17,600 --> 00:00:19,566
在这一期的视频里面

11
00:00:19,566 --> 00:00:20,850
可能会分开 4 个内容

12
00:00:20,850 --> 00:00:22,966
跟大家一起去分享和介绍

13
00:00:22,966 --> 00:00:23,333
第一个

14
00:00:23,333 --> 00:00:24,200
就是看一下

15
00:00:24,200 --> 00:00:26,766
Kimi 的一个 MoBA 的一个技术的文章

16
00:00:26,766 --> 00:00:28,566
的一个整体的走读

17
00:00:28,683 --> 00:00:31,133
接着看一下 Kimi 的 MoBA 跟 DeepSeek

18
00:00:31,133 --> 00:00:32,400
最近也是发布

19
00:00:32,400 --> 00:00:34,683
因为网上说了 DeepSeek 跟 Kimi

20
00:00:34,683 --> 00:00:36,883
其实是同一天里面撞车

21
00:00:36,883 --> 00:00:39,533
发布了一个相关长序列的文章

22
00:00:39,766 --> 00:00:42,450
对比一下两篇文章有哪些区别

23
00:00:42,566 --> 00:00:44,200
接着再做一个

24
00:00:44,200 --> 00:00:46,483
Kimi 的一个 MoBA 的在线测试

25
00:00:46,483 --> 00:00:48,450
最后对整个产业和大模型

26
00:00:48,450 --> 00:00:50,483
进行一个简单的思考和总结

27
00:00:50,533 --> 00:00:52,000
那为什么叫做 MoBA

28
00:00:52,000 --> 00:00:52,966
其实现在

29
00:00:52,966 --> 00:00:55,283
就正式的来到了第一个内容

30
00:00:55,283 --> 00:00:58,083
看一下 Kimi 这个 MoBA 的技术文章

31
00:00:58,083 --> 00:00:59,650
相关的解读

32
00:01:00,600 --> 00:01:02,733
打开了这一篇文章

33
00:01:02,733 --> 00:01:07,333
叫做 MoBA MIXTURE OF BLOCK ATTENTION FOR LONG-CONTEXT

34
00:01:07,333 --> 00:01:09,166
那简单的看一下这个标题

35
00:01:09,166 --> 00:01:09,650
最重要

36
00:01:09,650 --> 00:01:10,533
就是提出了

37
00:01:10,533 --> 00:01:14,250
一个混合的 Blook attention 相关的结构

38
00:01:14,250 --> 00:01:16,000
那新的一个 attention 的结构

39
00:01:16,000 --> 00:01:18,133
用于处理长序列的问题

40
00:01:18,133 --> 00:01:19,966
因此叫做 MoBA

41
00:01:20,250 --> 00:01:22,533
那整个 MoBA 里面的作者蛮有意思

42
00:01:22,533 --> 00:01:25,200
就是杨植麟自己是一个作者之一

43
00:01:25,250 --> 00:01:26,450
那同一天晚上

44
00:01:26,450 --> 00:01:28,566
也就是 2025 年 2 月 18 晚上

45
00:01:28,566 --> 00:01:30,566
Kimi 的创始人杨植麟

46
00:01:30,766 --> 00:01:33,483
跟幻方量化的创始人梁文峰

47
00:01:33,483 --> 00:01:35,166
同样也是发表了一篇文章

48
00:01:35,166 --> 00:01:37,850
也是作为一个作者之一

49
00:01:37,850 --> 00:01:40,366
因此在会在文章的后面

50
00:01:40,366 --> 00:01:41,166
第三节里面

51
00:01:41,166 --> 00:01:43,800
去对比两个文章的一个差异

52
00:01:43,850 --> 00:01:44,800
那今天的视频

53
00:01:44,800 --> 00:01:48,250
重点去解读一下 MoBA 这个内容

54
00:01:48,250 --> 00:01:49,966
那其实大家会发现

55
00:01:49,966 --> 00:01:50,883
或者这篇文章

56
00:01:50,883 --> 00:01:51,800
就提到了

57
00:01:51,800 --> 00:01:53,050
为了去追求

58
00:01:53,050 --> 00:01:54,966
或者 AGI 的过程当中

59
00:01:55,200 --> 00:01:57,166
其实 scaling 到长序列

60
00:01:57,166 --> 00:01:58,800
是非常的重要

61
00:01:58,800 --> 00:02:00,600
为了解决这个长序列的问题

62
00:02:00,600 --> 00:02:03,683
其实前年已经做了很多相关的研究

63
00:02:04,000 --> 00:02:04,933
在这篇文章里面

64
00:02:04,933 --> 00:02:07,333
就提出了一个新的解决方案

65
00:02:07,333 --> 00:02:08,200
叫做 MoBA

66
00:02:08,200 --> 00:02:10,000
一个新的网络模型的结构

67
00:02:10,000 --> 00:02:11,400
那最重要的就是

68
00:02:11,400 --> 00:02:13,966
在以前的 attention 的机制上面

69
00:02:13,966 --> 00:02:18,083
去把 Mixture of Experts (MoE)的架构引入进来

70
00:02:18,366 --> 00:02:19,333
最重要的一个好处

71
00:02:19,333 --> 00:02:19,533
就是

72
00:02:19,533 --> 00:02:22,250
在我现有的一个网络模型的架构

73
00:02:22,250 --> 00:02:24,000
能够方便的去替换掉

74
00:02:24,000 --> 00:02:26,850
我现在的一个稀疏的模型的架构

75
00:02:26,933 --> 00:02:29,333
非常无缝的去实现

76
00:02:29,333 --> 00:02:30,000
那最后

77
00:02:30,000 --> 00:02:33,683
还给出整个 MoBA 相关的一个开源实现

78
00:02:33,683 --> 00:02:35,400
那大家有兴趣或者我等一下

79
00:02:35,400 --> 00:02:36,966
也会简单的去打开

80
00:02:36,966 --> 00:02:38,800
那接着看一下 introduction

81
00:02:38,800 --> 00:02:40,650
一开始的一个介绍的内容

82
00:02:40,800 --> 00:02:41,283
说实话

83
00:02:41,283 --> 00:02:44,250
最重要的就是对 AGI 的一个追求

84
00:02:44,250 --> 00:02:46,250
现在长序列越来越重要

85
00:02:46,250 --> 00:02:47,483
包括 COT

86
00:02:47,533 --> 00:02:49,883
之前 Kimi 发布的一个 K1.5

87
00:02:49,883 --> 00:02:52,250
也就是 1 月中旬的时候

88
00:02:52,333 --> 00:02:53,000
但是

89
00:02:53,000 --> 00:02:55,683
由于现在的一个 Transformer 架构的问题

90
00:02:55,933 --> 00:02:57,600
你会发现对于长序列处理

91
00:02:57,600 --> 00:02:58,883
不是非常的好

92
00:02:58,883 --> 00:03:00,133
于是就发现

93
00:03:00,133 --> 00:03:02,483
很多人去解决这个长序列的问题

94
00:03:02,566 --> 00:03:04,933
不过你会发现前人解决的问题

95
00:03:05,000 --> 00:03:07,000
都有很多的限制

96
00:03:07,050 --> 00:03:11,050
因此 constant 大教记提出了一个疑问了

97
00:03:11,450 --> 00:03:13,650
如何在网络模型不变的情况下

98
00:03:13,650 --> 00:03:16,083
使用更少的结构去实现

99
00:03:16,083 --> 00:03:19,766
于是就提出了一个 mixture of block attention

100
00:03:19,766 --> 00:03:22,200
相关的一个网络模型新的结构

101
00:03:22,200 --> 00:03:24,966
那这个网络模型新的结构叫做 MoBA

102
00:03:24,966 --> 00:03:25,650
这个 MoBA

103
00:03:25,650 --> 00:03:28,000
主要是去解决传统注意力机制

104
00:03:28,000 --> 00:03:31,000
也就是最原始的 transormer decorder 的时候

105
00:03:31,366 --> 00:03:32,850
计算效率非常低下

106
00:03:32,850 --> 00:03:35,800
也就是需要大量的矩阵乘将上下文

107
00:03:35,800 --> 00:03:37,533
特别是遇到长序列的时候

108
00:03:37,533 --> 00:03:39,850
分配到不同的块的当中

109
00:03:39,850 --> 00:03:40,966
所以这里面就说了

110
00:03:40,966 --> 00:03:44,283
他用了很多 relative Blook 相关的 Blook

111
00:03:44,283 --> 00:03:45,966
在整篇的文章当中

112
00:03:45,966 --> 00:03:47,933
可能会分开好几个内容

113
00:03:47,933 --> 00:03:49,050
跟大家去分享

114
00:03:49,050 --> 00:03:49,450
第一个

115
00:03:49,450 --> 00:03:52,800
就是 Blook 分解和路由策略

116
00:03:53,050 --> 00:03:53,400
第二点

117
00:03:53,400 --> 00:03:56,450
就是跟传统 traditional attention

118
00:03:56,450 --> 00:03:58,733
跟传统的注意力机制来比

119
00:03:58,733 --> 00:04:00,133
怎么去节省计算

120
00:04:00,133 --> 00:04:00,600
最后

121
00:04:00,600 --> 00:04:03,600
就是提出了一些实验相关的内容

122
00:04:03,600 --> 00:04:04,533
可能第一部分

123
00:04:04,533 --> 00:04:05,283
跟第二部分是

124
00:04:05,283 --> 00:04:07,450
现在可能会更关注于

125
00:04:07,450 --> 00:04:09,366
这个网络模型的算法结构

126
00:04:09,366 --> 00:04:11,166
有哪些新的创新

127
00:04:11,166 --> 00:04:11,650
第二部分

128
00:04:11,650 --> 00:04:13,250
可能更弱化一点

129
00:04:13,250 --> 00:04:14,050
第三部分

130
00:04:14,050 --> 00:04:16,800
更属于 experiences 实验相关的部分

131
00:04:16,800 --> 00:04:18,566
简单的读一下

132
00:04:18,566 --> 00:04:19,083
那现在

133
00:04:19,083 --> 00:04:21,533
来到了最核心的第一个内容

134
00:04:21,533 --> 00:04:22,966
就是 math 这篇文章

135
00:04:22,966 --> 00:04:25,366
最重要的一个算法原理了

136
00:04:25,966 --> 00:04:26,650
那首先

137
00:04:26,650 --> 00:04:28,850
在这篇文章的办法里面

138
00:04:28,850 --> 00:04:30,450
最重要的就是这篇文章

139
00:04:30,450 --> 00:04:32,800
引用了一个新的网络模型结构

140
00:04:32,800 --> 00:04:34,400
叫做 MoBA

141
00:04:34,400 --> 00:04:35,533
MoBA 最重要

142
00:04:35,533 --> 00:04:38,333
这个就是通过动态的去选择

143
00:04:38,333 --> 00:04:40,283
历史的一个 segmentation

144
00:04:40,283 --> 00:04:41,650
或者历史的一个片段

145
00:04:41,650 --> 00:04:44,450
叫做历史的 Blook 去做

146
00:04:44,450 --> 00:04:47,133
的 attention 注意力机制的计算

147
00:04:47,133 --> 00:04:47,683
那这个

148
00:04:47,683 --> 00:04:49,283
就是这篇论文所提供

149
00:04:49,283 --> 00:04:50,933
一个最核心的思想

150
00:04:50,933 --> 00:04:53,850
那了解这个最核心的思想之前

151
00:04:53,850 --> 00:04:55,250
首先还是回顾一下

152
00:04:55,250 --> 00:04:56,850
传统的一个 attention 机制

153
00:04:56,850 --> 00:04:58,333
到底是怎么去实现

154
00:04:58,533 --> 00:04:59,733
最核心的 attention

155
00:04:59,733 --> 00:05:01,650
主要是由 QKV 三个矩阵

156
00:05:02,000 --> 00:05:04,333
首先最核心的就是 q 乘以 k

157
00:05:04,333 --> 00:05:05,733
然后 q 乘以 k 的一个 score

158
00:05:05,733 --> 00:05:08,366
之后去经过 Softmast 的计算

159
00:05:08,400 --> 00:05:10,683
最终等于 QK score

160
00:05:10,733 --> 00:05:11,400
QK score

161
00:05:11,400 --> 00:05:12,650
乘以 v 最终

162
00:05:12,650 --> 00:05:13,766
就等于得到

163
00:05:13,766 --> 00:05:15,250
整个 attention 的结构

164
00:05:15,250 --> 00:05:18,366
的输出叫做 attention UKV

165
00:05:18,400 --> 00:05:19,166
相关的内容

166
00:05:19,166 --> 00:05:23,283
就是最原始的 attention decoder 相关的计算

167
00:05:23,850 --> 00:05:26,083
这篇文章里面在 2.2 了

168
00:05:26,083 --> 00:05:29,083
就迎来了一个新的网络模型的架构

169
00:05:29,083 --> 00:05:30,683
那这个新的网络模型架构

170
00:05:30,683 --> 00:05:32,533
最核心可以看到这里面

171
00:05:32,533 --> 00:05:34,600
跟刚才那条公式差不多

172
00:05:34,600 --> 00:05:36,283
但是多了一个 i

173
00:05:36,333 --> 00:05:37,483
这里面的有一个 i

174
00:05:37,566 --> 00:05:40,483
这个 i 就是选择的一个 key

175
00:05:40,483 --> 00:05:43,166
跟 values 对应的一个子集

176
00:05:43,166 --> 00:05:44,083
那现在

177
00:05:44,083 --> 00:05:46,200
还没有讲到这一个具体的图

178
00:05:46,200 --> 00:05:48,650
没关系先看完整体的算法

179
00:05:48,800 --> 00:05:50,000
那这个算法里面

180
00:05:50,000 --> 00:05:52,400
最核心的就是 block partition

181
00:05:52,400 --> 00:05:54,533
也就是对 block 进行分类

182
00:05:54,533 --> 00:05:57,050
还有怎么去选择相关的策略

183
00:05:57,050 --> 00:05:59,250
那这里面对于整体的一个

184
00:05:59,250 --> 00:06:00,966
上下文的长度

185
00:06:00,966 --> 00:06:01,800
也就是 n

186
00:06:01,850 --> 00:06:03,800
切分成为 n 个 block

187
00:06:03,800 --> 00:06:04,883
也就是 n 个块

188
00:06:04,883 --> 00:06:07,133
根据模型序列越来越长了

189
00:06:07,133 --> 00:06:09,400
所以需要对它进行一个分割

190
00:06:09,933 --> 00:06:10,333
分完之后

191
00:06:10,333 --> 00:06:12,333
就可以得到每个 Blook 的大小

192
00:06:12,333 --> 00:06:14,400
就是 b 等于 n 除以大 n

193
00:06:14,400 --> 00:06:15,283
除以小 n

194
00:06:15,333 --> 00:06:16,283
那可以看到

195
00:06:16,283 --> 00:06:19,766
每个 Blook 的 index i 就实大 i 加小 i

196
00:06:19,766 --> 00:06:21,800
就是这一条计算公式

197
00:06:21,883 --> 00:06:23,333
为了在 attention 机制

198
00:06:23,333 --> 00:06:25,800
去引入了一个 mixture of Expert

199
00:06:25,966 --> 00:06:27,333
所以这里面有一个 TOPK

200
00:06:27,333 --> 00:06:29,483
然后这里面整体的公式

201
00:06:29,483 --> 00:06:30,733
就引入了 TOPK

202
00:06:30,733 --> 00:06:32,450
基于 attention 的机制

203
00:06:32,650 --> 00:06:34,400
把 top k 引进来了

204
00:06:34,400 --> 00:06:35,566
那下面的内容

205
00:06:35,566 --> 00:06:36,733
其实跟是 shazeer

206
00:06:36,733 --> 00:06:38,533
就是 ZOMI 之前分布的一系列

207
00:06:38,533 --> 00:06:40,083
MoE 的模型的架构

208
00:06:40,083 --> 00:06:41,850
或者 MoE 的模型的算法

209
00:06:41,850 --> 00:06:43,650
基本上哦是一致

210
00:06:43,883 --> 00:06:46,133
也就是把 MoE 之间的一个路由

211
00:06:46,133 --> 00:06:48,483
跟门控引入进来

212
00:06:48,483 --> 00:06:51,050
然后基本上内容比较相似

213
00:06:51,050 --> 00:06:52,733
所以说它最重要的一个创新

214
00:06:52,733 --> 00:06:56,600
就是把 MoE 引入了一个 attention 机制里面

215
00:06:56,600 --> 00:06:59,083
去实现了一个网络模型的效果

216
00:06:59,083 --> 00:07:00,133
变得非常好

217
00:07:00,450 --> 00:07:03,133
那这里面最核心的一个点就说到了

218
00:07:03,133 --> 00:07:05,166
其实最重要的事情是

219
00:07:05,166 --> 00:07:06,966
自回归的大语言模型当中

220
00:07:06,966 --> 00:07:09,450
去保持因果关系

221
00:07:09,450 --> 00:07:11,883
每天一个因果的关系

222
00:07:11,933 --> 00:07:14,133
从而使得之前的 token

223
00:07:14,133 --> 00:07:16,283
能够预测下一个大 token

224
00:07:16,283 --> 00:07:16,933
那这里面

225
00:07:16,933 --> 00:07:19,933
其实有两个事情是最重要

226
00:07:19,933 --> 00:07:22,166
第一个就是因果关系

227
00:07:22,166 --> 00:07:24,966
不去关注未来的一些 Blook

228
00:07:25,400 --> 00:07:25,850
那第二点

229
00:07:25,850 --> 00:07:27,800
就是当前的注意力

230
00:07:27,800 --> 00:07:29,450
跟因果的一个掩盖

231
00:07:29,450 --> 00:07:31,000
说白了可以看到

232
00:07:31,000 --> 00:07:31,533
现在

233
00:07:31,533 --> 00:07:34,683
虽然还没有讲到一个内容

234
00:07:34,683 --> 00:07:35,333
不过没关系

235
00:07:35,333 --> 00:07:37,050
简单的先看完原理

236
00:07:37,050 --> 00:07:38,733
再看一下相关的内容

237
00:07:38,733 --> 00:07:40,850
首先刚才讲到了两个很重要的点

238
00:07:40,850 --> 00:07:42,600
第一个就是因果的关系

239
00:07:42,600 --> 00:07:43,966
那因为 MoBA 没有

240 
00:07:43,966 --> 00:07:46,683
办法去确定查询的一个 token 的来由

241
00:07:46,850 --> 00:07:47,766
或者它的来源

242
00:07:47,766 --> 00:07:50,200
是由哪个 Blook 来引起

243
00:07:50,200 --> 00:07:52,533
所以尽可能的因果干系

244
00:07:52,533 --> 00:07:55,566
也就只看到前面的一些 Blook

245
00:07:55,566 --> 00:07:56,933
或者前面的 token

246
00:07:56,933 --> 00:07:58,200
而不去看后面

247
00:07:58,200 --> 00:07:59,883
所以就说到了 XI

248
00:07:59,883 --> 00:08:01,683
就视之为居无穷

249
00:08:01,683 --> 00:08:03,683
那这种方式去实现

250
00:08:03,683 --> 00:08:06,483
可能我只关心我之前看到的文本

251
00:08:06,483 --> 00:08:07,166
那第二个

252
00:08:07,166 --> 00:08:08,166
就是 cost Blook

253
00:08:08,166 --> 00:08:10,366
attention 跟 costal masking

254
00:08:10,366 --> 00:08:12,083
costal masking 就很重要了

255
00:08:12,083 --> 00:08:14,800
这里面就去定一个 cost Blook

256
00:08:14,850 --> 00:08:16,400
比如说当前的一个 Blook

257
00:08:16,600 --> 00:08:17,850
当前 Blook 的路由

258
00:08:17,850 --> 00:08:19,083
如果是随机

259
00:08:19,083 --> 00:08:21,166
那么就可能会引起一个问题

260
00:08:21,166 --> 00:08:23,566
就是违反了因果关系

261
00:08:23,566 --> 00:08:26,200
因为肯定先看到前面的一个文章

262
00:08:26,200 --> 00:08:28,000
才会有后面的内容

263
00:08:28,050 --> 00:08:28,883
所以说路由

264
00:08:28,883 --> 00:08:31,883
不能随机的去选择 QKV 三个的内容

265
00:08:31,883 --> 00:08:32,683
或者 q

266
00:08:32,683 --> 00:08:36,683
不能随机的去选择 KV 两个不同的矩阵

267
00:08:36,683 --> 00:08:38,050
后面的所有的内容

268
00:08:38,050 --> 00:08:39,733
所以这里面就做了一个约束

269
00:08:39,733 --> 00:08:42,283
叫做 causal masking 相关的内容

270
00:08:42,483 --> 00:08:42,933
那这里面

271
00:08:42,933 --> 00:08:45,483
基本上跟 attention Mask 是一样

272
00:08:45,600 --> 00:08:48,166
只是去约束 mixture of Expert 里面

273
00:08:48,166 --> 00:08:50,450
mixture 里面的一个具体的内容

274
00:08:50,450 --> 00:08:52,483
所以把大语言模型的 mask

275
00:08:52,533 --> 00:08:54,483
跟 mixture 相关

276
00:08:54,483 --> 00:08:55,966
进行了一个结合

277
00:08:55,966 --> 00:08:56,566
那最后

278
00:08:56,566 --> 00:08:58,250
还有几个小的内容

279
00:08:58,250 --> 00:08:58,850
不过没关系

280
00:08:58,850 --> 00:08:59,683
先不看

281
00:08:59,683 --> 00:09:02,566
现在正式的来到了左边

282
00:09:02,566 --> 00:09:04,283
那现在来放大来看一下

283
00:09:04,333 --> 00:09:05,766
左边的这个模块

284
00:09:06,250 --> 00:09:07,533
在这篇文章里面

285
00:09:07,533 --> 00:09:08,283
最重要的就是

286
00:09:08,283 --> 00:09:10,050
把整个路由的机制

287
00:09:10,050 --> 00:09:11,450
提前到 Attention 里面

288
00:09:11,450 --> 00:09:12,133
那 Attention

289
00:09:12,133 --> 00:09:13,250
一般都是算

290
00:09:13,250 --> 00:09:16,683
QK v 三个不同的值

291
00:09:16,683 --> 00:09:18,850
那在 q 乘以 k 的过程当中

292
00:09:18,850 --> 00:09:21,850
通过路由去分发到不同的 Blook

293
00:09:22,050 --> 00:09:24,483
然后再计算一个 value

294
00:09:24,483 --> 00:09:24,883
最后

295
00:09:24,883 --> 00:09:27,733
把上就变成第一个 Attention score

296
00:09:27,733 --> 00:09:29,733
就是处理不同的 Blook

297
00:09:29,966 --> 00:09:30,533
同样

298
00:09:30,533 --> 00:09:33,650
第二个 token 也可以选择不同的 block

299
00:09:33,650 --> 00:09:35,766
和不同的一个 block and block

300
00:09:35,800 --> 00:09:38,333
然后形成下一个 attention score

301
00:09:38,333 --> 00:09:39,533
通过这种方式

302
00:09:39,600 --> 00:09:42,600
把一个 mixture 往前提

303
00:09:42,600 --> 00:09:44,000
那以前的 Expert

304
00:09:44,000 --> 00:09:46,333
现在就变成一个 attention

305
00:09:46,333 --> 00:09:47,533
的一个具体的机制

306
00:09:47,533 --> 00:09:50,283
所以叫做 mixture of block

307
00:09:50,333 --> 00:09:52,850
attention 的原因就在这里面

308
00:09:53,733 --> 00:09:55,883
论文当中就把每一个 block

309
00:09:55,883 --> 00:09:58,766
实际上变成 MoE 里面的 x

310
00:09:59,000 --> 00:10:01,483
这种方式来进行一个计算

311
00:10:01,483 --> 00:10:03,850
然后因为整个序列里面

312
00:10:03,850 --> 00:10:06,166
k 跟 v 分成很多 block

313
00:10:06,166 --> 00:10:08,966
所以一个分的过程当中

314
00:10:08,966 --> 00:10:11,733
就按序列长度来进行划分

315
00:10:11,733 --> 00:10:13,333
因此在整篇文章当中

316
00:10:13,333 --> 00:10:15,250
就可以处理很长的序列了

317
00:10:15,250 --> 00:10:16,400
因为序列之间

318
00:10:16,400 --> 00:10:18,483
不再是由一个非常长

319
00:10:18,483 --> 00:10:19,200
或者非常大

320
00:10:19,200 --> 00:10:21,733
一个 k 的矩阵和 v 的矩阵而组成

321
00:10:21,733 --> 00:10:23,333
而是由很多个小

322
00:10:23,333 --> 00:10:23,883
不同

323
00:10:23,883 --> 00:10:27,000
k 的矩阵跟 v 的矩阵进行组合而成

324
00:10:27,400 --> 00:10:29,766
就组成了文章最核心的一个思想

325
00:10:29,766 --> 00:10:32,166
叫做 MoBA MoBA

326
00:10:34,933 --> 00:10:35,733
那现在

327
00:10:35,733 --> 00:10:39,366
回到这篇文章剩下的一个几个小点

328
00:10:39,366 --> 00:10:41,850
就是 fine gain 的一个 block segmentation

329
00:10:42,050 --> 00:10:44,850
也就是对应的细粒度的一个块的切分

330
00:10:44,850 --> 00:10:46,366
所谓的细粒度的块的切分

331
00:10:46,366 --> 00:10:48,000
就是块

332
00:10:48,000 --> 00:10:48,966
到底是在什

333
00:10:48,966 --> 00:10:51,566
么情况下进行一个切分

334
00:10:52,400 --> 00:10:53,483
那在这篇文章里面

335
00:10:53,483 --> 00:10:53,883
作者

336
00:10:53,883 --> 00:10:57,050
就探讨了很多的细粒度切分的方式

337
00:10:57,050 --> 00:10:57,966
最后就发现

338
00:10:57,966 --> 00:10:59,933
其实现在在大语言模型里面

339
00:10:59,933 --> 00:11:02,883
一般输进去整个 Transformer 的计算是 BSH

340
00:11:03,333 --> 00:11:04,533
更多的是在 s

341
00:11:04,533 --> 00:11:06,733
sequence 里面进行一个切分

342
00:11:06,733 --> 00:11:08,566
那通过这种方式去切分

343
00:11:08,566 --> 00:11:10,683
而不是切分 FFN 层

344
00:11:10,683 --> 00:11:11,966
也就是 hidden layer

345
00:11:12,133 --> 00:11:12,966
使得现在

346
00:11:12,966 --> 00:11:15,450
能够处理更长序列的内容了

347
00:11:15,450 --> 00:11:18,366
那这个就是细粒度切分的一个原因

348
00:11:18,366 --> 00:11:19,850
或者它的一个方法

349
00:11:19,883 --> 00:11:20,450
那第二

350
00:11:20,450 --> 00:11:23,683
就是 MoBA 跟一个原来的 Attention 之间

351
00:11:23,683 --> 00:11:24,683
一个混合

352
00:11:24,683 --> 00:11:25,683
和使用

353
00:11:25,683 --> 00:11:27,133
那整个 MoBA 的结构

354
00:11:27,133 --> 00:11:28,400
可以对 Attention 结构

355
00:11:28,400 --> 00:11:29,400
进行一个替换

356
00:11:29,400 --> 00:11:30,933
就完全直接替换

357
00:11:30,933 --> 00:11:32,933
使得在训练的过程当中

358
00:11:33,083 --> 00:11:35,733
不用像 DeepSeek 的一个 NSA 一样

359
00:11:35,733 --> 00:11:37,800
需要全部重新进行训练

360
00:11:37,800 --> 00:11:40,766
因为整个挑选的结构都已经改变了

361
00:11:40,766 --> 00:11:43,166
那 MoBA 这里面的设计就做得非常好

362
00:11:43,166 --> 00:11:44,400
它可以做一个替换

363
00:11:44,650 --> 00:11:46,283
因为只是进行了一个分 Blook

364
00:11:46,283 --> 00:11:47,566
然后加了一个路由

365
00:11:47,566 --> 00:11:49,333
有些参数我可以不用用

366
00:11:49,333 --> 00:11:51,000
所以说这里面整个 MoBA

367
00:11:51,000 --> 00:11:52,166
它可以对 LLAMA

368
00:11:52,166 --> 00:11:55,050
千万的这些模型进行重新的微调

369
00:11:55,283 --> 00:11:56,166
或者基于以前

370
00:11:56,166 --> 00:11:59,000
Kimi 之前训练的超大规模的大模型

371
00:11:59,000 --> 00:12:00,333
进行一个替换

372
00:12:00,333 --> 00:12:02,800
这个替换就非常的自然和顺滑

373
00:12:02,850 --> 00:12:03,850
那这面还说了

374
00:12:03,850 --> 00:12:05,333
它其实也对比了很多

375
00:12:05,333 --> 00:12:07,166
之前人家的一些工作

376
00:12:07,166 --> 00:12:08,650
包括 sliding in Windows

377
00:12:08,650 --> 00:12:09,800
attention head

378
00:12:09,800 --> 00:12:13,200
还有一个 attention sink 了各种各样的工作

379
00:12:13,400 --> 00:12:14,533
但是其他的工作

380
00:12:14,533 --> 00:12:18,000
还是没有在整个 MoBA 的一个泛化性好

381
00:12:18,050 --> 00:12:19,933
所以说 MoBA 的一个泛化性

382
00:12:19,933 --> 00:12:21,366
确实做的非常的好

383
00:12:21,400 --> 00:12:23,200
那做的这么好

384
00:12:23,200 --> 00:12:24,800
具体是怎么去实现

385
00:12:24,800 --> 00:12:28,050
于是就来到了 2.3 的这个工作

386
00:12:28,050 --> 00:12:31,450
implementation 对应的实现的方案

387
00:12:31,850 --> 00:12:33,283
在这个实行方案里面

388
00:12:33,283 --> 00:12:34,050
就说到了

389
00:12:34,050 --> 00:12:36,166
其实最重要的还是五个步骤

390
00:12:36,166 --> 00:12:36,800
第一个步骤

391
00:12:36,800 --> 00:12:39,250
就是去根据门控的网络

392
00:12:39,250 --> 00:12:41,166
和因果的编码了

393
00:12:41,166 --> 00:12:45,333
确定现在 q 所对应的 KV 的 block

394
00:12:45,333 --> 00:12:48,600
也就是现在需要去确定查询

395
00:12:48,600 --> 00:12:50,483
k token 对应的 KVB

396
00:12:50,566 --> 00:12:51,650
对应的分配

397
00:12:51,650 --> 00:12:53,650
也就是先分配 block

398
00:12:53,650 --> 00:12:56,000
对一个 seq 进行切分

399
00:12:56,083 --> 00:12:57,883
那接着就第二步了

400
00:12:57,883 --> 00:12:59,483
第二步就根据所分配

401
00:12:59,483 --> 00:13:01,166
也是第一步分配的 KVB

402
00:13:01,250 --> 00:13:05,000
KV 的 block 安排出查询的顺序

403
00:13:05,000 --> 00:13:06,933
也是 q 的一个具体的顺序

404
00:13:06,933 --> 00:13:09,166
那有了相关的顺序分配之后

405
00:13:09,450 --> 00:13:10,050
第三步

406
00:13:10,050 --> 00:13:12,083
就是根据 q 的 token

407
00:13:12,083 --> 00:13:14,450
去计算了每一个 kv block

408
00:13:14,450 --> 00:13:17,966
也就是最终的第三步这个内容了

409
00:13:18,000 --> 00:13:19,800
根据 q 的一个 seq 词

410
00:13:19,800 --> 00:13:21,733
去切换 k 跟 v

411
00:13:21,766 --> 00:13:23,133
k 跟 v 切完之后

412
00:13:23,133 --> 00:13:26,250
就计算每一个小 block 里面的一个 attention

413
00:13:26,566 --> 00:13:27,766
score 那算完之后

414
00:13:27,766 --> 00:13:29,366
就结束了第三步

415
00:13:29,366 --> 00:13:31,800
那接着就来到了第四步了

416
00:13:31,800 --> 00:13:35,083
第四步就是注意力机制的一个输出

417
00:13:35,083 --> 00:13:37,883
从排为原始的序列

418
00:13:37,883 --> 00:13:39,000
那这个蛮重要

419
00:13:39,000 --> 00:13:40,533
就是还是要重排

420
00:13:40,533 --> 00:13:43,000
因为刚才已经切换到很多个 block 了

421
00:13:43,000 --> 00:13:44,333
block 之间是割裂

422
00:13:44,333 --> 00:13:46,200
所以现在要进行个重排

423
00:13:46,200 --> 00:13:46,850
把它 connect

424
00:13:46,850 --> 00:13:48,283
回一起那最后

425
00:13:48,283 --> 00:13:49,933
就是 Softmax 的一个合并

426
00:13:49,933 --> 00:13:51,483
注意力机制的输出了

427
00:13:51,483 --> 00:13:54,166
因为整个 token 其实是打散

428
00:13:54,166 --> 00:13:55,683
签了很多的不同的 Blook

429
00:13:55,683 --> 00:13:56,283
所以最后

430
00:13:56,283 --> 00:13:59,283
还是要进行一个最终的 Softmax 的合并

431
00:13:59,283 --> 00:14:01,166
那这面就讲完 5 个步骤

432
00:14:01,166 --> 00:14:04,533
实现了 implementation 相关的内容

433
00:14:04,533 --> 00:14:07,200
那整个 MoBA 的一个算法的原理

434
00:14:07,200 --> 00:14:07,883
就在这里面

435
00:14:07,883 --> 00:14:09,733
简单的去看一下

436
00:14:09,883 --> 00:14:12,683
跟大家一起简单的做一个走读

437
00:14:13,083 --> 00:14:14,250
整个文章的算法

438
00:14:14,250 --> 00:14:15,650
或者整个 MoBA 的算法

439
00:14:15,650 --> 00:14:16,366
实现起来

440
00:14:16,366 --> 00:14:17,400
并没有那么难

441
00:14:17,483 --> 00:14:17,933
首先

442
00:14:17,933 --> 00:14:20,650
首要有一个 QKV 相关的矩阵

443
00:14:20,650 --> 00:14:22,683
那设置了 MoBA 的一个超参

444
00:14:22,683 --> 00:14:25,333
特别是 block size l TOPK 的一个数量

445
00:14:25,400 --> 00:14:28,566
接着还要需要去设置 h 跟 d

446
00:14:28,566 --> 00:14:30,250
那 h 就是 attention head

447
00:14:30,450 --> 00:14:32,283
那第一就是 head 的定了

448
00:14:32,283 --> 00:14:33,333
head 的维度

449
00:14:33,333 --> 00:14:34,883
同样的现在来去

450
00:14:34,883 --> 00:14:37,450
需要去计算有多少升个 block

451
00:14:37,450 --> 00:14:38,850
也就是 n 的大小

452
00:14:38,850 --> 00:14:39,883
所以 n 除以 b

453
00:14:40,133 --> 00:14:41,333
然后就是一个

454
00:14:41,333 --> 00:14:43,000
具体有多少个 block 了

455
00:14:43,000 --> 00:14:45,450
那算完相关超参之后

456
00:14:45,450 --> 00:14:46,966
就来到了第一步了

457
00:14:46,966 --> 00:14:49,050
第一步就是 split KV into block

458
00:14:49,050 --> 00:14:52,283
也对 KV 进行一个分块

459
00:14:52,283 --> 00:14:53,166
分完块之后

460
00:14:53,166 --> 00:14:56,283
就存在一个 k 跟 v 的队列里面

461
00:14:56,533 --> 00:14:57,133
切换完之后

462
00:14:57,133 --> 00:15:00,283
就需要去计算每一个动态的 Blook

463
00:15:00,283 --> 00:15:03,166
对应门控的分数得分

464
00:15:03,166 --> 00:15:04,366
有了这个内容之后

465
00:15:04,366 --> 00:15:06,000
就开始去选择

466
00:15:06,000 --> 00:15:08,166
有因果关系对应的 Blook 了

467
00:15:08,166 --> 00:15:09,200
也就往前选

468
00:15:09,200 --> 00:15:10,400
不去选一些

469
00:15:10,400 --> 00:15:12,366
未来就没有遇到过的一些

470
00:15:12,366 --> 00:15:12,966
Blook 了

471
00:15:12,966 --> 00:15:15,250
因为这里面跟那个 Attention 机制

472
00:15:15,250 --> 00:15:16,533
其实比较像

473
00:15:16,650 --> 00:15:18,166
那之前的 MoE 的结构

474
00:15:18,166 --> 00:15:20,050
会放在 FFN 层

475
00:15:20,050 --> 00:15:22,483
是因为它不需要考虑到上下文的问题

476
00:15:22,483 --> 00:15:24,850
因为上下文在 QKV 计算的时候

477
00:15:24,850 --> 00:15:25,850
已经处理完了

478
00:15:25,850 --> 00:15:28,000
那现在把 MoE

479
00:15:28,000 --> 00:15:30,966
特别是把 m Multi of Expert 往前提到

480
00:15:30,966 --> 00:15:32,200
而 Attention 机制的时候

481
00:15:32,200 --> 00:15:34,450
所以需要对 mask

482
00:15:34,450 --> 00:15:35,483
进行一个处理

483
00:15:35,600 --> 00:15:36,250
处理完之后

484
00:15:36,250 --> 00:15:38,733
就来到了第 10 跟 12 行

485
00:15:38,733 --> 00:15:39,333
对应

486
00:15:39,333 --> 00:15:43,533
去计算 QKV 相关的一个 sub block 相关的内容

487
00:15:43,733 --> 00:15:44,483
那最后

488
00:15:44,483 --> 00:15:46,200
计算整个 Attention 之后

489
00:15:46,200 --> 00:15:48,600
就是 13 跟 14 行也计算完之后

490
00:15:48,600 --> 00:15:49,733
就整合起来

491
00:15:49,733 --> 00:15:51,333
然后经过一个 Softmax

492
00:15:51,400 --> 00:15:53,850
把所有的答案都 combine 起来

493
00:15:54,166 --> 00:15:55,600
所有之前分

494
00:15:55,600 --> 00:15:57,133
或者切换完之后的 Blook

495
00:15:57,133 --> 00:15:59,200
合并完之后就会称 o

496
00:15:59,250 --> 00:16:01,133
所以整体的算法比较简单

497
00:16:01,133 --> 00:16:01,850
去替换

498
00:16:01,850 --> 00:16:05,283
传统的 attention 结构也是非常的丝滑

499
00:16:05,283 --> 00:16:08,566
那整篇文章的原理基本上介绍完了

500
00:16:08,566 --> 00:16:09,800
哎呦 ZOMI

501
00:16:09,800 --> 00:16:12,533
突然现在想到一篇文章蛮有意思

502
00:16:12,533 --> 00:16:14,333
现在打开另外一篇文章

503
00:16:14,333 --> 00:16:18,050
跟大家一起去看看的 a few moments later

504
00:16:18,766 --> 00:16:19,400
这篇文章

505
00:16:19,400 --> 00:16:21,966
就是 Shazeer 在 2022 年的时候发布

506
00:16:21,966 --> 00:16:23,683
一个 switch Transformer 嘛

507
00:16:23,683 --> 00:16:26,283
当时 ZOMI 就解读了一下 switch Transformer 嘛

508
00:16:26,283 --> 00:16:28,400
因为整个 switch Transformer

509
00:16:28,400 --> 00:16:31,483
在 MoE 加工里面是非常的核心

510
00:16:31,483 --> 00:16:33,933
那其实那篇文章看一下这个图

511
00:16:33,933 --> 00:16:35,000
最重要的就是

512
00:16:35,000 --> 00:16:37,850
在整个 Transformer 加工里面

513
00:16:37,850 --> 00:16:38,683
去引入了

514
00:16:38,683 --> 00:16:41,200
或者把整个 FFN 成了替换

515
00:16:41,200 --> 00:16:43,483
成为 MoE 这个 Export 的架构

516
00:16:43,533 --> 00:16:46,333
但是这篇论文里面的 experience

517
00:16:46,450 --> 00:16:48,366
或者里面的最后的副录

518
00:16:48,366 --> 00:16:49,250
看一下

519
00:16:49,283 --> 00:16:51,733
他其实做了一个蛮有意思的工作

520
00:16:51,966 --> 00:16:54,600
就是在整个 self attention 的时候

521
00:16:54,600 --> 00:16:57,333
其实引入了一个路由的专家

522
00:16:57,333 --> 00:16:58,283
但是这个

523
00:16:58,283 --> 00:17:01,000
为什么会放在一个里面

524
00:17:01,000 --> 00:17:02,283
是因为 switch Transformer

525
00:17:02,283 --> 00:17:04,083
就 Shazeer 发现了这种效果

526
00:17:04,083 --> 00:17:05,483
其实并没有那么的好

527
00:17:05,483 --> 00:17:07,000
就是刚才讲到

528
00:17:07,250 --> 00:17:10,166
其实会引入了一些因果关系的内容

529
00:17:10,200 --> 00:17:11,883
所以为了解决这个内容

530
00:17:11,933 --> 00:17:14,533
整个 Kimi 杨植麟团队

531
00:17:14,533 --> 00:17:17,733
就重新引入了一个因果关系的 mask

532
00:17:17,733 --> 00:17:20,050
通过这个 Max 去处理掉这个问题

533
00:17:20,050 --> 00:17:21,883
所以说提升了一个性能

534
00:17:21,883 --> 00:17:25,050
真正的把 Shazeer 这个思想的原理 idea

535
00:17:25,083 --> 00:17:27,533
应用到自己的一个应用领域里面

536
00:17:28,333 --> 00:17:29,283
所以现在

537
00:17:29,283 --> 00:17:30,366
回头看一下

538
00:17:30,366 --> 00:17:32,450
整个 MoBA 的一个思想的原理

539
00:17:32,450 --> 00:17:33,450
会发现

540
00:17:33,483 --> 00:17:34,800
哎呦蛮有意思

541
00:17:34,800 --> 00:17:36,333
之前 22 年的工作

542
00:17:36,333 --> 00:17:38,366
被实现出来了

543
00:17:38,366 --> 00:17:40,800
或者 22 年的一个探索的尝试

544
00:17:40,800 --> 00:17:43,883
被真正的通过新的算法的创新

545
00:17:43,883 --> 00:17:46,366
能够把之前的算法实现掉

546
00:17:46,400 --> 00:17:47,800
那现在来看一下

547
00:17:47,800 --> 00:17:51,850
整个文章所相关的一些 experiences 了

548
00:17:51,850 --> 00:17:52,450
那最后

549
00:17:52,450 --> 00:17:53,733
其实主要是 experience 了

550
00:17:53,733 --> 00:17:54,933
简单的过一过

551
00:17:54,933 --> 00:17:55,766
那左边

552
00:17:55,766 --> 00:17:58,083
放大来看看这两个图

553
00:17:58,133 --> 00:17:58,650
蛮有意思

554
00:17:58,650 --> 00:18:00,483
就是随着序列的长度

555
00:18:00,483 --> 00:18:01,366
越来越长

556
00:18:01,650 --> 00:18:03,650
以前用 fashion attention 的时候

557
00:18:03,650 --> 00:18:05,333
你就会发现计算的时间

558
00:18:05,333 --> 00:18:06,533
其实是在增加

559
00:18:06,533 --> 00:18:07,733
因为序列越长

560
00:18:07,733 --> 00:18:10,400
fashion attention 计算的时候越来越难

561
00:18:10,400 --> 00:18:12,733
但是我通过整个 m

562
00:18:12,733 --> 00:18:13,600
就是 mission

563
00:18:13,650 --> 00:18:15,083
然后把 QK 变

564
00:18:15,083 --> 00:18:17,133
成 ASP 进行个切分

565
00:18:17,133 --> 00:18:18,883
使得处理序列的时候

566
00:18:18,883 --> 00:18:20,050
就越来越方便了

567
00:18:20,050 --> 00:18:22,600
反正利用大规模的分布式的能力

568
00:18:22,800 --> 00:18:24,933
去解决了长序列的问题

569
00:18:24,933 --> 00:18:26,000
那这个蛮有意思

570
00:18:26,000 --> 00:18:27,133
就是之前

571
00:18:27,133 --> 00:18:29,883
会出现了很多各种各样的 EP 也好

572
00:18:29,883 --> 00:18:31,600
各种各样的分布式并行

573
00:18:31,650 --> 00:18:33,683
说实话分布式并行的优化

574
00:18:33,683 --> 00:18:35,850
还不如算法跟硬件

575
00:18:35,850 --> 00:18:38,766
或者算法的一个新的创新

576
00:18:39,250 --> 00:18:39,283
那

577
00:18:39,283 --> 00:18:41,850
看完刚才那个长序列的一个性能情况

578
00:18:41,850 --> 00:18:42,050
之后

579
00:18:42,050 --> 00:18:44,933
再看一下其他的几个相关的实验

580
00:18:45,000 --> 00:18:46,566
蛮有意思的就是这里面

581
00:18:46,566 --> 00:18:49,483
还有一个 scaling low 的对比的实验哦

582
00:18:50,283 --> 00:18:52,650
那从图 3A 左边可以看到

583
00:18:52,650 --> 00:18:53,483
full attention

584
00:18:53,483 --> 00:18:55,850
跟 MoBA 的一个 scaling 的曲式

585
00:18:55,850 --> 00:18:57,050
是非常的像

586
00:18:57,050 --> 00:18:59,566
所以蓝色跟绿色的这两条曲线

587
00:18:59,600 --> 00:19:01,083
基本上一致

588
00:19:01,083 --> 00:19:02,850
那这个实验就表明

589
00:19:02,850 --> 00:19:05,333
尽管 MoBA 的一个稀疏注意力机制

590
00:19:05,566 --> 00:19:07,966
稀疏程度已经高达了 75%了

591
00:19:07,966 --> 00:19:10,733
但是它的实现跟完全注意力机制

592
00:19:10,733 --> 00:19:12,366
也就是 full attention 机制

593
00:19:12,450 --> 00:19:13,850
取得了相同

594
00:19:13,850 --> 00:19:16,366
或者相类似的一个 scaling 的效果

595
00:19:16,600 --> 00:19:18,250
所以说 MoBA 这个效果

596
00:19:18,250 --> 00:19:19,766
或者这个模型的算法

597
00:19:19,766 --> 00:19:20,683
非常的好

598
00:19:20,683 --> 00:19:21,450
那另外的话

599
00:19:21,450 --> 00:19:23,650
还做了一个长序列的验证

600
00:19:23,650 --> 00:19:25,766
就 sequence 等于 32K

601
00:19:25,850 --> 00:19:28,483
刚才左边的 asequence 是 8K

602
00:19:28,483 --> 00:19:29,933
那 sequence

603
00:19:29,933 --> 00:19:32,083
最大序列长度降低了

604
00:19:32,083 --> 00:19:32,683
但是

605
00:19:32,683 --> 00:19:35,600
从经过了五个实验会发现

606
00:19:35,600 --> 00:19:38,000
整体 MoBA 最后的块的一个损失

607
00:19:38,000 --> 00:19:41,083
跟 full attention 的时候是比较类似

608
00:19:41,083 --> 00:19:42,483
可能略高了一点

609
00:19:42,483 --> 00:19:45,000
但是整体的损失值也在降低

610
00:19:45,000 --> 00:19:45,800
那这个实验

611
00:19:45,800 --> 00:19:46,483
就说明了

612
00:19:46,483 --> 00:19:49,600
MoBA 具有长下象纹的一个扩展性

613
00:19:49,850 --> 00:19:52,333
它的泛化性是非常的好

614
00:19:52,333 --> 00:19:54,733
而且后面还做了很多相关

615
00:19:54,733 --> 00:19:56,850
一个消融的实验

616
00:19:56,850 --> 00:19:59,333
对 MoBA 的细粒度的切换的块

617
00:19:59,333 --> 00:20:02,483
进行了有效性的一个具体的验证

618
00:20:02,850 --> 00:20:05,166
那继续往下看一下这个图

619
00:20:05,166 --> 00:20:06,800
那这个图可以看到

620
00:20:06,966 --> 00:20:08,966
虽然只是使用了 MoBA 时候

621
00:20:08,966 --> 00:20:11,050
会导致 Token 的位置

622
00:20:11,050 --> 00:20:14,600
损失度可能会略高一点点

623
00:20:14,933 --> 00:20:15,250
不过

624
00:20:15,250 --> 00:20:17,600
使用的 MoBA 跟 attention 的混合的方案

625
00:20:17,600 --> 00:20:20,200
具体的一个训练的时候的损失方案

626
00:20:20,283 --> 00:20:21,650
基本上一致

627
00:20:21,650 --> 00:20:23,083
那这一结果就表明

628
00:20:23,083 --> 00:20:25,400
整个模版的一个使用的方式

629
00:20:25,400 --> 00:20:27,000
或者整个模版的一个切换

630
00:20:27,000 --> 00:20:28,133
是非常的好

631
00:20:28,133 --> 00:20:31,133
反正我可以对 attention 进行一个平替

632
00:20:31,450 --> 00:20:32,566
即使平替的时候

633
00:20:32,566 --> 00:20:34,200
可能会损失了一点精度

634
00:20:34,200 --> 00:20:36,800
那也可以混合的替换

635
00:20:36,800 --> 00:20:38,683
或者有序的进行一个替换

636
00:20:38,683 --> 00:20:39,133
反正

637
00:20:39,133 --> 00:20:41,483
就证明整个 MoBA 的一个模型的效果

638
00:20:41,483 --> 00:20:42,533
是非常好

639
00:20:42,533 --> 00:20:45,483
平滑切换起来了也非常的丝滑

640
00:20:45,483 --> 00:20:48,450
那最后往下看一下还有什么内容

641
00:20:48,450 --> 00:20:50,200
那最后还是一些简单

642
00:20:50,200 --> 00:20:52,683
Benchmark 的一个尝试和探索

643
00:20:52,966 --> 00:20:53,600
整个实验

644
00:20:53,600 --> 00:20:56,766
使用了 8B 的一个 LLAMA 进行一个魔改

645
00:20:56,766 --> 00:20:58,683
也就把之前的一个 Attention 机制

646
00:20:58,683 --> 00:21:00,566
换成 MoBA 的机制

647
00:21:00,566 --> 00:21:02,566
还有在整个大海捞针

648
00:21:02,566 --> 00:21:05,250
一个长序列的测评里面

649
00:21:05,250 --> 00:21:07,766
取得了非常好的效果

650
00:21:07,766 --> 00:21:08,400
那最后

651
00:21:08,400 --> 00:21:11,366
就相关的一些工作还有一些引用了

652
00:21:11,366 --> 00:21:14,533
那这篇论文跟大家解读的差不多

653
00:21:15,400 --> 00:21:17,883
现在来到了第二个内容

654
00:21:17,883 --> 00:21:18,766
回到 PPT

655
00:21:18,766 --> 00:21:19,800
看一下 Kimi 的

656
00:21:19,800 --> 00:21:22,400
MoBA 跟 DeepSeek 的一个 NSA

657
00:21:22,400 --> 00:21:24,000
因为是同一天发布的嘛

658
00:21:24,000 --> 00:21:25,800
它们之间的一个区别

659
00:21:27,133 --> 00:21:28,533
其实网上都说了

660
00:21:28,533 --> 00:21:29,733
Kimi

661
00:21:29,733 --> 00:21:32,050
跟 DeepSeek 的一个梁文峰

662
00:21:32,050 --> 00:21:33,283
在同一天

663
00:21:33,283 --> 00:21:35,883
都撞车的发布了相关的文章

664
00:21:35,883 --> 00:21:36,400
但是

665
00:21:36,400 --> 00:21:38,683
同样的提到了一个非常关键的词

666
00:21:38,683 --> 00:21:41,766
就是长文本等的注意力机制

667
00:21:41,766 --> 00:21:43,800
也就是 long context 的一个 Attention

668
00:21:43,800 --> 00:21:44,683
相关的内容

669
00:21:44,683 --> 00:21:45,483
那这里面

670
00:21:45,483 --> 00:21:47,966
为了去更好的解读两个文章

671
00:21:47,966 --> 00:21:49,083
相关的内容

672
00:21:49,400 --> 00:21:51,450
又做了一个多维度的比较

673
00:21:51,483 --> 00:21:54,400
首先第一个就是 DeepSeek 的一个 NSA

674
00:21:54,400 --> 00:21:57,450
中文就是 native spares the attention

675
00:21:57,450 --> 00:22:00,133
也就是原始的稀疏的 attention 架构

676
00:22:00,133 --> 00:22:00,733
那这里面

677
00:22:00,733 --> 00:22:03,250
就是一个 block 的 attention

678
00:22:03,250 --> 00:22:04,683
的相关的架构

679
00:22:04,850 --> 00:22:05,483
那整体来说

680
00:22:05,483 --> 00:22:08,000
先看看 NSA 有什么区别

681
00:22:08,050 --> 00:22:09,450 
那 NSA 蛮有意思

682
00:22:09,450 --> 00:22:12,133
就是原生的稀疏注意力机制

683
00:22:12,133 --> 00:22:14,250
能够跟一个长文本结合起来

684
00:22:14,250 --> 00:22:17,166
也就原生实现了一个稀疏的注意力

685
00:22:17,166 --> 00:22:18,166
那最重要的就是

686
00:22:18,166 --> 00:22:20,133
处理长文本的问题

687
00:22:20,133 --> 00:22:22,133
那试剂是比较纯粹

688
00:22:22,133 --> 00:22:23,200
真正的从训练

689
00:22:23,200 --> 00:22:25,450
推理原始的进行一个开始

690
00:22:25,450 --> 00:22:27,000
所以说它整体的训练

691
00:22:27,000 --> 00:22:28,166
是端到端

692
00:22:28,333 --> 00:22:29,566
截止到现在为止

693
00:22:29,566 --> 00:22:30,366
或者今天为止

694
00:22:30,366 --> 00:22:31,566
其实没有开源

695
00:22:31,566 --> 00:22:33,083
而且整个实现的方案

696
00:22:33,083 --> 00:22:35,200
是高度的去依赖于硬件

697
00:22:35,200 --> 00:22:38,133
也就是真正做到软硬件协同的优化

698
00:22:38,133 --> 00:22:40,283
但是开发难度也就比较高

699
00:22:40,283 --> 00:22:40,600
说实话

700
00:22:40,600 --> 00:22:43,050
它里面自己首录了很多相关的算子

701
00:22:43,166 --> 00:22:45,733
特别是魔改了 fashion attention 相关的内容

702
00:22:45,733 --> 00:22:47,566
最终来去实现

703
00:22:47,566 --> 00:22:48,483
那看一下

704
00:22:48,483 --> 00:22:49,933
对比 MoBA

705
00:22:49,933 --> 00:22:51,366
就是 Kimi

706
00:22:51,366 --> 00:22:52,000
最重要的就是

707
00:22:52,000 --> 00:22:54,566
它基于块的混合注意机制

708
00:22:54,566 --> 00:22:56,366
所以里面的 b 就是 Blook

709
00:22:56,366 --> 00:22:57,600
通过一个门控网络

710
00:22:57,600 --> 00:23:00,250
动态的去选择相关的块

711
00:23:00,250 --> 00:23:03,766
而把 m 就是一个 mission of Expert

712
00:23:04,050 --> 00:23:05,133
里面的专家

713
00:23:05,133 --> 00:23:05,966
MV 的架构

714
00:23:05,966 --> 00:23:07,600
用在探寻里面

715
00:23:07,766 --> 00:23:10,683
同样的也是处理一个长序列

716
00:23:10,683 --> 00:23:11,533
不过蛮有意思

717
00:23:11,533 --> 00:23:14,250
就是跟现有的一个全注意力机制

718
00:23:14,250 --> 00:23:15,533
Transformer 架构

719
00:23:15,533 --> 00:23:17,450
可以天然的无缝的集成

720
00:23:17,483 --> 00:23:19,733
所以不需要大量的重新训练

721
00:23:19,733 --> 00:23:20,850
就能够实现了

722
00:23:20,850 --> 00:23:23,766
那猪米觉得这可能是很大的一个差异

723
00:23:24,083 --> 00:23:26,333
那可能 Kimi 就直接开源了

724
00:23:26,333 --> 00:23:27,766
那有没有未来

725
00:23:27,933 --> 00:23:29,850
DeepSeek 的一个 NSA 开源

726
00:23:29,850 --> 00:23:31,883
猪米觉得也是蛮有可能

727
00:23:31,883 --> 00:23:32,650
不过 Kimi

728
00:23:32,650 --> 00:23:35,000
因为它说可以无缝的衔接

729
00:23:35,000 --> 00:23:36,450
其实它直接集成着

730
00:23:36,450 --> 00:23:38,250
hunger face 的一个 Transformer 的套件

731
00:23:38,366 --> 00:23:40,800
所以说可以块速灵活的进行一个替换

732
00:23:41,283 --> 00:23:42,200
那硬件程度

733
00:23:42,200 --> 00:23:42,650
没有意思

734
00:23:42,650 --> 00:23:44,883
这个差别是非常的大

735
00:23:44,883 --> 00:23:46,883
整个 MoBA 的一个开源链接里面

736
00:23:46,883 --> 00:23:49,450
实际上是没有首入对应的算子

737
00:23:49,450 --> 00:23:53,000
可以进行一个很块速的一个替换

738
00:23:53,000 --> 00:23:54,250
而且刚才讲到了

739
00:23:54,250 --> 00:23:56,333
可以进行一个无缝的衔接

740
00:23:56,733 --> 00:23:58,333
对硬件的依赖比较低

741
00:23:58,366 --> 00:23:59,333
那这里面

742
00:23:59,333 --> 00:24:00,600
Deepsick 的 NSA

743
00:24:00,600 --> 00:24:02,133
因为工程能力特别的好

744
00:24:02,133 --> 00:24:03,650
对硬件的依赖比较高

745
00:24:03,650 --> 00:24:05,683
但是提速也比较块

746
00:24:06,000 --> 00:24:07,966
最后就是集成的难度了

747
00:24:07,966 --> 00:24:09,733
那集成难度跟开发难度

748
00:24:09,733 --> 00:24:11,766
确实 NSA 相对难一点

749
00:24:11,766 --> 00:24:13,083
MoBA 相对易一点

750
00:24:13,083 --> 00:24:14,766
但是对于开发者来说

751
00:24:14,766 --> 00:24:17,683
可能 MoBA 其实易用性更好

752
00:24:17,683 --> 00:24:19,283
或者可替换性更好

753
00:24:19,283 --> 00:24:20,083
那未来

754
00:24:20,083 --> 00:24:21,483
可能选择哪种方案

755
00:24:21,483 --> 00:24:22,733
就要看 NSA

756
00:24:22,766 --> 00:24:23,683
要不要开源

757
00:24:23,683 --> 00:24:25,050
然后 MoBA

758
00:24:25,050 --> 00:24:27,766
一个用户或者开发者的选择了

759
00:24:27,933 --> 00:24:29,166
不过相同之处

760
00:24:29,166 --> 00:24:32,250
他们都是去处理在 TTS

761
00:24:32,250 --> 00:24:33,733
不是偷偷爽

762
00:24:33,800 --> 00:24:35,733
是 text time scaling

763
00:24:35,883 --> 00:24:37,366
处理 text time scaling 的时候

764
00:24:37,366 --> 00:24:39,250
可能会遇到非常长文本

765
00:24:39,250 --> 00:24:40,650
也就是 reasoning 的模型

766
00:24:40,650 --> 00:24:41,966
推理的模型

767
00:24:41,966 --> 00:24:44,166
会吐非常多的自问自答

768
00:24:44,733 --> 00:24:46,850
最后大家的一个准确率

769
00:24:46,850 --> 00:24:49,533
在大海捞针里面都非常的高

770
00:24:49,533 --> 00:24:50,766
这个就是多维度

771
00:24:50,766 --> 00:24:53,933
对两个模型进行比较的比较

772
00:24:53,933 --> 00:24:54,283
暗中

773
00:24:54,283 --> 00:24:56,566
还要看一下他们的相同点

774
00:24:56,566 --> 00:24:58,200
相同点刚才讲到了

775
00:24:58,200 --> 00:25:01,050
反正就是在 TTS 偷偷爽

776
00:25:01,050 --> 00:25:03,200
就是 testime scaling 里面

777
00:25:03,450 --> 00:25:05,400
去解决长序列的问题

778
00:25:05,483 --> 00:25:08,533
大家都是用了稀疏的注意力

779
00:25:08,533 --> 00:25:10,966
虽然大家的 idea 跟想法不一样

780
00:25:10,966 --> 00:25:12,450
但是很多东西

781
00:25:12,450 --> 00:25:13,566
或者技术路线

782
00:25:13,566 --> 00:25:15,083
都是殊途同归

783
00:25:15,483 --> 00:25:17,683
那为什么会用这个技术路线

784
00:25:17,683 --> 00:25:20,366
是因为在现在的一个 Reasoning 的模型

785
00:25:20,366 --> 00:25:21,966
更多的是通过 Reasoning

786
00:25:22,050 --> 00:25:24,200
去处理长序列

787
00:25:24,200 --> 00:25:25,766
所以他们的一个目标

788
00:25:25,766 --> 00:25:29,050
跟适用的场景都是非常的像

789
00:25:29,166 --> 00:25:30,766
最后看一下区别点

790
00:25:31,166 --> 00:25:32,850
区别点还是蛮重

791
00:25:32,966 --> 00:25:36,133
首先目标 NSA 主要是 Reasoning 的模型

792
00:25:36,133 --> 00:25:36,883
而 MoBA

793
00:25:36,883 --> 00:25:40,450
更多的是针对长序列进行一个处理

794
00:25:40,883 --> 00:25:41,250
第二个

795
00:25:41,250 --> 00:25:42,883
就是看一下他们的技术路线了

796
00:25:42,883 --> 00:25:44,733
技术力还是天差地别

797
00:25:44,733 --> 00:25:47,533
一个 NSA 是原生的稀疏注意力

798
00:25:47,533 --> 00:25:49,533
就是重新发明了一种

799
00:25:49,533 --> 00:25:50,966
新的注意力机制

800
00:25:51,083 --> 00:25:51,733
MoBA

801
00:25:51,733 --> 00:25:54,483
就是把注意力机制进行划分

802
00:25:54,483 --> 00:25:56,200
块进行一个实现

803
00:25:56,200 --> 00:25:58,283
那这种方式可能会更天然如

804
00:25:58,283 --> 00:25:59,050
果这么选的话

805
00:25:59,050 --> 00:26:00,200
可能短期内

806
00:26:00,200 --> 00:26:00,966
选择 MoBA

807
00:26:00,966 --> 00:26:03,483
可能是一个更好的技术路线方向

808
00:26:03,483 --> 00:26:05,333
或者把 deepSeek V3

809
00:26:05,400 --> 00:26:07,166
使用 MoBA 来去提速

810
00:26:07,366 --> 00:26:08,683
进一步的稀疏

811
00:26:08,683 --> 00:26:10,933
进一步的加速模型的效果

812
00:26:10,933 --> 00:26:13,000
或者提升模型的序列的长度

813
00:26:13,000 --> 00:26:15,133
那这也是一种很好的方案哦

814
00:26:15,333 --> 00:26:16,133
那另外的话

815
00:26:16,133 --> 00:26:18,600
其实刚才讲到了他们两个方案

816
00:26:18,600 --> 00:26:19,533
跟目标

817
00:26:19,533 --> 00:26:20,566
其实比较像

818
00:26:20,566 --> 00:26:22,483
一个专门针对逻辑推理

819
00:26:22,483 --> 00:26:24,533
一个专门针对长文本

820
00:26:24,766 --> 00:26:25,600
相对来说

821
00:26:25,600 --> 00:26:26,883
其实差异比较大

822
00:26:26,883 --> 00:26:29,000
就是这里面的开发跟部署了

823
00:26:29,000 --> 00:26:31,683
整体 NSA 的开发难度是非常的高

824
00:26:31,683 --> 00:26:34,600
所以比较依赖于 NSA 要不要开源

825
00:26:34,600 --> 00:26:36,933
那 MoBA 的它的替换能度

826
00:26:36,933 --> 00:26:37,850
比较灵活

827
00:26:37,850 --> 00:26:38,883
设备性也比较高

828
00:26:38,883 --> 00:26:40,650
而且开发难度相对比较低

829
00:26:40,733 --> 00:26:44,366
现在 Kimi 也对 MoBA 进行一个开源

830
00:26:44,450 --> 00:26:45,733
所以说整体

831
00:26:45,733 --> 00:26:47,883
如果短期内块速的去实现

832
00:26:47,883 --> 00:26:50,400
或者块速的复现的看 MoBA 的架构

833
00:26:50,400 --> 00:26:53,133
可能会更加的通人性化

834
00:26:54,166 --> 00:26:55,166
在第三个内容

835
00:26:55,166 --> 00:26:55,800
看一下

836
00:26:55,800 --> 00:26:57,283
做一个简单的小实验

837
00:26:57,283 --> 00:27:00,283
来看一下整个 MoBA 是怎么去申请

838
00:27:00,600 --> 00:27:03,083
打开 Kimi 的一个官网

839
00:27:03,083 --> 00:27:05,450
这里面其实现在还用不到

840
00:27:05,450 --> 00:27:06,850
主要是他在内测所

841
00:27:06,850 --> 00:27:09,766
看一下 long Kimi

842
00:27:09,766 --> 00:27:12,883
就是能够实现 200 万字的一个超能力

843
00:27:12,883 --> 00:27:14,966
那点击了一个申请

844
00:27:14,966 --> 00:27:15,800
工作人员告诉我

845
00:27:15,800 --> 00:27:17,800
可能我需要等一段时间

846
00:27:17,800 --> 00:27:18,733
会等一两天

847
00:27:18,733 --> 00:27:20,083
他给我开通一下账户

848
00:27:20,083 --> 00:27:21,600
让我去体验一下

849
00:27:21,600 --> 00:27:23,283
那 Kimi 的这个长序列

850
00:27:23,283 --> 00:27:25,166
超长序列的一个记忆力能力

851
00:27:25,166 --> 00:27:27,200
猪米也想赶紧的去体验一下

852
00:27:27,200 --> 00:27:28,133
丢很多

853
00:27:28,133 --> 00:27:28,966
的信息给他

854
00:27:28,966 --> 00:27:30,083
看他能不能真正

855
00:27:30,083 --> 00:27:32,000
帮做到很好的思考

856
00:27:33,883 --> 00:27:35,166
因为实验部分

857
00:27:35,166 --> 00:27:36,600
没有办法去使用了

858
00:27:36,600 --> 00:27:37,683
只能过一段时间

859
00:27:37,683 --> 00:27:39,483
回头再进行个体验

860
00:27:39,483 --> 00:27:41,850
现在来到了第四个内容

861
00:27:41,850 --> 00:27:44,283
看一下整体的整个 Kimi

862
00:27:44,283 --> 00:27:46,200
这个 MOMA 对于整个行业

863
00:27:46,200 --> 00:27:48,533
或者整个大模型相关带来的思考

864
00:27:48,566 --> 00:27:49,283
那首先

865
00:27:49,283 --> 00:27:50,566
哎呦 ZOMI 总结了两条

866
00:27:50,566 --> 00:27:51,000
第一条

867
00:27:51,000 --> 00:27:53,333
就是 Reasoning 能够就是自推理

868
00:27:53,333 --> 00:27:55,166
或者推理的模型

869
00:27:55,166 --> 00:27:57,850
自然而然的带来了一个 long context

870
00:27:57,850 --> 00:27:59,250
就是长序列的问题

871
00:27:59,366 --> 00:28:01,200
未来的大模型的 scaling low

872
00:28:01,200 --> 00:28:03,600
会慢慢地转向 TTS

873
00:28:03,766 --> 00:28:05,683
对应的 testime scaling

874
00:28:05,733 --> 00:28:08,883
所以 long context 这个最重要的需求就是

875
00:28:08,883 --> 00:28:09,966
可能会继续挤压

876
00:28:09,966 --> 00:28:11,766
或者继续去压缩内存

877
00:28:11,883 --> 00:28:15,566
减少哦 long context 一个访存的压力

878
00:28:15,600 --> 00:28:17,850
所以未来的算法可能慢慢的转向

879
00:28:17,850 --> 00:28:19,333
AIInfra 层面

880
00:28:19,333 --> 00:28:20,966
卷越来越转

881
00:28:20,966 --> 00:28:22,366
AIInfra 越来越卷

882
00:28:22,366 --> 00:28:23,800
稀疏越来越卷

883
00:28:23,800 --> 00:28:25,400
一个访存的压力

884
00:28:25,400 --> 00:28:28,000
提升访存的代换和使用率

885
00:28:28,483 --> 00:28:28,883
第二个

886
00:28:28,883 --> 00:28:31,766
就是不管是 DeepSeek 的一个 NAS

887
00:28:31,766 --> 00:28:33,166
还是 Kimi 的 MoBA

888
00:28:33,166 --> 00:28:34,400
它只是个开始

889
00:28:34,483 --> 00:28:35,650
长序列的优化

890
00:28:35,650 --> 00:28:37,200
未来可能会出现新

891
00:28:37,200 --> 00:28:38,850
并行策略的优化方案

892
00:28:38,883 --> 00:28:40,283
那像 NAS

893
00:28:40,283 --> 00:28:41,683
DeepSeek 这种方案

894
00:28:41,683 --> 00:28:43,766
实现的是软硬件结合

895
00:28:43,766 --> 00:28:45,800
说实话它相对来说过于复杂

896
00:28:45,800 --> 00:28:47,333
工程压力也比较大

897
00:28:47,533 --> 00:28:49,733
可能不利于并行策略的优化

898
00:28:49,733 --> 00:28:50,933
或者底层 AIInfra

899
00:28:50,933 --> 00:28:53,366
一个非常好的一个实现

900
00:28:53,483 --> 00:28:54,166
可能未来了

901
00:28:54,166 --> 00:28:55,366
需要引入更多

902
00:28:55,366 --> 00:28:57,766
像 MoBA 相关的一个优化的方案

903
00:28:57,766 --> 00:28:58,766
或者稀疏的方案

904
00:28:58,766 --> 00:29:00,250
或者稀疏的算法从

905
00:29:00,250 --> 00:29:02,000
而使得算法跟 AIInfra

906
00:29:02,000 --> 00:29:03,166
跟软硬件

907
00:29:03,400 --> 00:29:05,050
更好的进行一个结合

908
00:29:05,050 --> 00:29:06,283
那今天的内容

909
00:29:06,283 --> 00:29:07,650
就先到这里为止

910
00:29:07,650 --> 00:29:09,933
简单的来看一下这篇文章

911
00:29:09,933 --> 00:29:12,050
相关的链接都在这个 PPT 里面

912
00:29:12,050 --> 00:29:13,366
如果有兴趣的小伙伴

913
00:29:13,366 --> 00:29:14,850
也可以去上面去获取

914
00:29:14,850 --> 00:29:15,800
那今天的内容

915
00:29:15,800 --> 00:29:16,766
就先到这里为止

916
00:29:16,766 --> 00:29:17,450
谢谢各位

917
00:29:17,450 --> 00:29:18,283
拜了个拜

918
00:29:19,050 --> 00:29:19,733
卷的不行了

919
00:29:19,733 --> 00:29:20,683
卷的不行了

920
00:29:20,683 --> 00:29:22,366
AI 系统的全套知识都在这里

921
00:29:22,366 --> 00:29:24,000
欢迎跟着目录进行学习

922
00:29:24,400 --> 00:29:25,333
给我一键三连

923
00:29:25,333 --> 00:29:26,533
给我一键三连

924
00:29:27,000 --> 00:29:27,683
谢谢各位

925
00:29:27,683 --> 00:29:28,733
拜了个拜

